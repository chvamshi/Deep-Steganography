{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LOSS1_DS_img_crop_50%.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfk0hulgv7I9yXE3iZt0r9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chvamshi/Deep-Steganography/blob/main/LOSS1_DS_img_crop_50_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_7yYQrOxZZg",
        "outputId": "e0b2066f-5d02-47ff-a344-d72abfd465b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip && ls tiny-imagenet-200"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4On8a0UxcJd",
        "outputId": "34e11293-e318-4467-f3ac-b4558586ea77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-04 19:37:22--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  18.7MB/s    in 16s     \n",
            "\n",
            "2022-02-04 19:37:39 (14.5 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n",
            "test  train  val  wnids.txt  words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports necessary libraries and modules\n",
        "from itertools import islice\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import utils\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os \n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import random\n",
        "from torchvision import datasets, utils\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision.transforms import RandomResizedCrop\n",
        "from torchvision.transforms.functional import resized_crop\n",
        "from random import shuffle\n",
        "from IPython.display import Image\n",
        "from PIL import Image\n",
        "\n",
        "# Use GPU if it is present\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "vpID7LDYoj27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e1616f-cd14-465a-8886-18262399b982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory path\n",
        "# os.chdir(\"..\")\n",
        "cwd = '/content/tiny-imagenet-200'\n",
        "\n",
        "# Hyper Parameters\n",
        "num_epochs = 50\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "beta = 1\n",
        "\n",
        "# Mean and std deviation of imagenet dataset. Source: http://cs231n.stanford.edu/reports/2017/pdfs/101.pdf\n",
        "std = [0.229, 0.224, 0.225]\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "\n",
        "# TODO: Define train, validation and models\n",
        "MODELS_PATH = '/content/output/models'\n",
        "# TRAIN_PATH = cwd+'/train/'\n",
        "# VALID_PATH = cwd+'/valid/'\n",
        "VALID_PATH = cwd+'/val/'\n",
        "TRAIN_PATH = cwd+'/train/'\n",
        "TEST_PATH = cwd+'/test/'\n",
        "\n",
        "# if not os.path.exists(MODELS_PATH): os.mkdir(MODELS_PATH)"
      ],
      "metadata": {
        "id": "93ZuzEhExq-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def variance(image1, image2):\n",
        "    var1 = torch.var(image1)\n",
        "    var2 = torch.var(image2)\n",
        "\n",
        "    return var1+var2\n",
        "\n",
        "def customized_loss(S_prime, C_prime, S, C, B):\n",
        "    ''' Calculates loss specified on the paper.'''\n",
        "    \n",
        "    loss_cover = torch.nn.functional.l1_loss(C_prime, C)\n",
        "    loss_cover = loss_cover/(64*64)\n",
        "    loss_secret = torch.nn.functional.l1_loss(S_prime, S)\n",
        "    loss_secret = loss_secret/(64*64)\n",
        "\n",
        "    variance_cover = variance(C_prime, C)\n",
        "    variance_secret = variance(S_prime, S)\n",
        "\n",
        "    loss_all = loss_cover+loss_secret+variance_cover+variance_secret\n",
        "    loss_all = loss_all/4\n",
        "\n",
        "    return loss_all, loss_cover, loss_secret\n",
        "\n",
        "def denormalize(image, std, mean):\n",
        "    ''' Denormalizes a tensor of images.'''\n",
        "\n",
        "    for t in range(3):\n",
        "        image[t, :, :] = (image[t, :, :] * std[t]) + mean[t]\n",
        "    return image\n",
        "\n",
        "def imshow(img, idx, learning_rate, beta):\n",
        "    '''Prints out an image given in tensor format.'''\n",
        "    \n",
        "    img = denormalize(img, std, mean)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.title('Example '+str(idx)+', lr='+str(learning_rate)+', B='+str(beta))\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "def gaussian(tensor, mean=0, stddev=0.1):\n",
        "    '''Adds random noise to a tensor.'''\n",
        "    \n",
        "    noise = torch.nn.init.normal(torch.Tensor(tensor.size()), 0, 0.1).to(device)\n",
        "    return Variable(tensor + noise)\n",
        "\n",
        "def tensor_to_image(tensor):\n",
        "    \"\"\"\n",
        "    Transforms a torch tensor into numpy uint8 array (image)\n",
        "    :param tensor: (batch_size x channels x height x width) torch tensor in range [-1.0, 1.0]\n",
        "    :return: (batch_size x height x width x channels) uint8 array\n",
        "    \"\"\"\n",
        "    image = tensor.cpu().numpy()[0]\n",
        "    # image = tensor.permute(0, 2, 3, 1).cpu().numpy()\n",
        "    image = np.transpose(image, (1,2,0))\n",
        "    image = (image + 1) * 127.5\n",
        "    return np.clip(image, 0, 255).astype(np.uint8)"
      ],
      "metadata": {
        "id": "VwOcOzBtyJeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def random_float(min, max):\n",
        "    \"\"\"\n",
        "    Return a random number\n",
        "    :param min:\n",
        "    :param max:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return np.random.rand() * (max - min) + min\n",
        "\n",
        "\n",
        "def get_random_rectangle_inside(image, height_ratio_range, width_ratio_range):\n",
        "    \"\"\"\n",
        "    Returns a random rectangle inside the image, where the size is random and is controlled by height_ratio_range and width_ratio_range.\n",
        "    This is analogous to a random crop. For example, if height_ratio_range is (0.7, 0.9), then a random number in that range will be chosen\n",
        "    (say it is 0.75 for illustration), and the image will be cropped such that the remaining height equals 0.75. In fact,\n",
        "    a random 'starting' position rs will be chosen from (0, 0.25), and the crop will start at rs and end at rs + 0.75. This ensures\n",
        "    that we crop from top/bottom with equal probability.\n",
        "    The same logic applies to the width of the image, where width_ratio_range controls the width crop range.\n",
        "    :param image: The image we want to crop\n",
        "    :param height_ratio_range: The range of remaining height ratio\n",
        "    :param width_ratio_range:  The range of remaining width ratio.\n",
        "    :return: \"Cropped\" rectange with width and height drawn randomly height_ratio_range and width_ratio_range\n",
        "    \"\"\"\n",
        "    # print(image.shape)\n",
        "    image_height = image.shape[1]\n",
        "    image_width = image.shape[2]\n",
        "\n",
        "    remaining_height = int(np.rint(random_float(height_ratio_range[0], height_ratio_range[1]) * image_height))\n",
        "    remaining_width = int(np.rint(random_float(width_ratio_range[0], width_ratio_range[0]) * image_width))\n",
        "\n",
        "    if remaining_height == image_height:\n",
        "        height_start = 0\n",
        "    else:\n",
        "        height_start = np.random.randint(0, image_height - remaining_height)\n",
        "\n",
        "    if remaining_width == image_width:\n",
        "        width_start = 0\n",
        "    else:\n",
        "        width_start = np.random.randint(0, image_width - remaining_width)\n",
        "\n",
        "    return height_start, height_start+remaining_height, width_start, width_start+remaining_width\n",
        "\n",
        "\n",
        "class Crop(nn.Module):\n",
        "    \"\"\"\n",
        "    Randomly crops the image from top/bottom and left/right. The amount to crop is controlled by parameters\n",
        "    heigth_ratio_range and width_ratio_range\n",
        "    \"\"\"\n",
        "    def __init__(self, height_ratio_range, width_ratio_range):\n",
        "        \"\"\"\n",
        "        :param height_ratio_range:\n",
        "        :param width_ratio_range:\n",
        "        \"\"\"\n",
        "        super(Crop, self).__init__()\n",
        "        self.height_ratio_range = height_ratio_range\n",
        "        self.width_ratio_range = width_ratio_range\n",
        "\n",
        "\n",
        "    def forward(self, noised_and_cover):\n",
        "        noised_image = noised_and_cover[0]\n",
        "        print(\"Noisedimageshape: \",noised_image.shape)\n",
        "        shape = noised_image.shape\n",
        "        # crop_rectangle is in form (from, to) where @from and @to are 2D points -- (height, width)\n",
        "\n",
        "        h_start, h_end, w_start, w_end = get_random_rectangle_inside(noised_image, self.height_ratio_range, self.width_ratio_range)\n",
        "\n",
        "        noised_image =  noised_image[\n",
        "               :,\n",
        "               h_start: h_end,\n",
        "               w_start: w_end].clone()\n",
        "        noised_image1 = noised_image.expand(-1,64,64)\n",
        "        print(noised_image1.shape)\n",
        "        noised_and_cover[0] = noised_image1.clone()\n",
        "\n",
        "        return noised_and_cover"
      ],
      "metadata": {
        "id": "mB6Bmyhk2oZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparation Network (2 conv layers)\n",
        "class PrepNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrepNetwork, self).__init__()\n",
        "        self.initialP3 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.initialP4 = nn.Sequential(\n",
        "            nn.Conv2d(3, 10, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(10, 10, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.initialP5 = nn.Sequential(\n",
        "            nn.Conv2d(3, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(5, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalP3 = nn.Sequential(\n",
        "            nn.Conv2d(65, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.finalP4 = nn.Sequential(\n",
        "            nn.Conv2d(65, 10, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(10, 10, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalP5 = nn.Sequential(\n",
        "            nn.Conv2d(65, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "\n",
        "    def forward(self, p):\n",
        "        p1 = self.initialP3(p)\n",
        "        p2 = self.initialP4(p)\n",
        "        p3 = self.initialP5(p)\n",
        "        mid = torch.cat((p1, p2, p3), 1)\n",
        "        p4 = self.finalP3(mid)\n",
        "        p5 = self.finalP4(mid)\n",
        "        p6 = self.finalP5(mid)\n",
        "        # print(p4.shape)\n",
        "        # print(p5.shape)\n",
        "        # print(p6.shape)\n",
        "        out = torch.cat((p4, p5, p6), 1)\n",
        "        return out\n",
        "\n",
        "# Hiding Network (5 conv layers)\n",
        "class HidingNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HidingNetwork, self).__init__()\n",
        "        self.initialH3 = nn.Sequential(\n",
        "            nn.Conv2d(68, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.initialH4 = nn.Sequential(\n",
        "            nn.Conv2d(68, 10, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(10, 10, kernel_size=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(10, 10, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(10, 10, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.initialH5 = nn.Sequential(\n",
        "            nn.Conv2d(68, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(5, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(5, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(5, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalH3 = nn.Sequential(\n",
        "            nn.Conv2d(65, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.finalH4 = nn.Sequential(\n",
        "            nn.Conv2d(65, 10, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(10, 10, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalH5 = nn.Sequential(\n",
        "            nn.Conv2d(65, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalH = nn.Sequential(\n",
        "            nn.Conv2d(65, 3, kernel_size=1, padding=0))\n",
        "        \n",
        "    def forward(self, h):\n",
        "        h1 = self.initialH3(h)\n",
        "        h2 = self.initialH4(h)\n",
        "        h3 = self.initialH5(h)\n",
        "        mid = torch.cat((h1, h2, h3), 1)\n",
        "        h4 = self.finalH3(mid)\n",
        "        h5 = self.finalH4(mid)\n",
        "        h6 = self.finalH5(mid)\n",
        "        mid2 = torch.cat((h4, h5, h6), 1)\n",
        "        out = self.finalH(mid2)\n",
        "        out_noise = gaussian(out.data, 0, 0.1)\n",
        "        return out, out_noise\n",
        "\n",
        "# Reveal Network (2 conv layers)\n",
        "class RevealNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RevealNetwork, self).__init__()\n",
        "        self.initialR3 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.initialR4 = nn.Sequential(\n",
        "            nn.Conv2d(3, 10, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(10, 10, kernel_size=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(10, 10, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(10, 10, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.initialR5 = nn.Sequential(\n",
        "            nn.Conv2d(3, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(5, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(5, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(5, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalR3 = nn.Sequential(\n",
        "            nn.Conv2d(65, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.finalR4 = nn.Sequential(\n",
        "            nn.Conv2d(65, 10, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(10, 10, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalR5 = nn.Sequential(\n",
        "            nn.Conv2d(65, 5, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalR = nn.Sequential(\n",
        "            nn.Conv2d(65, 3, kernel_size=1, padding=0))\n",
        "\n",
        "    def forward(self, r, img_shape):\n",
        "        # r = r.expand(img_shape)\n",
        "        r1 = self.initialR3(r)\n",
        "        r2 = self.initialR4(r)\n",
        "        r3 = self.initialR5(r)\n",
        "        mid = torch.cat((r1, r2, r3), 1)\n",
        "        r4 = self.finalR3(mid)\n",
        "        r5 = self.finalR4(mid)\n",
        "        r6 = self.finalR5(mid)\n",
        "        mid2 = torch.cat((r4, r5, r6), 1)\n",
        "        out = self.finalR(mid2)\n",
        "        return out\n",
        "\n",
        "# Join three networks in one module\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.m1 = PrepNetwork().to(device)\n",
        "        self.m2 = HidingNetwork().to(device)\n",
        "        self.m3 = RevealNetwork().to(device)\n",
        "        self.crop = Crop((0.7,0.9),(0.7,0.9)).to(device)\n",
        "\n",
        "    def forward(self, secret, cover):\n",
        "        img_shape = secret.shape\n",
        "        x_1 = self.m1(secret)\n",
        "        mid = torch.cat((x_1, cover), 1)\n",
        "        x_2, x_2_noise = self.m2(mid)\n",
        "        # x_2_noise = resized_crop(img=x_2_noise, top = 0, left=0, height=19, width=19, size=[64,64])\n",
        "        x_2 =  resized_crop(img=x_2, top = 0, left=0, height=32, width=32, size=[64,64])\n",
        "        # x_3 = self.m3(x_2_noise, img_shape)\n",
        "        x_3 = self.m3(x_2, img_shape)\n",
        "        return x_2, x_3 "
      ],
      "metadata": {
        "id": "a2M2cTYmy3Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates net object\n",
        "net = Net().to(device)"
      ],
      "metadata": {
        "id": "kQNOhd-9zijc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates training set\n",
        "train_data = datasets.ImageFolder(\n",
        "        TRAIN_PATH,\n",
        "        transforms.Compose([\n",
        "        # transforms.Scale(256),\n",
        "        # transforms.RandomCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean,\n",
        "        std=std)\n",
        "        ]))\n",
        "train_data = Subset(train_data, [i for i in range(32000)])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=batch_size, num_workers=1, \n",
        "        pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "# Creates test set\n",
        "test_data = datasets.ImageFolder(\n",
        "        TEST_PATH, \n",
        "        transforms.Compose([\n",
        "        # transforms.Scale(256),\n",
        "        # transforms.RandomCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean,\n",
        "        std=std)\n",
        "        ]))\n",
        "test_data = Subset(test_data,[i for i in range(5000)])\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=2, num_workers=1, \n",
        "        pin_memory=True, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "0LAVNXSRzlUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/drive/MyDrive/\"\n",
        "loss_history = []\n",
        "\n",
        "def train_model(train_loader, beta, learning_rate):\n",
        "    \n",
        "    # Save optimizer\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    loss_history = []\n",
        "    # Iterate over batches performing forward and backward passes\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Train mode\n",
        "        net.train()\n",
        "        \n",
        "        train_losses = []\n",
        "        # Train one epoch\n",
        "        for idx, train_batch in enumerate(train_loader):\n",
        "\n",
        "            data, _  = train_batch\n",
        "            data = data.to(device)\n",
        "\n",
        "            # Saves secret images and secret covers\n",
        "            train_covers = data[:len(data)//2]\n",
        "            train_secrets = data[len(data)//2:]\n",
        "            \n",
        "            # Creates variable from secret and cover images\n",
        "            train_secrets = Variable(train_secrets, requires_grad=False)\n",
        "            train_covers = Variable(train_covers, requires_grad=False)\n",
        "\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            train_hidden, train_output = net(train_secrets, train_covers)\n",
        "\n",
        "            # Calculate loss and perform backprop\n",
        "            train_loss, train_loss_cover, train_loss_secret = customized_loss(train_output, train_hidden, train_secrets, train_covers, beta)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Saves training loss\n",
        "            # print(train_loss.data)\n",
        "            train_losses.append(train_loss.data.item())\n",
        "            loss_history.append(train_loss.data.item())\n",
        "            \n",
        "            # Prints mini-batch losses\n",
        "            print('Training: Batch {0}/{1}. Loss of {2:.4f}, cover loss of {3:.4f}, secret loss of {4:.4f}'.format(idx+1, len(train_loader), train_loss.data.item(), train_loss_cover.data.item(), train_loss_secret.data.item()))\n",
        "    \n",
        "        # Save model\n",
        "        # torch.save(net.state_dict(), '/content/Epoch N{}.pkl'.format(epoch+1))\n",
        "        torch.save(net.state_dict(), PATH+\"LOSS1_ds_crop_50.pth\")\n",
        "        with open(PATH+\"LOSS1_loss_history_crop_50.txt\", \"a\") as f:\n",
        "            for s in loss_history:\n",
        "                f.write(str(s) +\"\\n\")\n",
        "        \n",
        "        mean_train_loss = np.mean(train_losses)\n",
        "    \n",
        "        # Prints epoch average loss\n",
        "        print ('Epoch [{0}/{1}], Average_loss: {2:.4f}'.format(\n",
        "                epoch+1, num_epochs, mean_train_loss))\n",
        "    \n",
        "    return net, mean_train_loss, loss_history"
      ],
      "metadata": {
        "id": "qBuFUtolzoYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "# net.load_state_dict(torch.load(PATH+\"LOSS1_ds_crop_50.pth\"))\n",
        "# loss_history = []\n",
        "# with open(PATH+\"LOSS1_loss_history_crop_50.txt\", \"r\") as f:\n",
        "#   for line in f:\n",
        "#     loss_history.append(float(line.strip()))\n",
        "\n",
        "net, mean_train_loss, loss_history = train_model(train_loader, beta, learning_rate)\n",
        "# loss_history.extend(loss_history1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVneQXAL0AVQ",
        "outputId": "a0436d50-da20-4bd1-8adf-56fb8885dff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training: Batch 6/1000. Loss of 0.6925, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 7/1000. Loss of 0.6482, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 8/1000. Loss of 0.7116, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 9/1000. Loss of 0.6864, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 10/1000. Loss of 0.6611, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 11/1000. Loss of 0.7071, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 12/1000. Loss of 0.6192, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 13/1000. Loss of 0.5817, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 14/1000. Loss of 0.8147, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 15/1000. Loss of 0.6897, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 16/1000. Loss of 0.6155, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 17/1000. Loss of 0.6431, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 18/1000. Loss of 0.5654, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 19/1000. Loss of 0.6819, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 20/1000. Loss of 0.7367, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 21/1000. Loss of 0.6625, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 22/1000. Loss of 0.5520, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 23/1000. Loss of 0.5744, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 24/1000. Loss of 0.5568, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 25/1000. Loss of 0.6406, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 26/1000. Loss of 0.6583, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 27/1000. Loss of 0.6129, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 28/1000. Loss of 0.5653, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 29/1000. Loss of 0.5986, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 30/1000. Loss of 0.6059, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 31/1000. Loss of 0.6156, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 32/1000. Loss of 0.6792, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 33/1000. Loss of 0.6566, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 34/1000. Loss of 0.5370, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 35/1000. Loss of 0.6302, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 36/1000. Loss of 0.7055, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 37/1000. Loss of 0.5442, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 38/1000. Loss of 0.6505, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 39/1000. Loss of 0.5436, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 40/1000. Loss of 0.6608, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 41/1000. Loss of 0.5954, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 42/1000. Loss of 0.6850, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 43/1000. Loss of 0.5980, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 44/1000. Loss of 0.7553, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 45/1000. Loss of 0.6674, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 46/1000. Loss of 0.6799, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 47/1000. Loss of 0.6378, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 48/1000. Loss of 0.6474, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 49/1000. Loss of 0.6066, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 50/1000. Loss of 0.6258, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 51/1000. Loss of 0.6565, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 52/1000. Loss of 0.5831, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 53/1000. Loss of 0.7090, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 54/1000. Loss of 0.5532, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 55/1000. Loss of 0.6131, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 56/1000. Loss of 0.6350, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 57/1000. Loss of 0.6493, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 58/1000. Loss of 0.6159, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 59/1000. Loss of 0.6837, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 60/1000. Loss of 0.6163, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 61/1000. Loss of 0.6524, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 62/1000. Loss of 0.7252, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 63/1000. Loss of 0.7324, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 64/1000. Loss of 0.5844, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 65/1000. Loss of 0.6758, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 66/1000. Loss of 0.6953, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 67/1000. Loss of 0.6140, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 68/1000. Loss of 0.6622, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 69/1000. Loss of 0.6556, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 70/1000. Loss of 0.6092, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 71/1000. Loss of 0.6307, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 72/1000. Loss of 0.5991, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 73/1000. Loss of 0.5656, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 74/1000. Loss of 0.6885, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 75/1000. Loss of 0.6799, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 76/1000. Loss of 0.6945, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 77/1000. Loss of 0.7107, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 78/1000. Loss of 0.6624, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 79/1000. Loss of 0.6352, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 80/1000. Loss of 0.6570, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 81/1000. Loss of 0.5962, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 82/1000. Loss of 0.6914, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 83/1000. Loss of 0.7452, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 84/1000. Loss of 0.6268, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 85/1000. Loss of 0.6221, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 86/1000. Loss of 0.6835, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 87/1000. Loss of 0.5842, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 88/1000. Loss of 0.5359, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 89/1000. Loss of 0.5902, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 90/1000. Loss of 0.6128, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 91/1000. Loss of 0.6465, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 92/1000. Loss of 0.7024, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 93/1000. Loss of 0.5975, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 94/1000. Loss of 0.6932, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 95/1000. Loss of 0.5749, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 96/1000. Loss of 0.7657, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 97/1000. Loss of 0.6488, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 98/1000. Loss of 0.5817, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 99/1000. Loss of 0.5561, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 100/1000. Loss of 0.6691, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 101/1000. Loss of 0.6437, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 102/1000. Loss of 0.6009, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 103/1000. Loss of 0.6806, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 104/1000. Loss of 0.5399, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 105/1000. Loss of 0.7105, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 106/1000. Loss of 0.6331, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 107/1000. Loss of 0.6154, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 108/1000. Loss of 0.6001, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 109/1000. Loss of 0.6722, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 110/1000. Loss of 0.7347, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 111/1000. Loss of 0.6845, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 112/1000. Loss of 0.6459, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 113/1000. Loss of 0.6760, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 114/1000. Loss of 0.5852, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 115/1000. Loss of 0.6514, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 116/1000. Loss of 0.6292, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 117/1000. Loss of 0.6437, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 118/1000. Loss of 0.6561, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 119/1000. Loss of 0.7009, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 120/1000. Loss of 0.5562, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 121/1000. Loss of 0.6948, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 122/1000. Loss of 0.7378, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 123/1000. Loss of 0.6830, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 124/1000. Loss of 0.6199, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 125/1000. Loss of 0.6306, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 126/1000. Loss of 0.7079, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 127/1000. Loss of 0.6727, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 128/1000. Loss of 0.5720, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 129/1000. Loss of 0.4942, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 130/1000. Loss of 0.6110, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 131/1000. Loss of 0.6118, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 132/1000. Loss of 0.7172, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 133/1000. Loss of 0.6141, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 134/1000. Loss of 0.5790, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 135/1000. Loss of 0.5761, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 136/1000. Loss of 0.5094, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 137/1000. Loss of 0.6308, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 138/1000. Loss of 0.6414, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 139/1000. Loss of 0.6909, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 140/1000. Loss of 0.6814, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 141/1000. Loss of 0.6787, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 142/1000. Loss of 0.6135, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 143/1000. Loss of 0.5841, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 144/1000. Loss of 0.6180, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 145/1000. Loss of 0.5912, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 146/1000. Loss of 0.7030, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 147/1000. Loss of 0.6630, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 148/1000. Loss of 0.5793, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 149/1000. Loss of 0.7004, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 150/1000. Loss of 0.6588, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 151/1000. Loss of 0.8107, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 152/1000. Loss of 0.7133, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 153/1000. Loss of 0.7774, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 154/1000. Loss of 0.8023, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 155/1000. Loss of 0.5746, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 156/1000. Loss of 0.6469, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 157/1000. Loss of 0.6745, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 158/1000. Loss of 0.6216, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 159/1000. Loss of 0.6412, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 160/1000. Loss of 0.6866, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 161/1000. Loss of 0.6934, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 162/1000. Loss of 0.6589, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 163/1000. Loss of 0.6506, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 164/1000. Loss of 0.7584, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 165/1000. Loss of 0.6438, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 166/1000. Loss of 0.7493, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 167/1000. Loss of 0.6425, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 168/1000. Loss of 0.6062, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 169/1000. Loss of 0.6152, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 170/1000. Loss of 0.7436, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 171/1000. Loss of 0.7308, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 172/1000. Loss of 0.6224, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 173/1000. Loss of 0.6384, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 174/1000. Loss of 0.6343, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 175/1000. Loss of 0.6420, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 176/1000. Loss of 0.5380, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 177/1000. Loss of 0.5895, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 178/1000. Loss of 0.6239, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 179/1000. Loss of 0.6036, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 180/1000. Loss of 0.7503, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 181/1000. Loss of 0.6823, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 182/1000. Loss of 0.5909, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 183/1000. Loss of 0.5842, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 184/1000. Loss of 0.6869, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 185/1000. Loss of 0.5990, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 186/1000. Loss of 0.6728, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 187/1000. Loss of 0.7153, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 188/1000. Loss of 0.5893, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 189/1000. Loss of 0.6598, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 190/1000. Loss of 0.6076, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 191/1000. Loss of 0.7251, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 192/1000. Loss of 0.7105, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 193/1000. Loss of 0.6602, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 194/1000. Loss of 0.7083, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 195/1000. Loss of 0.7126, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 196/1000. Loss of 0.7170, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 197/1000. Loss of 0.7287, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 198/1000. Loss of 0.6796, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 199/1000. Loss of 0.6096, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 200/1000. Loss of 0.6110, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 201/1000. Loss of 0.6545, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 202/1000. Loss of 0.6868, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 203/1000. Loss of 0.6173, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 204/1000. Loss of 0.6636, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 205/1000. Loss of 0.6398, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 206/1000. Loss of 0.6112, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 207/1000. Loss of 0.5458, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 208/1000. Loss of 0.7210, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 209/1000. Loss of 0.6364, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 210/1000. Loss of 0.6469, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 211/1000. Loss of 0.7759, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 212/1000. Loss of 0.6066, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 213/1000. Loss of 0.6723, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 214/1000. Loss of 0.6000, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 215/1000. Loss of 0.6295, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 216/1000. Loss of 0.6376, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 217/1000. Loss of 0.5449, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 218/1000. Loss of 0.7003, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 219/1000. Loss of 0.5882, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 220/1000. Loss of 0.6145, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 221/1000. Loss of 0.5688, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 222/1000. Loss of 0.6036, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 223/1000. Loss of 0.6089, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 224/1000. Loss of 0.6348, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 225/1000. Loss of 0.7151, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 226/1000. Loss of 0.7041, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 227/1000. Loss of 0.6401, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 228/1000. Loss of 0.6602, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 229/1000. Loss of 0.6872, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 230/1000. Loss of 0.6561, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 231/1000. Loss of 0.6072, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 232/1000. Loss of 0.5907, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 233/1000. Loss of 0.6319, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 234/1000. Loss of 0.6202, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 235/1000. Loss of 0.6799, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 236/1000. Loss of 0.6390, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 237/1000. Loss of 0.6718, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 238/1000. Loss of 0.5782, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 239/1000. Loss of 0.5801, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 240/1000. Loss of 0.5527, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 241/1000. Loss of 0.6286, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 242/1000. Loss of 0.6182, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 243/1000. Loss of 0.5775, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 244/1000. Loss of 0.6874, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 245/1000. Loss of 0.6004, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 246/1000. Loss of 0.6582, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 247/1000. Loss of 0.5950, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 248/1000. Loss of 0.6305, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 249/1000. Loss of 0.5973, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 250/1000. Loss of 0.6596, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 251/1000. Loss of 0.5337, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 252/1000. Loss of 0.7316, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 253/1000. Loss of 0.7501, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 254/1000. Loss of 0.7002, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 255/1000. Loss of 0.7542, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 256/1000. Loss of 0.6754, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 257/1000. Loss of 0.6406, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 258/1000. Loss of 0.6335, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 259/1000. Loss of 0.6618, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 260/1000. Loss of 0.7027, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 261/1000. Loss of 0.5905, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 262/1000. Loss of 0.6077, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 263/1000. Loss of 0.6036, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 264/1000. Loss of 0.6755, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 265/1000. Loss of 0.6123, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 266/1000. Loss of 0.6727, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 267/1000. Loss of 0.6890, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 268/1000. Loss of 0.6834, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 269/1000. Loss of 0.6346, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 270/1000. Loss of 0.6398, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 271/1000. Loss of 0.6054, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 272/1000. Loss of 0.6430, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 273/1000. Loss of 0.6565, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 274/1000. Loss of 0.5827, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 275/1000. Loss of 0.5624, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 276/1000. Loss of 0.6763, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 277/1000. Loss of 0.7010, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 278/1000. Loss of 0.6735, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 279/1000. Loss of 0.6109, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 280/1000. Loss of 0.6731, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 281/1000. Loss of 0.7153, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 282/1000. Loss of 0.7473, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 283/1000. Loss of 0.7234, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 284/1000. Loss of 0.7068, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 285/1000. Loss of 0.6590, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 286/1000. Loss of 0.5784, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 287/1000. Loss of 0.6540, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 288/1000. Loss of 0.6471, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 289/1000. Loss of 0.6588, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 290/1000. Loss of 0.6745, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 291/1000. Loss of 0.5820, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 292/1000. Loss of 0.6075, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 293/1000. Loss of 0.6834, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 294/1000. Loss of 0.7568, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 295/1000. Loss of 0.6663, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 296/1000. Loss of 0.7276, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 297/1000. Loss of 0.6616, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 298/1000. Loss of 0.6691, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 299/1000. Loss of 0.7790, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 300/1000. Loss of 0.6703, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 301/1000. Loss of 0.6630, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 302/1000. Loss of 0.6870, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 303/1000. Loss of 0.6084, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 304/1000. Loss of 0.5398, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 305/1000. Loss of 0.6968, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 306/1000. Loss of 0.5668, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 307/1000. Loss of 0.6371, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 308/1000. Loss of 0.6814, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 309/1000. Loss of 0.6623, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 310/1000. Loss of 0.6224, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 311/1000. Loss of 0.6042, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 312/1000. Loss of 0.5875, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 313/1000. Loss of 0.6797, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 314/1000. Loss of 0.6031, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 315/1000. Loss of 0.5992, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 316/1000. Loss of 0.6834, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 317/1000. Loss of 0.6070, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 318/1000. Loss of 0.6240, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 319/1000. Loss of 0.6164, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 320/1000. Loss of 0.6478, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 321/1000. Loss of 0.6126, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 322/1000. Loss of 0.6328, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 323/1000. Loss of 0.6434, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 324/1000. Loss of 0.7380, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 325/1000. Loss of 0.6817, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 326/1000. Loss of 0.6075, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 327/1000. Loss of 0.6271, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 328/1000. Loss of 0.6781, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 329/1000. Loss of 0.6482, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 330/1000. Loss of 0.6646, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 331/1000. Loss of 0.5794, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 332/1000. Loss of 0.6296, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 333/1000. Loss of 0.6963, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 334/1000. Loss of 0.6857, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 335/1000. Loss of 0.5848, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 336/1000. Loss of 0.6059, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 337/1000. Loss of 0.6552, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 338/1000. Loss of 0.5420, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 339/1000. Loss of 0.6065, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 340/1000. Loss of 0.6938, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 341/1000. Loss of 0.5624, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 342/1000. Loss of 0.6630, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 343/1000. Loss of 0.6863, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 344/1000. Loss of 0.6179, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 345/1000. Loss of 0.6295, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 346/1000. Loss of 0.6928, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 347/1000. Loss of 0.7105, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 348/1000. Loss of 0.5791, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 349/1000. Loss of 0.6528, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 350/1000. Loss of 0.5736, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 351/1000. Loss of 0.6923, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 352/1000. Loss of 0.5811, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 353/1000. Loss of 0.6467, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 354/1000. Loss of 0.6484, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 355/1000. Loss of 0.7707, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 356/1000. Loss of 0.7102, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 357/1000. Loss of 0.6425, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 358/1000. Loss of 0.6536, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 359/1000. Loss of 0.6252, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 360/1000. Loss of 0.5697, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 361/1000. Loss of 0.7746, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 362/1000. Loss of 0.7404, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 363/1000. Loss of 0.6735, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 364/1000. Loss of 0.6559, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 365/1000. Loss of 0.5901, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 366/1000. Loss of 0.7027, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 367/1000. Loss of 0.6416, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 368/1000. Loss of 0.6766, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 369/1000. Loss of 0.5688, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 370/1000. Loss of 0.6140, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 371/1000. Loss of 0.6942, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 372/1000. Loss of 0.6558, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 373/1000. Loss of 0.7601, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 374/1000. Loss of 0.6267, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 375/1000. Loss of 0.6175, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 376/1000. Loss of 0.7378, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 377/1000. Loss of 0.6644, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 378/1000. Loss of 0.6133, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 379/1000. Loss of 0.7154, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 380/1000. Loss of 0.6234, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 381/1000. Loss of 0.6805, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 382/1000. Loss of 0.5794, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 383/1000. Loss of 0.6101, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 384/1000. Loss of 0.6325, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 385/1000. Loss of 0.6310, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 386/1000. Loss of 0.6475, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 387/1000. Loss of 0.6584, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 388/1000. Loss of 0.6972, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 389/1000. Loss of 0.5161, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 390/1000. Loss of 0.5741, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 391/1000. Loss of 0.6556, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 392/1000. Loss of 0.6825, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 393/1000. Loss of 0.6977, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 394/1000. Loss of 0.6027, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 395/1000. Loss of 0.5678, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 396/1000. Loss of 0.6242, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 397/1000. Loss of 0.6568, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 398/1000. Loss of 0.6432, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 399/1000. Loss of 0.7227, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 400/1000. Loss of 0.6276, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 401/1000. Loss of 0.6021, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 402/1000. Loss of 0.5948, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 403/1000. Loss of 0.5729, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 404/1000. Loss of 0.6334, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 405/1000. Loss of 0.6434, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 406/1000. Loss of 0.6392, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 407/1000. Loss of 0.7282, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 408/1000. Loss of 0.6017, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 409/1000. Loss of 0.8314, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 410/1000. Loss of 0.5964, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 411/1000. Loss of 0.5203, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 412/1000. Loss of 0.6867, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 413/1000. Loss of 0.6565, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 414/1000. Loss of 0.6094, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 415/1000. Loss of 0.5650, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 416/1000. Loss of 0.6857, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 417/1000. Loss of 0.6612, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 418/1000. Loss of 0.6753, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 419/1000. Loss of 0.7018, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 420/1000. Loss of 0.6659, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 421/1000. Loss of 0.6499, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 422/1000. Loss of 0.6744, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 423/1000. Loss of 0.6751, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 424/1000. Loss of 0.5814, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 425/1000. Loss of 0.6182, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 426/1000. Loss of 0.7551, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 427/1000. Loss of 0.6545, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 428/1000. Loss of 0.5522, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 429/1000. Loss of 0.6839, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 430/1000. Loss of 0.6357, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 431/1000. Loss of 0.6785, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 432/1000. Loss of 0.5532, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 433/1000. Loss of 0.6493, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 434/1000. Loss of 0.5727, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 435/1000. Loss of 0.6566, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 436/1000. Loss of 0.6954, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 437/1000. Loss of 0.6634, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 438/1000. Loss of 0.5572, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 439/1000. Loss of 0.6861, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 440/1000. Loss of 0.6402, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 441/1000. Loss of 0.6564, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 442/1000. Loss of 0.5278, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 443/1000. Loss of 0.5615, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 444/1000. Loss of 0.7373, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 445/1000. Loss of 0.5583, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 446/1000. Loss of 0.6263, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 447/1000. Loss of 0.5964, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 448/1000. Loss of 0.6129, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 449/1000. Loss of 0.5798, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 450/1000. Loss of 0.6487, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 451/1000. Loss of 0.6482, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 452/1000. Loss of 0.6821, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 453/1000. Loss of 0.6839, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 454/1000. Loss of 0.6030, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 455/1000. Loss of 0.6662, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 456/1000. Loss of 0.6757, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 457/1000. Loss of 0.6687, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 458/1000. Loss of 0.6509, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 459/1000. Loss of 0.7106, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 460/1000. Loss of 0.6096, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 461/1000. Loss of 0.5489, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 462/1000. Loss of 0.6454, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 463/1000. Loss of 0.6112, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 464/1000. Loss of 0.6127, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 465/1000. Loss of 0.7073, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 466/1000. Loss of 0.6930, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 467/1000. Loss of 0.6105, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 468/1000. Loss of 0.5802, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 469/1000. Loss of 0.7034, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 470/1000. Loss of 0.6496, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 471/1000. Loss of 0.6479, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 472/1000. Loss of 0.6040, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 473/1000. Loss of 0.7104, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 474/1000. Loss of 0.6342, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 475/1000. Loss of 0.6704, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 476/1000. Loss of 0.6989, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 477/1000. Loss of 0.6593, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 478/1000. Loss of 0.6542, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 479/1000. Loss of 0.6705, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 480/1000. Loss of 0.6746, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 481/1000. Loss of 0.7211, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 482/1000. Loss of 0.6518, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 483/1000. Loss of 0.6969, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 484/1000. Loss of 0.6172, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 485/1000. Loss of 0.5819, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 486/1000. Loss of 0.7059, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 487/1000. Loss of 0.7102, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 488/1000. Loss of 0.6665, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 489/1000. Loss of 0.6269, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 490/1000. Loss of 0.7164, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 491/1000. Loss of 0.6497, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 492/1000. Loss of 0.6247, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 493/1000. Loss of 0.7103, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 494/1000. Loss of 0.6677, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 495/1000. Loss of 0.6948, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 496/1000. Loss of 0.6010, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 497/1000. Loss of 0.6041, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 498/1000. Loss of 0.6596, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 499/1000. Loss of 0.6782, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 500/1000. Loss of 0.6694, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 501/1000. Loss of 0.6153, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 502/1000. Loss of 0.7316, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 503/1000. Loss of 0.6753, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 504/1000. Loss of 0.7089, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 505/1000. Loss of 0.6493, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 506/1000. Loss of 0.6820, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 507/1000. Loss of 0.7030, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 508/1000. Loss of 0.6894, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 509/1000. Loss of 0.6432, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 510/1000. Loss of 0.6099, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 511/1000. Loss of 0.5721, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 512/1000. Loss of 0.6201, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 513/1000. Loss of 0.6345, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 514/1000. Loss of 0.5873, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 515/1000. Loss of 0.6284, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 516/1000. Loss of 0.6594, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 517/1000. Loss of 0.6550, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 518/1000. Loss of 0.6087, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 519/1000. Loss of 0.6891, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 520/1000. Loss of 0.6802, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 521/1000. Loss of 0.6605, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 522/1000. Loss of 0.6515, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 523/1000. Loss of 0.5883, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 524/1000. Loss of 0.6989, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 525/1000. Loss of 0.7050, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 526/1000. Loss of 0.7203, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 527/1000. Loss of 0.7378, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 528/1000. Loss of 0.5441, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 529/1000. Loss of 0.6324, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 530/1000. Loss of 0.5929, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 531/1000. Loss of 0.5807, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 532/1000. Loss of 0.5952, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 533/1000. Loss of 0.6224, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 534/1000. Loss of 0.6172, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 535/1000. Loss of 0.6473, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 536/1000. Loss of 0.6128, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 537/1000. Loss of 0.6317, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 538/1000. Loss of 0.6165, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 539/1000. Loss of 0.6581, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 540/1000. Loss of 0.5473, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 541/1000. Loss of 0.4934, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 542/1000. Loss of 0.7139, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 543/1000. Loss of 0.6484, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 544/1000. Loss of 0.6109, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 545/1000. Loss of 0.6159, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 546/1000. Loss of 0.6889, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 547/1000. Loss of 0.6870, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 548/1000. Loss of 0.6615, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 549/1000. Loss of 0.7359, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 550/1000. Loss of 0.7696, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 551/1000. Loss of 0.6506, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 552/1000. Loss of 0.6709, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 553/1000. Loss of 0.5780, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 554/1000. Loss of 0.6426, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 555/1000. Loss of 0.5559, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 556/1000. Loss of 0.6265, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 557/1000. Loss of 0.6611, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 558/1000. Loss of 0.6301, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 559/1000. Loss of 0.5521, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 560/1000. Loss of 0.6688, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 561/1000. Loss of 0.6986, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 562/1000. Loss of 0.6505, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 563/1000. Loss of 0.7218, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 564/1000. Loss of 0.6227, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 565/1000. Loss of 0.6557, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 566/1000. Loss of 0.7218, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 567/1000. Loss of 0.6216, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 568/1000. Loss of 0.6319, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 569/1000. Loss of 0.6563, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 570/1000. Loss of 0.7535, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 571/1000. Loss of 0.7176, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 572/1000. Loss of 0.6701, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 573/1000. Loss of 0.5667, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 574/1000. Loss of 0.6801, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 575/1000. Loss of 0.6314, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 576/1000. Loss of 0.6031, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 577/1000. Loss of 0.6448, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 578/1000. Loss of 0.6858, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 579/1000. Loss of 0.6586, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 580/1000. Loss of 0.6336, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 581/1000. Loss of 0.6519, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 582/1000. Loss of 0.8214, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 583/1000. Loss of 0.6304, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 584/1000. Loss of 0.6821, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 585/1000. Loss of 0.6341, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 586/1000. Loss of 0.6144, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 587/1000. Loss of 0.6827, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 588/1000. Loss of 0.7464, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 589/1000. Loss of 0.7790, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 590/1000. Loss of 0.7172, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 591/1000. Loss of 0.6670, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 592/1000. Loss of 0.6078, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 593/1000. Loss of 0.6580, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 594/1000. Loss of 0.5881, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 595/1000. Loss of 0.6758, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 596/1000. Loss of 0.5640, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 597/1000. Loss of 0.7598, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 598/1000. Loss of 0.7087, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 599/1000. Loss of 0.7639, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 600/1000. Loss of 0.6499, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 601/1000. Loss of 0.7629, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 602/1000. Loss of 0.6435, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 603/1000. Loss of 0.6800, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 604/1000. Loss of 0.6821, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 605/1000. Loss of 0.6668, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 606/1000. Loss of 0.5739, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 607/1000. Loss of 0.6486, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 608/1000. Loss of 0.7406, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 609/1000. Loss of 0.7387, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 610/1000. Loss of 0.6223, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 611/1000. Loss of 0.7483, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 612/1000. Loss of 0.6013, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 613/1000. Loss of 0.5673, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 614/1000. Loss of 0.6819, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 615/1000. Loss of 0.6181, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 616/1000. Loss of 0.7077, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 617/1000. Loss of 0.6503, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 618/1000. Loss of 0.6784, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 619/1000. Loss of 0.7401, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 620/1000. Loss of 0.6584, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 621/1000. Loss of 0.6088, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 622/1000. Loss of 0.6423, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 623/1000. Loss of 0.7485, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 624/1000. Loss of 0.6676, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 625/1000. Loss of 0.6940, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 626/1000. Loss of 0.6596, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 627/1000. Loss of 0.6162, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 628/1000. Loss of 0.7079, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 629/1000. Loss of 0.5532, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 630/1000. Loss of 0.6573, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 631/1000. Loss of 0.7287, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 632/1000. Loss of 0.7208, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 633/1000. Loss of 0.6473, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 634/1000. Loss of 0.6634, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 635/1000. Loss of 0.6481, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 636/1000. Loss of 0.7335, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 637/1000. Loss of 0.6510, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 638/1000. Loss of 0.6277, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 639/1000. Loss of 0.6435, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 640/1000. Loss of 0.6350, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 641/1000. Loss of 0.6137, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 642/1000. Loss of 0.7830, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 643/1000. Loss of 0.7222, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 644/1000. Loss of 0.6505, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 645/1000. Loss of 0.6152, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 646/1000. Loss of 0.6601, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 647/1000. Loss of 0.5833, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 648/1000. Loss of 0.7624, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 649/1000. Loss of 0.6518, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 650/1000. Loss of 0.7306, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 651/1000. Loss of 0.6066, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 652/1000. Loss of 0.7317, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 653/1000. Loss of 0.6274, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 654/1000. Loss of 0.7508, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 655/1000. Loss of 0.6057, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 656/1000. Loss of 0.6455, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 657/1000. Loss of 0.5945, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 658/1000. Loss of 0.6027, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 659/1000. Loss of 0.6251, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 660/1000. Loss of 0.6759, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 661/1000. Loss of 0.6272, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 662/1000. Loss of 0.7295, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 663/1000. Loss of 0.5827, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 664/1000. Loss of 0.6134, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 665/1000. Loss of 0.5864, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 666/1000. Loss of 0.5613, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 667/1000. Loss of 0.7064, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 668/1000. Loss of 0.6330, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 669/1000. Loss of 0.6524, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 670/1000. Loss of 0.6432, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 671/1000. Loss of 0.6412, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 672/1000. Loss of 0.7156, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 673/1000. Loss of 0.7459, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 674/1000. Loss of 0.6452, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 675/1000. Loss of 0.6909, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 676/1000. Loss of 0.6121, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 677/1000. Loss of 0.6281, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 678/1000. Loss of 0.5538, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 679/1000. Loss of 0.6148, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 680/1000. Loss of 0.6889, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 681/1000. Loss of 0.6667, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 682/1000. Loss of 0.6002, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 683/1000. Loss of 0.6909, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 684/1000. Loss of 0.6174, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 685/1000. Loss of 0.5455, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 686/1000. Loss of 0.6019, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 687/1000. Loss of 0.6136, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 688/1000. Loss of 0.6064, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 689/1000. Loss of 0.5897, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 690/1000. Loss of 0.6368, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 691/1000. Loss of 0.7329, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 692/1000. Loss of 0.6402, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 693/1000. Loss of 0.6470, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 694/1000. Loss of 0.6110, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 695/1000. Loss of 0.6943, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 696/1000. Loss of 0.7107, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 697/1000. Loss of 0.6842, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 698/1000. Loss of 0.7044, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 699/1000. Loss of 0.6558, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 700/1000. Loss of 0.6885, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 701/1000. Loss of 0.6779, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 702/1000. Loss of 0.6669, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 703/1000. Loss of 0.7153, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 704/1000. Loss of 0.6575, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 705/1000. Loss of 0.6206, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 706/1000. Loss of 0.6420, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 707/1000. Loss of 0.6354, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 708/1000. Loss of 0.6883, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 709/1000. Loss of 0.7468, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 710/1000. Loss of 0.7337, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 711/1000. Loss of 0.6776, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 712/1000. Loss of 0.6332, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 713/1000. Loss of 0.6313, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 714/1000. Loss of 0.6587, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 715/1000. Loss of 0.6874, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 716/1000. Loss of 0.5377, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 717/1000. Loss of 0.6395, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 718/1000. Loss of 0.6083, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 719/1000. Loss of 0.7322, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 720/1000. Loss of 0.5541, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 721/1000. Loss of 0.6580, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 722/1000. Loss of 0.7037, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 723/1000. Loss of 0.7456, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 724/1000. Loss of 0.6396, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 725/1000. Loss of 0.6046, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 726/1000. Loss of 0.6781, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 727/1000. Loss of 0.6142, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 728/1000. Loss of 0.6660, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 729/1000. Loss of 0.7037, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 730/1000. Loss of 0.7138, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 731/1000. Loss of 0.5898, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 732/1000. Loss of 0.6564, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 733/1000. Loss of 0.6611, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 734/1000. Loss of 0.6786, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 735/1000. Loss of 0.6452, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 736/1000. Loss of 0.6244, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 737/1000. Loss of 0.6157, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 738/1000. Loss of 0.7161, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 739/1000. Loss of 0.6819, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 740/1000. Loss of 0.6619, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 741/1000. Loss of 0.6473, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 742/1000. Loss of 0.6358, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 743/1000. Loss of 0.6275, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 744/1000. Loss of 0.6000, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 745/1000. Loss of 0.7259, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 746/1000. Loss of 0.6607, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 747/1000. Loss of 0.6800, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 748/1000. Loss of 0.6161, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 749/1000. Loss of 0.6428, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 750/1000. Loss of 0.6466, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 751/1000. Loss of 0.6316, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 752/1000. Loss of 0.6472, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 753/1000. Loss of 0.6350, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 754/1000. Loss of 0.5650, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 755/1000. Loss of 0.6251, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 756/1000. Loss of 0.6433, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 757/1000. Loss of 0.6626, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 758/1000. Loss of 0.7065, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 759/1000. Loss of 0.6567, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 760/1000. Loss of 0.6825, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 761/1000. Loss of 0.6544, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 762/1000. Loss of 0.6295, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 763/1000. Loss of 0.6613, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 764/1000. Loss of 0.6177, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 765/1000. Loss of 0.7514, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 766/1000. Loss of 0.6394, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 767/1000. Loss of 0.5520, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 768/1000. Loss of 0.6082, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 769/1000. Loss of 0.7465, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 770/1000. Loss of 0.6327, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 771/1000. Loss of 0.6566, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 772/1000. Loss of 0.7115, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 773/1000. Loss of 0.6242, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 774/1000. Loss of 0.5997, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 775/1000. Loss of 0.6582, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 776/1000. Loss of 0.7593, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 777/1000. Loss of 0.7765, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 778/1000. Loss of 0.6899, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 779/1000. Loss of 0.6969, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 780/1000. Loss of 0.6679, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 781/1000. Loss of 0.7114, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 782/1000. Loss of 0.5682, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 783/1000. Loss of 0.5960, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 784/1000. Loss of 0.6580, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 785/1000. Loss of 0.6960, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 786/1000. Loss of 0.6682, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 787/1000. Loss of 0.6868, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 788/1000. Loss of 0.6909, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 789/1000. Loss of 0.6444, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 790/1000. Loss of 0.6625, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 791/1000. Loss of 0.6377, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 792/1000. Loss of 0.7171, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 793/1000. Loss of 0.6961, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 794/1000. Loss of 0.5376, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 795/1000. Loss of 0.5462, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 796/1000. Loss of 0.6346, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 797/1000. Loss of 0.6777, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 798/1000. Loss of 0.7306, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 799/1000. Loss of 0.7098, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 800/1000. Loss of 0.6024, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 801/1000. Loss of 0.5867, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 802/1000. Loss of 0.6153, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 803/1000. Loss of 0.6140, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 804/1000. Loss of 0.6779, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 805/1000. Loss of 0.5914, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 806/1000. Loss of 0.6071, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 807/1000. Loss of 0.5918, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 808/1000. Loss of 0.5856, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 809/1000. Loss of 0.6500, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 810/1000. Loss of 0.6720, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 811/1000. Loss of 0.5897, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 812/1000. Loss of 0.6577, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 813/1000. Loss of 0.6540, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 814/1000. Loss of 0.6319, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 815/1000. Loss of 0.6317, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 816/1000. Loss of 0.7171, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 817/1000. Loss of 0.6633, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 818/1000. Loss of 0.6910, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 819/1000. Loss of 0.6721, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 820/1000. Loss of 0.6354, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 821/1000. Loss of 0.7440, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 822/1000. Loss of 0.6194, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 823/1000. Loss of 0.6559, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 824/1000. Loss of 0.6736, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 825/1000. Loss of 0.6693, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 826/1000. Loss of 0.6291, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 827/1000. Loss of 0.5622, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 828/1000. Loss of 0.5763, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 829/1000. Loss of 0.5959, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 830/1000. Loss of 0.5491, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 831/1000. Loss of 0.6408, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 832/1000. Loss of 0.6558, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 833/1000. Loss of 0.6789, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 834/1000. Loss of 0.6452, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 835/1000. Loss of 0.6040, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 836/1000. Loss of 0.7029, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 837/1000. Loss of 0.6994, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 838/1000. Loss of 0.6428, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 839/1000. Loss of 0.7128, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 840/1000. Loss of 0.5540, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 841/1000. Loss of 0.6776, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 842/1000. Loss of 0.6525, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 843/1000. Loss of 0.6579, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 844/1000. Loss of 0.5840, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 845/1000. Loss of 0.7160, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 846/1000. Loss of 0.6584, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 847/1000. Loss of 0.6574, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 848/1000. Loss of 0.6648, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 849/1000. Loss of 0.5693, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 850/1000. Loss of 0.6146, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 851/1000. Loss of 0.6192, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 852/1000. Loss of 0.6645, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 853/1000. Loss of 0.7061, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 854/1000. Loss of 0.5860, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 855/1000. Loss of 0.7731, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 856/1000. Loss of 0.6594, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 857/1000. Loss of 0.5822, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 858/1000. Loss of 0.6277, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 859/1000. Loss of 0.6057, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 860/1000. Loss of 0.7392, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 861/1000. Loss of 0.7856, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 862/1000. Loss of 0.5787, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 863/1000. Loss of 0.6455, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 864/1000. Loss of 0.7536, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 865/1000. Loss of 0.6057, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 866/1000. Loss of 0.5760, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 867/1000. Loss of 0.7058, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 868/1000. Loss of 0.6024, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 869/1000. Loss of 0.6075, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 870/1000. Loss of 0.5725, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 871/1000. Loss of 0.6695, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 872/1000. Loss of 0.6410, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 873/1000. Loss of 0.7277, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 874/1000. Loss of 0.5622, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 875/1000. Loss of 0.6340, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 876/1000. Loss of 0.6824, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 877/1000. Loss of 0.6011, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 878/1000. Loss of 0.6046, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 879/1000. Loss of 0.6557, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 880/1000. Loss of 0.6469, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 881/1000. Loss of 0.6929, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 882/1000. Loss of 0.6820, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 883/1000. Loss of 0.6274, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 884/1000. Loss of 0.6789, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 885/1000. Loss of 0.7078, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 886/1000. Loss of 0.6178, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 887/1000. Loss of 0.6490, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 888/1000. Loss of 0.7260, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 889/1000. Loss of 0.7599, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 890/1000. Loss of 0.6690, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 891/1000. Loss of 0.7475, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 892/1000. Loss of 0.7168, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 893/1000. Loss of 0.5702, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 894/1000. Loss of 0.5905, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 895/1000. Loss of 0.5715, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 896/1000. Loss of 0.8132, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 897/1000. Loss of 0.7029, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 898/1000. Loss of 0.5581, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 899/1000. Loss of 0.6252, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 900/1000. Loss of 0.7080, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 901/1000. Loss of 0.6669, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 902/1000. Loss of 0.6423, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 903/1000. Loss of 0.6229, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 904/1000. Loss of 0.6947, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 905/1000. Loss of 0.6005, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 906/1000. Loss of 0.5434, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 907/1000. Loss of 0.7177, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 908/1000. Loss of 0.5751, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 909/1000. Loss of 0.5931, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 910/1000. Loss of 0.5807, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 911/1000. Loss of 0.5643, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 912/1000. Loss of 0.5542, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 913/1000. Loss of 0.5905, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 914/1000. Loss of 0.6058, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 915/1000. Loss of 0.6384, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 916/1000. Loss of 0.7162, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 917/1000. Loss of 0.6149, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 918/1000. Loss of 0.6036, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 919/1000. Loss of 0.6455, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 920/1000. Loss of 0.6760, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 921/1000. Loss of 0.5798, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 922/1000. Loss of 0.5888, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 923/1000. Loss of 0.6883, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 924/1000. Loss of 0.7382, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 925/1000. Loss of 0.5289, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 926/1000. Loss of 0.6224, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 927/1000. Loss of 0.7759, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 928/1000. Loss of 0.5705, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 929/1000. Loss of 0.6362, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 930/1000. Loss of 0.5240, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 931/1000. Loss of 0.6577, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 932/1000. Loss of 0.6836, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 933/1000. Loss of 0.5958, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 934/1000. Loss of 0.6611, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 935/1000. Loss of 0.6320, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 936/1000. Loss of 0.6064, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 937/1000. Loss of 0.6050, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 938/1000. Loss of 0.7271, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 939/1000. Loss of 0.5147, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 940/1000. Loss of 0.6368, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 941/1000. Loss of 0.7148, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 942/1000. Loss of 0.7224, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 943/1000. Loss of 0.6056, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 944/1000. Loss of 0.6653, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 945/1000. Loss of 0.6735, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 946/1000. Loss of 0.7054, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 947/1000. Loss of 0.7129, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 948/1000. Loss of 0.6857, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 949/1000. Loss of 0.6463, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 950/1000. Loss of 0.7271, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 951/1000. Loss of 0.6748, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 952/1000. Loss of 0.6228, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 953/1000. Loss of 0.6237, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 954/1000. Loss of 0.6420, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 955/1000. Loss of 0.6120, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 956/1000. Loss of 0.5891, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 957/1000. Loss of 0.6508, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 958/1000. Loss of 0.6814, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 959/1000. Loss of 0.6584, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 960/1000. Loss of 0.7365, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 961/1000. Loss of 0.6233, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 962/1000. Loss of 0.6341, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 963/1000. Loss of 0.6767, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 964/1000. Loss of 0.6951, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 965/1000. Loss of 0.6877, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 966/1000. Loss of 0.6038, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 967/1000. Loss of 0.6021, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 968/1000. Loss of 0.5707, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 969/1000. Loss of 0.7362, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 970/1000. Loss of 0.7260, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 971/1000. Loss of 0.6821, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 972/1000. Loss of 0.6898, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 973/1000. Loss of 0.6013, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 974/1000. Loss of 0.6433, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 975/1000. Loss of 0.6820, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 976/1000. Loss of 0.7027, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 977/1000. Loss of 0.6136, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 978/1000. Loss of 0.6631, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 979/1000. Loss of 0.6109, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 980/1000. Loss of 0.5555, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 981/1000. Loss of 0.5540, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 982/1000. Loss of 0.6727, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 983/1000. Loss of 0.6927, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 984/1000. Loss of 0.7557, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 985/1000. Loss of 0.6462, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 986/1000. Loss of 0.6331, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 987/1000. Loss of 0.7686, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 988/1000. Loss of 0.6006, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 989/1000. Loss of 0.6282, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 990/1000. Loss of 0.6706, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 991/1000. Loss of 0.6894, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 992/1000. Loss of 0.6478, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 993/1000. Loss of 0.7143, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 994/1000. Loss of 0.6453, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 995/1000. Loss of 0.6631, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 996/1000. Loss of 0.6234, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 997/1000. Loss of 0.6433, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 998/1000. Loss of 0.6698, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 999/1000. Loss of 0.6603, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 1000/1000. Loss of 0.5444, cover loss of 0.0002, secret loss of 0.0002\n",
            "Epoch [46/50], Average_loss: 0.6498\n",
            "Training: Batch 1/1000. Loss of 0.6319, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 2/1000. Loss of 0.6009, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 3/1000. Loss of 0.6766, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 4/1000. Loss of 0.6393, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 5/1000. Loss of 0.6034, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 6/1000. Loss of 0.6922, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 7/1000. Loss of 0.6361, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 8/1000. Loss of 0.6692, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 9/1000. Loss of 0.6820, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 10/1000. Loss of 0.7698, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 11/1000. Loss of 0.6401, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 12/1000. Loss of 0.5720, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 13/1000. Loss of 0.6399, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 14/1000. Loss of 0.7030, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 15/1000. Loss of 0.6843, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 16/1000. Loss of 0.6213, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 17/1000. Loss of 0.6292, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 18/1000. Loss of 0.7907, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 19/1000. Loss of 0.6745, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 20/1000. Loss of 0.6628, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 21/1000. Loss of 0.6803, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 22/1000. Loss of 0.6692, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 23/1000. Loss of 0.6130, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 24/1000. Loss of 0.6265, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 25/1000. Loss of 0.6483, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 26/1000. Loss of 0.5746, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 27/1000. Loss of 0.7247, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 28/1000. Loss of 0.6473, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 29/1000. Loss of 0.6932, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 30/1000. Loss of 0.6898, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 31/1000. Loss of 0.5450, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 32/1000. Loss of 0.6071, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 33/1000. Loss of 0.6266, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 34/1000. Loss of 0.6856, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 35/1000. Loss of 0.6436, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 36/1000. Loss of 0.5865, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 37/1000. Loss of 0.6208, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 38/1000. Loss of 0.6502, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 39/1000. Loss of 0.6457, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 40/1000. Loss of 0.5425, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 41/1000. Loss of 0.6156, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 42/1000. Loss of 0.7082, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 43/1000. Loss of 0.6774, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 44/1000. Loss of 0.6403, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 45/1000. Loss of 0.7939, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 46/1000. Loss of 0.7957, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 47/1000. Loss of 0.7206, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 48/1000. Loss of 0.6640, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 49/1000. Loss of 0.6313, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 50/1000. Loss of 0.6011, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 51/1000. Loss of 0.6599, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 52/1000. Loss of 0.6062, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 53/1000. Loss of 0.6438, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 54/1000. Loss of 0.5652, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 55/1000. Loss of 0.6038, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 56/1000. Loss of 0.6101, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 57/1000. Loss of 0.5816, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 58/1000. Loss of 0.6817, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 59/1000. Loss of 0.6426, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 60/1000. Loss of 0.6801, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 61/1000. Loss of 0.6871, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 62/1000. Loss of 0.5948, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 63/1000. Loss of 0.6067, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 64/1000. Loss of 0.6322, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 65/1000. Loss of 0.6984, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 66/1000. Loss of 0.6680, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 67/1000. Loss of 0.6988, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 68/1000. Loss of 0.5226, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 69/1000. Loss of 0.6118, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 70/1000. Loss of 0.7065, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 71/1000. Loss of 0.6269, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 72/1000. Loss of 0.5571, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 73/1000. Loss of 0.6061, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 74/1000. Loss of 0.5919, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 75/1000. Loss of 0.5777, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 76/1000. Loss of 0.6652, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 77/1000. Loss of 0.5883, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 78/1000. Loss of 0.6167, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 79/1000. Loss of 0.6680, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 80/1000. Loss of 0.7664, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 81/1000. Loss of 0.5881, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 82/1000. Loss of 0.6937, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 83/1000. Loss of 0.6631, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 84/1000. Loss of 0.7048, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 85/1000. Loss of 0.6616, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 86/1000. Loss of 0.6377, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 87/1000. Loss of 0.6335, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 88/1000. Loss of 0.6161, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 89/1000. Loss of 0.6420, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 90/1000. Loss of 0.6655, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 91/1000. Loss of 0.7279, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 92/1000. Loss of 0.6056, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 93/1000. Loss of 0.5971, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 94/1000. Loss of 0.6643, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 95/1000. Loss of 0.6548, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 96/1000. Loss of 0.6648, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 97/1000. Loss of 0.6993, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 98/1000. Loss of 0.5987, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 99/1000. Loss of 0.6501, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 100/1000. Loss of 0.5665, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 101/1000. Loss of 0.5692, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 102/1000. Loss of 0.6460, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 103/1000. Loss of 0.5476, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 104/1000. Loss of 0.5261, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 105/1000. Loss of 0.6292, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 106/1000. Loss of 0.6744, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 107/1000. Loss of 0.6723, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 108/1000. Loss of 0.7254, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 109/1000. Loss of 0.6380, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 110/1000. Loss of 0.6990, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 111/1000. Loss of 0.5837, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 112/1000. Loss of 0.7249, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 113/1000. Loss of 0.6324, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 114/1000. Loss of 0.6686, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 115/1000. Loss of 0.5968, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 116/1000. Loss of 0.5491, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 117/1000. Loss of 0.5999, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 118/1000. Loss of 0.6986, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 119/1000. Loss of 0.6666, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 120/1000. Loss of 0.6464, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 121/1000. Loss of 0.6193, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 122/1000. Loss of 0.6284, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 123/1000. Loss of 0.6524, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 124/1000. Loss of 0.6551, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 125/1000. Loss of 0.6433, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 126/1000. Loss of 0.6510, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 127/1000. Loss of 0.6962, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 128/1000. Loss of 0.5765, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 129/1000. Loss of 0.6782, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 130/1000. Loss of 0.7000, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 131/1000. Loss of 0.6014, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 132/1000. Loss of 0.7519, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 133/1000. Loss of 0.6079, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 134/1000. Loss of 0.6044, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 135/1000. Loss of 0.6347, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 136/1000. Loss of 0.6115, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 137/1000. Loss of 0.7238, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 138/1000. Loss of 0.6271, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 139/1000. Loss of 0.6491, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 140/1000. Loss of 0.7214, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 141/1000. Loss of 0.6617, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 142/1000. Loss of 0.7241, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 143/1000. Loss of 0.7261, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 144/1000. Loss of 0.6828, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 145/1000. Loss of 0.6491, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 146/1000. Loss of 0.6020, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 147/1000. Loss of 0.6556, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 148/1000. Loss of 0.7793, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 149/1000. Loss of 0.5669, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 150/1000. Loss of 0.6746, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 151/1000. Loss of 0.6756, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 152/1000. Loss of 0.7137, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 153/1000. Loss of 0.6672, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 154/1000. Loss of 0.6847, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 155/1000. Loss of 0.6527, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 156/1000. Loss of 0.6010, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 157/1000. Loss of 0.6805, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 158/1000. Loss of 0.7421, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 159/1000. Loss of 0.6350, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 160/1000. Loss of 0.6648, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 161/1000. Loss of 0.6288, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 162/1000. Loss of 0.6470, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 163/1000. Loss of 0.6070, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 164/1000. Loss of 0.5968, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 165/1000. Loss of 0.5870, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 166/1000. Loss of 0.6179, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 167/1000. Loss of 0.7336, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 168/1000. Loss of 0.5933, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 169/1000. Loss of 0.6285, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 170/1000. Loss of 0.6064, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 171/1000. Loss of 0.6999, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 172/1000. Loss of 0.6459, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 173/1000. Loss of 0.6176, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 174/1000. Loss of 0.6401, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 175/1000. Loss of 0.6229, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 176/1000. Loss of 0.6605, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 177/1000. Loss of 0.6518, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 178/1000. Loss of 0.6561, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 179/1000. Loss of 0.6906, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 180/1000. Loss of 0.7136, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 181/1000. Loss of 0.6137, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 182/1000. Loss of 0.5673, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 183/1000. Loss of 0.6871, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 184/1000. Loss of 0.6114, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 185/1000. Loss of 0.6966, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 186/1000. Loss of 0.6962, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 187/1000. Loss of 0.6066, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 188/1000. Loss of 0.5813, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 189/1000. Loss of 0.5624, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 190/1000. Loss of 0.6442, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 191/1000. Loss of 0.7405, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 192/1000. Loss of 0.7747, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 193/1000. Loss of 0.6375, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 194/1000. Loss of 0.6240, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 195/1000. Loss of 0.6159, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 196/1000. Loss of 0.6798, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 197/1000. Loss of 0.5653, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 198/1000. Loss of 0.7807, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 199/1000. Loss of 0.7112, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 200/1000. Loss of 0.5847, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 201/1000. Loss of 0.7557, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 202/1000. Loss of 0.5815, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 203/1000. Loss of 0.6408, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 204/1000. Loss of 0.6723, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 205/1000. Loss of 0.5905, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 206/1000. Loss of 0.6899, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 207/1000. Loss of 0.6006, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 208/1000. Loss of 0.6498, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 209/1000. Loss of 0.6517, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 210/1000. Loss of 0.5721, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 211/1000. Loss of 0.5716, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 212/1000. Loss of 0.5732, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 213/1000. Loss of 0.5833, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 214/1000. Loss of 0.6020, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 215/1000. Loss of 0.6507, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 216/1000. Loss of 0.6069, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 217/1000. Loss of 0.6054, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 218/1000. Loss of 0.6341, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 219/1000. Loss of 0.8122, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 220/1000. Loss of 0.6281, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 221/1000. Loss of 0.6132, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 222/1000. Loss of 0.6391, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 223/1000. Loss of 0.6157, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 224/1000. Loss of 0.5892, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 225/1000. Loss of 0.5929, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 226/1000. Loss of 0.6808, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 227/1000. Loss of 0.6870, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 228/1000. Loss of 0.6624, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 229/1000. Loss of 0.6145, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 230/1000. Loss of 0.6688, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 231/1000. Loss of 0.6221, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 232/1000. Loss of 0.5914, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 233/1000. Loss of 0.6313, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 234/1000. Loss of 0.6039, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 235/1000. Loss of 0.5513, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 236/1000. Loss of 0.6188, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 237/1000. Loss of 0.6350, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 238/1000. Loss of 0.6370, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 239/1000. Loss of 0.5815, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 240/1000. Loss of 0.7420, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 241/1000. Loss of 0.6832, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 242/1000. Loss of 0.6459, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 243/1000. Loss of 0.7227, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 244/1000. Loss of 0.6267, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 245/1000. Loss of 0.6720, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 246/1000. Loss of 0.7117, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 247/1000. Loss of 0.6961, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 248/1000. Loss of 0.6141, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 249/1000. Loss of 0.6277, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 250/1000. Loss of 0.5849, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 251/1000. Loss of 0.6770, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 252/1000. Loss of 0.7269, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 253/1000. Loss of 0.5754, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 254/1000. Loss of 0.7188, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 255/1000. Loss of 0.7029, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 256/1000. Loss of 0.6731, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 257/1000. Loss of 0.6606, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 258/1000. Loss of 0.5931, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 259/1000. Loss of 0.6774, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 260/1000. Loss of 0.6802, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 261/1000. Loss of 0.7201, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 262/1000. Loss of 0.6354, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 263/1000. Loss of 0.7295, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 264/1000. Loss of 0.6523, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 265/1000. Loss of 0.5530, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 266/1000. Loss of 0.5912, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 267/1000. Loss of 0.7143, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 268/1000. Loss of 0.6889, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 269/1000. Loss of 0.5026, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 270/1000. Loss of 0.6575, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 271/1000. Loss of 0.6458, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 272/1000. Loss of 0.5852, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 273/1000. Loss of 0.6937, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 274/1000. Loss of 0.5779, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 275/1000. Loss of 0.6133, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 276/1000. Loss of 0.6117, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 277/1000. Loss of 0.5703, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 278/1000. Loss of 0.6441, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 279/1000. Loss of 0.7268, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 280/1000. Loss of 0.6050, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 281/1000. Loss of 0.6450, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 282/1000. Loss of 0.5971, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 283/1000. Loss of 0.7192, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 284/1000. Loss of 0.5402, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 285/1000. Loss of 0.5797, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 286/1000. Loss of 0.6154, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 287/1000. Loss of 0.7528, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 288/1000. Loss of 0.6830, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 289/1000. Loss of 0.6498, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 290/1000. Loss of 0.6088, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 291/1000. Loss of 0.6130, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 292/1000. Loss of 0.6717, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 293/1000. Loss of 0.6730, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 294/1000. Loss of 0.6142, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 295/1000. Loss of 0.7275, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 296/1000. Loss of 0.7207, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 297/1000. Loss of 0.6894, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 298/1000. Loss of 0.6161, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 299/1000. Loss of 0.5992, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 300/1000. Loss of 0.6494, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 301/1000. Loss of 0.5741, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 302/1000. Loss of 0.5468, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 303/1000. Loss of 0.6816, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 304/1000. Loss of 0.6098, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 305/1000. Loss of 0.5500, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 306/1000. Loss of 0.5493, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 307/1000. Loss of 0.7637, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 308/1000. Loss of 0.6545, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 309/1000. Loss of 0.7374, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 310/1000. Loss of 0.5651, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 311/1000. Loss of 0.6069, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 312/1000. Loss of 0.6381, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 313/1000. Loss of 0.6746, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 314/1000. Loss of 0.6725, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 315/1000. Loss of 0.7754, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 316/1000. Loss of 0.7300, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 317/1000. Loss of 0.6129, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 318/1000. Loss of 0.5595, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 319/1000. Loss of 0.6948, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 320/1000. Loss of 0.6095, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 321/1000. Loss of 0.5580, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 322/1000. Loss of 0.6535, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 323/1000. Loss of 0.6900, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 324/1000. Loss of 0.6785, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 325/1000. Loss of 0.5601, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 326/1000. Loss of 0.5899, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 327/1000. Loss of 0.5955, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 328/1000. Loss of 0.5998, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 329/1000. Loss of 0.8182, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 330/1000. Loss of 0.7053, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 331/1000. Loss of 0.6491, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 332/1000. Loss of 0.7025, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 333/1000. Loss of 0.6729, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 334/1000. Loss of 0.6503, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 335/1000. Loss of 0.6329, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 336/1000. Loss of 0.6535, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 337/1000. Loss of 0.6562, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 338/1000. Loss of 0.5935, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 339/1000. Loss of 0.6727, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 340/1000. Loss of 0.6270, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 341/1000. Loss of 0.6516, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 342/1000. Loss of 0.7890, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 343/1000. Loss of 0.6632, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 344/1000. Loss of 0.6790, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 345/1000. Loss of 0.6902, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 346/1000. Loss of 0.6222, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 347/1000. Loss of 0.7027, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 348/1000. Loss of 0.6404, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 349/1000. Loss of 0.6474, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 350/1000. Loss of 0.6017, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 351/1000. Loss of 0.7388, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 352/1000. Loss of 0.7600, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 353/1000. Loss of 0.7329, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 354/1000. Loss of 0.6934, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 355/1000. Loss of 0.6831, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 356/1000. Loss of 0.7044, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 357/1000. Loss of 0.5482, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 358/1000. Loss of 0.6992, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 359/1000. Loss of 0.5628, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 360/1000. Loss of 0.6519, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 361/1000. Loss of 0.7612, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 362/1000. Loss of 0.6328, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 363/1000. Loss of 0.6721, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 364/1000. Loss of 0.6160, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 365/1000. Loss of 0.6859, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 366/1000. Loss of 0.6355, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 367/1000. Loss of 0.6664, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 368/1000. Loss of 0.5417, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 369/1000. Loss of 0.6632, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 370/1000. Loss of 0.6264, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 371/1000. Loss of 0.5704, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 372/1000. Loss of 0.6742, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 373/1000. Loss of 0.6704, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 374/1000. Loss of 0.6073, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 375/1000. Loss of 0.5454, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 376/1000. Loss of 0.6226, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 377/1000. Loss of 0.6821, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 378/1000. Loss of 0.5969, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 379/1000. Loss of 0.6990, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 380/1000. Loss of 0.6986, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 381/1000. Loss of 0.6183, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 382/1000. Loss of 0.6797, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 383/1000. Loss of 0.6415, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 384/1000. Loss of 0.6688, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 385/1000. Loss of 0.6479, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 386/1000. Loss of 0.7447, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 387/1000. Loss of 0.7838, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 388/1000. Loss of 0.6404, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 389/1000. Loss of 0.7158, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 390/1000. Loss of 0.5851, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 391/1000. Loss of 0.7641, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 392/1000. Loss of 0.6104, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 393/1000. Loss of 0.7270, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 394/1000. Loss of 0.6788, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 395/1000. Loss of 0.6598, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 396/1000. Loss of 0.5752, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 397/1000. Loss of 0.6704, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 398/1000. Loss of 0.6298, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 399/1000. Loss of 0.7320, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 400/1000. Loss of 0.6451, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 401/1000. Loss of 0.5751, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 402/1000. Loss of 0.6747, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 403/1000. Loss of 0.6561, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 404/1000. Loss of 0.6305, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 405/1000. Loss of 0.5565, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 406/1000. Loss of 0.5855, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 407/1000. Loss of 0.6210, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 408/1000. Loss of 0.6930, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 409/1000. Loss of 0.5974, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 410/1000. Loss of 0.6098, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 411/1000. Loss of 0.6578, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 412/1000. Loss of 0.6663, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 413/1000. Loss of 0.6185, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 414/1000. Loss of 0.6927, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 415/1000. Loss of 0.6292, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 416/1000. Loss of 0.6292, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 417/1000. Loss of 0.6607, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 418/1000. Loss of 0.6956, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 419/1000. Loss of 0.6067, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 420/1000. Loss of 0.6310, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 421/1000. Loss of 0.7104, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 422/1000. Loss of 0.8711, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 423/1000. Loss of 0.6381, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 424/1000. Loss of 0.7034, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 425/1000. Loss of 0.6087, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 426/1000. Loss of 0.6384, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 427/1000. Loss of 0.6748, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 428/1000. Loss of 0.6554, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 429/1000. Loss of 0.6664, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 430/1000. Loss of 0.7137, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 431/1000. Loss of 0.6999, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 432/1000. Loss of 0.6429, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 433/1000. Loss of 0.6114, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 434/1000. Loss of 0.6073, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 435/1000. Loss of 0.7403, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 436/1000. Loss of 0.7209, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 437/1000. Loss of 0.7394, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 438/1000. Loss of 0.7400, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 439/1000. Loss of 0.5866, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 440/1000. Loss of 0.7206, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 441/1000. Loss of 0.6655, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 442/1000. Loss of 0.5981, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 443/1000. Loss of 0.6030, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 444/1000. Loss of 0.5750, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 445/1000. Loss of 0.6842, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 446/1000. Loss of 0.7023, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 447/1000. Loss of 0.6029, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 448/1000. Loss of 0.6063, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 449/1000. Loss of 0.6804, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 450/1000. Loss of 0.6875, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 451/1000. Loss of 0.5958, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 452/1000. Loss of 0.7223, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 453/1000. Loss of 0.7021, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 454/1000. Loss of 0.5726, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 455/1000. Loss of 0.7263, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 456/1000. Loss of 0.6426, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 457/1000. Loss of 0.7708, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 458/1000. Loss of 0.6723, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 459/1000. Loss of 0.6027, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 460/1000. Loss of 0.6268, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 461/1000. Loss of 0.6805, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 462/1000. Loss of 0.6900, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 463/1000. Loss of 0.6465, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 464/1000. Loss of 0.6255, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 465/1000. Loss of 0.6884, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 466/1000. Loss of 0.5502, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 467/1000. Loss of 0.7205, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 468/1000. Loss of 0.6495, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 469/1000. Loss of 0.7192, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 470/1000. Loss of 0.6420, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 471/1000. Loss of 0.6302, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 472/1000. Loss of 0.6983, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 473/1000. Loss of 0.7026, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 474/1000. Loss of 0.6699, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 475/1000. Loss of 0.6167, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 476/1000. Loss of 0.7182, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 477/1000. Loss of 0.5974, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 478/1000. Loss of 0.5979, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 479/1000. Loss of 0.6734, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 480/1000. Loss of 0.5600, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 481/1000. Loss of 0.6844, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 482/1000. Loss of 0.6170, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 483/1000. Loss of 0.5851, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 484/1000. Loss of 0.6796, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 485/1000. Loss of 0.6072, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 486/1000. Loss of 0.6364, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 487/1000. Loss of 0.6089, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 488/1000. Loss of 0.5715, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 489/1000. Loss of 0.6442, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 490/1000. Loss of 0.5925, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 491/1000. Loss of 0.5875, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 492/1000. Loss of 0.6913, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 493/1000. Loss of 0.6095, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 494/1000. Loss of 0.5863, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 495/1000. Loss of 0.6985, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 496/1000. Loss of 0.6843, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 497/1000. Loss of 0.5792, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 498/1000. Loss of 0.6368, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 499/1000. Loss of 0.7328, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 500/1000. Loss of 0.6258, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 501/1000. Loss of 0.5858, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 502/1000. Loss of 0.6890, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 503/1000. Loss of 0.6433, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 504/1000. Loss of 0.5585, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 505/1000. Loss of 0.6833, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 506/1000. Loss of 0.6445, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 507/1000. Loss of 0.7155, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 508/1000. Loss of 0.6904, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 509/1000. Loss of 0.4942, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 510/1000. Loss of 0.7107, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 511/1000. Loss of 0.6481, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 512/1000. Loss of 0.7005, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 513/1000. Loss of 0.7050, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 514/1000. Loss of 0.6543, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 515/1000. Loss of 0.6295, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 516/1000. Loss of 0.6857, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 517/1000. Loss of 0.5742, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 518/1000. Loss of 0.5756, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 519/1000. Loss of 0.7532, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 520/1000. Loss of 0.6341, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 521/1000. Loss of 0.6208, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 522/1000. Loss of 0.6587, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 523/1000. Loss of 0.6557, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 524/1000. Loss of 0.6527, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 525/1000. Loss of 0.6638, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 526/1000. Loss of 0.7326, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 527/1000. Loss of 0.6717, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 528/1000. Loss of 0.6258, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 529/1000. Loss of 0.5606, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 530/1000. Loss of 0.6069, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 531/1000. Loss of 0.6028, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 532/1000. Loss of 0.6658, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 533/1000. Loss of 0.6075, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 534/1000. Loss of 0.6659, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 535/1000. Loss of 0.5942, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 536/1000. Loss of 0.6143, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 537/1000. Loss of 0.6948, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 538/1000. Loss of 0.6341, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 539/1000. Loss of 0.7328, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 540/1000. Loss of 0.7256, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 541/1000. Loss of 0.5290, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 542/1000. Loss of 0.6878, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 543/1000. Loss of 0.6372, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 544/1000. Loss of 0.6244, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 545/1000. Loss of 0.6614, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 546/1000. Loss of 0.6862, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 547/1000. Loss of 0.7779, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 548/1000. Loss of 0.7289, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 549/1000. Loss of 0.6798, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 550/1000. Loss of 0.6452, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 551/1000. Loss of 0.6643, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 552/1000. Loss of 0.7064, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 553/1000. Loss of 0.7180, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 554/1000. Loss of 0.6056, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 555/1000. Loss of 0.6419, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 556/1000. Loss of 0.7564, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 557/1000. Loss of 0.7623, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 558/1000. Loss of 0.6613, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 559/1000. Loss of 0.4918, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 560/1000. Loss of 0.7207, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 561/1000. Loss of 0.6056, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 562/1000. Loss of 0.7256, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 563/1000. Loss of 0.6998, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 564/1000. Loss of 0.7933, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 565/1000. Loss of 0.5581, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 566/1000. Loss of 0.5941, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 567/1000. Loss of 0.7226, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 568/1000. Loss of 0.6094, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 569/1000. Loss of 0.7018, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 570/1000. Loss of 0.6261, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 571/1000. Loss of 0.5978, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 572/1000. Loss of 0.5588, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 573/1000. Loss of 0.6745, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 574/1000. Loss of 0.5879, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 575/1000. Loss of 0.7302, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 576/1000. Loss of 0.6238, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 577/1000. Loss of 0.5913, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 578/1000. Loss of 0.5484, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 579/1000. Loss of 0.6080, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 580/1000. Loss of 0.7057, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 581/1000. Loss of 0.6275, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 582/1000. Loss of 0.6421, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 583/1000. Loss of 0.6123, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 584/1000. Loss of 0.6810, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 585/1000. Loss of 0.7186, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 586/1000. Loss of 0.5974, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 587/1000. Loss of 0.6946, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 588/1000. Loss of 0.7315, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 589/1000. Loss of 0.6943, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 590/1000. Loss of 0.6266, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 591/1000. Loss of 0.6071, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 592/1000. Loss of 0.6554, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 593/1000. Loss of 0.5946, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 594/1000. Loss of 0.5696, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 595/1000. Loss of 0.6594, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 596/1000. Loss of 0.6911, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 597/1000. Loss of 0.6201, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 598/1000. Loss of 0.6097, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 599/1000. Loss of 0.6280, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 600/1000. Loss of 0.7659, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 601/1000. Loss of 0.7094, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 602/1000. Loss of 0.7783, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 603/1000. Loss of 0.6918, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 604/1000. Loss of 0.6203, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 605/1000. Loss of 0.6824, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 606/1000. Loss of 0.6664, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 607/1000. Loss of 0.6047, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 608/1000. Loss of 0.7271, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 609/1000. Loss of 0.7386, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 610/1000. Loss of 0.6335, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 611/1000. Loss of 0.5991, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 612/1000. Loss of 0.6276, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 613/1000. Loss of 0.6059, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 614/1000. Loss of 0.6285, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 615/1000. Loss of 0.6797, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 616/1000. Loss of 0.6017, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 617/1000. Loss of 0.7415, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 618/1000. Loss of 0.7228, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 619/1000. Loss of 0.5437, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 620/1000. Loss of 0.7243, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 621/1000. Loss of 0.6741, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 622/1000. Loss of 0.6458, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 623/1000. Loss of 0.6169, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 624/1000. Loss of 0.6925, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 625/1000. Loss of 0.7087, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 626/1000. Loss of 0.6270, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 627/1000. Loss of 0.6871, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 628/1000. Loss of 0.6969, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 629/1000. Loss of 0.5680, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 630/1000. Loss of 0.5516, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 631/1000. Loss of 0.6739, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 632/1000. Loss of 0.6472, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 633/1000. Loss of 0.7599, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 634/1000. Loss of 0.7306, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 635/1000. Loss of 0.6119, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 636/1000. Loss of 0.5635, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 637/1000. Loss of 0.6850, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 638/1000. Loss of 0.6956, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 639/1000. Loss of 0.7191, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 640/1000. Loss of 0.6168, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 641/1000. Loss of 0.6628, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 642/1000. Loss of 0.6820, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 643/1000. Loss of 0.6338, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 644/1000. Loss of 0.7652, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 645/1000. Loss of 0.5531, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 646/1000. Loss of 0.6668, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 647/1000. Loss of 0.6314, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 648/1000. Loss of 0.6965, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 649/1000. Loss of 0.6415, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 650/1000. Loss of 0.7036, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 651/1000. Loss of 0.6217, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 652/1000. Loss of 0.5915, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 653/1000. Loss of 0.6070, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 654/1000. Loss of 0.5690, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 655/1000. Loss of 0.5580, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 656/1000. Loss of 0.6812, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 657/1000. Loss of 0.5997, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 658/1000. Loss of 0.7266, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 659/1000. Loss of 0.5828, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 660/1000. Loss of 0.6426, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 661/1000. Loss of 0.7104, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 662/1000. Loss of 0.5826, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 663/1000. Loss of 0.7181, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 664/1000. Loss of 0.6059, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 665/1000. Loss of 0.5889, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 666/1000. Loss of 0.6173, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 667/1000. Loss of 0.6688, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 668/1000. Loss of 0.5730, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 669/1000. Loss of 0.7174, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 670/1000. Loss of 0.5934, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 671/1000. Loss of 0.7160, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 672/1000. Loss of 0.6369, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 673/1000. Loss of 0.6123, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 674/1000. Loss of 0.6613, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 675/1000. Loss of 0.5970, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 676/1000. Loss of 0.6373, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 677/1000. Loss of 0.5833, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 678/1000. Loss of 0.6539, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 679/1000. Loss of 0.6679, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 680/1000. Loss of 0.7244, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 681/1000. Loss of 0.5958, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 682/1000. Loss of 0.6478, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 683/1000. Loss of 0.6234, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 684/1000. Loss of 0.6554, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 685/1000. Loss of 0.5999, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 686/1000. Loss of 0.6894, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 687/1000. Loss of 0.5694, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 688/1000. Loss of 0.5477, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 689/1000. Loss of 0.6664, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 690/1000. Loss of 0.7266, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 691/1000. Loss of 0.6663, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 692/1000. Loss of 0.7594, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 693/1000. Loss of 0.6094, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 694/1000. Loss of 0.7002, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 695/1000. Loss of 0.6418, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 696/1000. Loss of 0.6510, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 697/1000. Loss of 0.7034, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 698/1000. Loss of 0.5724, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 699/1000. Loss of 0.6296, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 700/1000. Loss of 0.6672, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 701/1000. Loss of 0.7869, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 702/1000. Loss of 0.5551, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 703/1000. Loss of 0.6075, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 704/1000. Loss of 0.5800, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 705/1000. Loss of 0.6774, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 706/1000. Loss of 0.6606, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 707/1000. Loss of 0.7504, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 708/1000. Loss of 0.5599, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 709/1000. Loss of 0.5828, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 710/1000. Loss of 0.7293, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 711/1000. Loss of 0.6261, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 712/1000. Loss of 0.6792, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 713/1000. Loss of 0.6637, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 714/1000. Loss of 0.5477, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 715/1000. Loss of 0.5612, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 716/1000. Loss of 0.6057, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 717/1000. Loss of 0.5820, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 718/1000. Loss of 0.7237, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 719/1000. Loss of 0.6730, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 720/1000. Loss of 0.6266, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 721/1000. Loss of 0.7203, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 722/1000. Loss of 0.6265, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 723/1000. Loss of 0.6789, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 724/1000. Loss of 0.6392, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 725/1000. Loss of 0.6823, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 726/1000. Loss of 0.6644, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 727/1000. Loss of 0.6306, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 728/1000. Loss of 0.6389, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 729/1000. Loss of 0.6524, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 730/1000. Loss of 0.7470, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 731/1000. Loss of 0.5837, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 732/1000. Loss of 0.6310, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 733/1000. Loss of 0.6757, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 734/1000. Loss of 0.7004, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 735/1000. Loss of 0.7099, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 736/1000. Loss of 0.7618, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 737/1000. Loss of 0.6421, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 738/1000. Loss of 0.6200, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 739/1000. Loss of 0.5504, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 740/1000. Loss of 0.6183, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 741/1000. Loss of 0.6142, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 742/1000. Loss of 0.7079, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 743/1000. Loss of 0.6613, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 744/1000. Loss of 0.6170, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 745/1000. Loss of 0.6999, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 746/1000. Loss of 0.7088, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 747/1000. Loss of 0.7030, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 748/1000. Loss of 0.6981, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 749/1000. Loss of 0.7120, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 750/1000. Loss of 0.6247, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 751/1000. Loss of 0.6243, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 752/1000. Loss of 0.6507, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 753/1000. Loss of 0.5688, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 754/1000. Loss of 0.5858, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 755/1000. Loss of 0.7206, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 756/1000. Loss of 0.6019, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 757/1000. Loss of 0.5997, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 758/1000. Loss of 0.6807, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 759/1000. Loss of 0.6547, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 760/1000. Loss of 0.6795, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 761/1000. Loss of 0.5846, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 762/1000. Loss of 0.6678, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 763/1000. Loss of 0.6741, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 764/1000. Loss of 0.6227, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 765/1000. Loss of 0.6093, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 766/1000. Loss of 0.6403, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 767/1000. Loss of 0.5929, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 768/1000. Loss of 0.7672, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 769/1000. Loss of 0.7345, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 770/1000. Loss of 0.6344, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 771/1000. Loss of 0.6206, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 772/1000. Loss of 0.7557, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 773/1000. Loss of 0.6446, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 774/1000. Loss of 0.6320, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 775/1000. Loss of 0.7338, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 776/1000. Loss of 0.6543, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 777/1000. Loss of 0.7126, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 778/1000. Loss of 0.6477, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 779/1000. Loss of 0.6077, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 780/1000. Loss of 0.5936, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 781/1000. Loss of 0.6842, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 782/1000. Loss of 0.6225, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 783/1000. Loss of 0.6583, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 784/1000. Loss of 0.6447, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 785/1000. Loss of 0.7156, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 786/1000. Loss of 0.6898, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 787/1000. Loss of 0.5579, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 788/1000. Loss of 0.6289, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 789/1000. Loss of 0.6408, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 790/1000. Loss of 0.6024, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 791/1000. Loss of 0.7700, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 792/1000. Loss of 0.6928, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 793/1000. Loss of 0.6620, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 794/1000. Loss of 0.6473, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 795/1000. Loss of 0.6839, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 796/1000. Loss of 0.5836, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 797/1000. Loss of 0.6492, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 798/1000. Loss of 0.6407, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 799/1000. Loss of 0.6769, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 800/1000. Loss of 0.5881, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 801/1000. Loss of 0.6880, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 802/1000. Loss of 0.7904, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 803/1000. Loss of 0.6837, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 804/1000. Loss of 0.5878, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 805/1000. Loss of 0.5542, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 806/1000. Loss of 0.6823, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 807/1000. Loss of 0.6797, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 808/1000. Loss of 0.6490, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 809/1000. Loss of 0.7035, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 810/1000. Loss of 0.7253, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 811/1000. Loss of 0.7320, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 812/1000. Loss of 0.5944, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 813/1000. Loss of 0.7609, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 814/1000. Loss of 0.6423, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 815/1000. Loss of 0.6345, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 816/1000. Loss of 0.6958, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 817/1000. Loss of 0.6738, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 818/1000. Loss of 0.7422, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 819/1000. Loss of 0.6201, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 820/1000. Loss of 0.6712, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 821/1000. Loss of 0.7319, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 822/1000. Loss of 0.6427, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 823/1000. Loss of 0.6949, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 824/1000. Loss of 0.5148, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 825/1000. Loss of 0.6409, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 826/1000. Loss of 0.6701, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 827/1000. Loss of 0.6721, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 828/1000. Loss of 0.6123, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 829/1000. Loss of 0.7471, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 830/1000. Loss of 0.6057, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 831/1000. Loss of 0.6477, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 832/1000. Loss of 0.6951, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 833/1000. Loss of 0.7089, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 834/1000. Loss of 0.5590, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 835/1000. Loss of 0.7086, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 836/1000. Loss of 0.6647, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 837/1000. Loss of 0.5370, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 838/1000. Loss of 0.7344, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 839/1000. Loss of 0.6459, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 840/1000. Loss of 0.6148, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 841/1000. Loss of 0.6710, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 842/1000. Loss of 0.5774, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 843/1000. Loss of 0.6014, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 844/1000. Loss of 0.6219, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 845/1000. Loss of 0.7279, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 846/1000. Loss of 0.6841, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 847/1000. Loss of 0.5938, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 848/1000. Loss of 0.7122, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 849/1000. Loss of 0.5722, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 850/1000. Loss of 0.5309, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 851/1000. Loss of 0.6483, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 852/1000. Loss of 0.6936, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 853/1000. Loss of 0.5682, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 854/1000. Loss of 0.6385, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 855/1000. Loss of 0.6652, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 856/1000. Loss of 0.5357, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 857/1000. Loss of 0.6013, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 858/1000. Loss of 0.6863, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 859/1000. Loss of 0.7318, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 860/1000. Loss of 0.7693, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 861/1000. Loss of 0.6253, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 862/1000. Loss of 0.5383, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 863/1000. Loss of 0.6491, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 864/1000. Loss of 0.7043, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 865/1000. Loss of 0.5950, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 866/1000. Loss of 0.6007, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 867/1000. Loss of 0.6469, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 868/1000. Loss of 0.5915, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 869/1000. Loss of 0.6697, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 870/1000. Loss of 0.7113, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 871/1000. Loss of 0.5958, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 872/1000. Loss of 0.4748, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 873/1000. Loss of 0.5905, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 874/1000. Loss of 0.7239, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 875/1000. Loss of 0.6872, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 876/1000. Loss of 0.6268, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 877/1000. Loss of 0.5869, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 878/1000. Loss of 0.6488, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 879/1000. Loss of 0.6179, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 880/1000. Loss of 0.7000, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 881/1000. Loss of 0.6803, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 882/1000. Loss of 0.5786, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 883/1000. Loss of 0.5720, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 884/1000. Loss of 0.5969, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 885/1000. Loss of 0.7621, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 886/1000. Loss of 0.6847, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 887/1000. Loss of 0.5743, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 888/1000. Loss of 0.7106, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 889/1000. Loss of 0.7433, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 890/1000. Loss of 0.7510, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 891/1000. Loss of 0.7246, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 892/1000. Loss of 0.5569, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 893/1000. Loss of 0.6910, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 894/1000. Loss of 0.7114, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 895/1000. Loss of 0.7254, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 896/1000. Loss of 0.5389, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 897/1000. Loss of 0.5576, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 898/1000. Loss of 0.6859, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 899/1000. Loss of 0.7104, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 900/1000. Loss of 0.6562, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 901/1000. Loss of 0.6862, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 902/1000. Loss of 0.5708, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 903/1000. Loss of 0.6575, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 904/1000. Loss of 0.5893, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 905/1000. Loss of 0.6030, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 906/1000. Loss of 0.5652, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 907/1000. Loss of 0.6742, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 908/1000. Loss of 0.6470, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 909/1000. Loss of 0.5811, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 910/1000. Loss of 0.7732, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 911/1000. Loss of 0.5528, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 912/1000. Loss of 0.6716, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 913/1000. Loss of 0.6046, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 914/1000. Loss of 0.6292, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 915/1000. Loss of 0.5888, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 916/1000. Loss of 0.6031, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 917/1000. Loss of 0.6112, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 918/1000. Loss of 0.7084, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 919/1000. Loss of 0.6542, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 920/1000. Loss of 0.6675, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 921/1000. Loss of 0.6647, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 922/1000. Loss of 0.5832, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 923/1000. Loss of 0.8137, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 924/1000. Loss of 0.5681, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 925/1000. Loss of 0.6090, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 926/1000. Loss of 0.6568, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 927/1000. Loss of 0.6497, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 928/1000. Loss of 0.6820, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 929/1000. Loss of 0.7381, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 930/1000. Loss of 0.6432, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 931/1000. Loss of 0.5812, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 932/1000. Loss of 0.6053, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 933/1000. Loss of 0.7076, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 934/1000. Loss of 0.5946, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 935/1000. Loss of 0.7136, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 936/1000. Loss of 0.6374, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 937/1000. Loss of 0.5565, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 938/1000. Loss of 0.6289, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 939/1000. Loss of 0.6400, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 940/1000. Loss of 0.6520, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 941/1000. Loss of 0.6441, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 942/1000. Loss of 0.6356, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 943/1000. Loss of 0.6203, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 944/1000. Loss of 0.5782, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 945/1000. Loss of 0.7855, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 946/1000. Loss of 0.6231, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 947/1000. Loss of 0.6613, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 948/1000. Loss of 0.6223, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 949/1000. Loss of 0.6209, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 950/1000. Loss of 0.6145, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 951/1000. Loss of 0.6824, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 952/1000. Loss of 0.5752, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 953/1000. Loss of 0.6045, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 954/1000. Loss of 0.6316, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 955/1000. Loss of 0.6681, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 956/1000. Loss of 0.5237, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 957/1000. Loss of 0.6602, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 958/1000. Loss of 0.5596, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 959/1000. Loss of 0.6770, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 960/1000. Loss of 0.6560, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 961/1000. Loss of 0.5926, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 962/1000. Loss of 0.6521, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 963/1000. Loss of 0.7575, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 964/1000. Loss of 0.5876, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 965/1000. Loss of 0.7887, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 966/1000. Loss of 0.6386, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 967/1000. Loss of 0.6324, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 968/1000. Loss of 0.7286, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 969/1000. Loss of 0.6079, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 970/1000. Loss of 0.6752, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 971/1000. Loss of 0.7266, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 972/1000. Loss of 0.6964, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 973/1000. Loss of 0.6476, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 974/1000. Loss of 0.6299, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 975/1000. Loss of 0.6701, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 976/1000. Loss of 0.6160, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 977/1000. Loss of 0.6188, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 978/1000. Loss of 0.5940, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 979/1000. Loss of 0.5867, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 980/1000. Loss of 0.6145, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 981/1000. Loss of 0.6087, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 982/1000. Loss of 0.7037, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 983/1000. Loss of 0.5908, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 984/1000. Loss of 0.7361, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 985/1000. Loss of 0.6119, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 986/1000. Loss of 0.5791, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 987/1000. Loss of 0.6154, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 988/1000. Loss of 0.7369, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 989/1000. Loss of 0.6159, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 990/1000. Loss of 0.6694, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 991/1000. Loss of 0.6260, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 992/1000. Loss of 0.6172, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 993/1000. Loss of 0.7349, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 994/1000. Loss of 0.6002, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 995/1000. Loss of 0.7243, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 996/1000. Loss of 0.5833, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 997/1000. Loss of 0.6231, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 998/1000. Loss of 0.6551, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 999/1000. Loss of 0.7054, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 1000/1000. Loss of 0.6726, cover loss of 0.0002, secret loss of 0.0002\n",
            "Epoch [47/50], Average_loss: 0.6496\n",
            "Training: Batch 1/1000. Loss of 0.6925, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 2/1000. Loss of 0.5890, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 3/1000. Loss of 0.5926, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 4/1000. Loss of 0.6052, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 5/1000. Loss of 0.5763, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 6/1000. Loss of 0.7625, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 7/1000. Loss of 0.5579, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 8/1000. Loss of 0.6649, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 9/1000. Loss of 0.6669, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 10/1000. Loss of 0.6941, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 11/1000. Loss of 0.6042, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 12/1000. Loss of 0.5765, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 13/1000. Loss of 0.6157, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 14/1000. Loss of 0.7225, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 15/1000. Loss of 0.6068, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 16/1000. Loss of 0.6067, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 17/1000. Loss of 0.6762, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 18/1000. Loss of 0.6707, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 19/1000. Loss of 0.5942, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 20/1000. Loss of 0.5472, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 21/1000. Loss of 0.6165, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 22/1000. Loss of 0.6382, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 23/1000. Loss of 0.7125, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 24/1000. Loss of 0.6736, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 25/1000. Loss of 0.7515, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 26/1000. Loss of 0.6589, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 27/1000. Loss of 0.6277, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 28/1000. Loss of 0.6542, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 29/1000. Loss of 0.6057, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 30/1000. Loss of 0.6230, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 31/1000. Loss of 0.5746, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 32/1000. Loss of 0.7002, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 33/1000. Loss of 0.6131, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 34/1000. Loss of 0.7068, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 35/1000. Loss of 0.7693, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 36/1000. Loss of 0.6582, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 37/1000. Loss of 0.6405, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 38/1000. Loss of 0.6428, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 39/1000. Loss of 0.5753, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 40/1000. Loss of 0.6368, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 41/1000. Loss of 0.6776, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 42/1000. Loss of 0.7245, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 43/1000. Loss of 0.6675, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 44/1000. Loss of 0.6387, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 45/1000. Loss of 0.6594, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 46/1000. Loss of 0.6908, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 47/1000. Loss of 0.7189, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 48/1000. Loss of 0.6296, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 49/1000. Loss of 0.6540, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 50/1000. Loss of 0.7089, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 51/1000. Loss of 0.6833, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 52/1000. Loss of 0.6521, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 53/1000. Loss of 0.6407, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 54/1000. Loss of 0.5607, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 55/1000. Loss of 0.7624, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 56/1000. Loss of 0.7199, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 57/1000. Loss of 0.6200, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 58/1000. Loss of 0.7296, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 59/1000. Loss of 0.6378, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 60/1000. Loss of 0.5829, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 61/1000. Loss of 0.7466, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 62/1000. Loss of 0.5321, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 63/1000. Loss of 0.5686, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 64/1000. Loss of 0.6157, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 65/1000. Loss of 0.7303, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 66/1000. Loss of 0.6387, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 67/1000. Loss of 0.5058, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 68/1000. Loss of 0.7056, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 69/1000. Loss of 0.6221, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 70/1000. Loss of 0.6886, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 71/1000. Loss of 0.5949, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 72/1000. Loss of 0.6341, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 73/1000. Loss of 0.7052, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 74/1000. Loss of 0.6109, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 75/1000. Loss of 0.8483, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 76/1000. Loss of 0.7479, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 77/1000. Loss of 0.6682, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 78/1000. Loss of 0.6179, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 79/1000. Loss of 0.6843, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 80/1000. Loss of 0.5753, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 81/1000. Loss of 0.6449, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 82/1000. Loss of 0.6426, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 83/1000. Loss of 0.7100, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 84/1000. Loss of 0.5898, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 85/1000. Loss of 0.6627, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 86/1000. Loss of 0.7472, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 87/1000. Loss of 0.7076, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 88/1000. Loss of 0.7069, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 89/1000. Loss of 0.6393, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 90/1000. Loss of 0.7035, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 91/1000. Loss of 0.5878, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 92/1000. Loss of 0.7331, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 93/1000. Loss of 0.6763, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 94/1000. Loss of 0.6299, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 95/1000. Loss of 0.5880, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 96/1000. Loss of 0.7622, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 97/1000. Loss of 0.5385, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 98/1000. Loss of 0.6703, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 99/1000. Loss of 0.6027, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 100/1000. Loss of 0.6690, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 101/1000. Loss of 0.5744, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 102/1000. Loss of 0.6363, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 103/1000. Loss of 0.6273, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 104/1000. Loss of 0.6573, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 105/1000. Loss of 0.6371, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 106/1000. Loss of 0.6686, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 107/1000. Loss of 0.6281, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 108/1000. Loss of 0.6407, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 109/1000. Loss of 0.6227, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 110/1000. Loss of 0.5235, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 111/1000. Loss of 0.7051, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 112/1000. Loss of 0.6024, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 113/1000. Loss of 0.7023, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 114/1000. Loss of 0.6486, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 115/1000. Loss of 0.6007, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 116/1000. Loss of 0.7036, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 117/1000. Loss of 0.6391, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 118/1000. Loss of 0.6583, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 119/1000. Loss of 0.6471, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 120/1000. Loss of 0.6213, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 121/1000. Loss of 0.7890, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 122/1000. Loss of 0.6159, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 123/1000. Loss of 0.6443, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 124/1000. Loss of 0.6512, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 125/1000. Loss of 0.6522, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 126/1000. Loss of 0.6109, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 127/1000. Loss of 0.6415, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 128/1000. Loss of 0.6318, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 129/1000. Loss of 0.6381, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 130/1000. Loss of 0.6837, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 131/1000. Loss of 0.7002, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 132/1000. Loss of 0.5973, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 133/1000. Loss of 0.7192, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 134/1000. Loss of 0.6599, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 135/1000. Loss of 0.6701, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 136/1000. Loss of 0.6906, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 137/1000. Loss of 0.7689, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 138/1000. Loss of 0.7162, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 139/1000. Loss of 0.6090, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 140/1000. Loss of 0.6439, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 141/1000. Loss of 0.6986, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 142/1000. Loss of 0.6805, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 143/1000. Loss of 0.6730, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 144/1000. Loss of 0.5969, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 145/1000. Loss of 0.6560, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 146/1000. Loss of 0.6271, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 147/1000. Loss of 0.6526, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 148/1000. Loss of 0.6857, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 149/1000. Loss of 0.6855, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 150/1000. Loss of 0.6725, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 151/1000. Loss of 0.6299, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 152/1000. Loss of 0.6857, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 153/1000. Loss of 0.6025, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 154/1000. Loss of 0.6640, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 155/1000. Loss of 0.6519, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 156/1000. Loss of 0.6233, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 157/1000. Loss of 0.6293, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 158/1000. Loss of 0.6975, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 159/1000. Loss of 0.7580, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 160/1000. Loss of 0.6708, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 161/1000. Loss of 0.5716, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 162/1000. Loss of 0.5743, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 163/1000. Loss of 0.6123, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 164/1000. Loss of 0.6283, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 165/1000. Loss of 0.6862, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 166/1000. Loss of 0.6886, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 167/1000. Loss of 0.6993, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 168/1000. Loss of 0.6476, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 169/1000. Loss of 0.6704, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 170/1000. Loss of 0.6307, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 171/1000. Loss of 0.6704, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 172/1000. Loss of 0.7249, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 173/1000. Loss of 0.5815, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 174/1000. Loss of 0.6740, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 175/1000. Loss of 0.5963, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 176/1000. Loss of 0.6775, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 177/1000. Loss of 0.7309, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 178/1000. Loss of 0.6482, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 179/1000. Loss of 0.6868, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 180/1000. Loss of 0.6342, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 181/1000. Loss of 0.6532, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 182/1000. Loss of 0.6305, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 183/1000. Loss of 0.5841, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 184/1000. Loss of 0.5904, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 185/1000. Loss of 0.6795, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 186/1000. Loss of 0.6499, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 187/1000. Loss of 0.6271, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 188/1000. Loss of 0.5960, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 189/1000. Loss of 0.6571, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 190/1000. Loss of 0.6748, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 191/1000. Loss of 0.6212, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 192/1000. Loss of 0.6409, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 193/1000. Loss of 0.5877, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 194/1000. Loss of 0.6409, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 195/1000. Loss of 0.6465, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 196/1000. Loss of 0.7093, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 197/1000. Loss of 0.7396, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 198/1000. Loss of 0.5544, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 199/1000. Loss of 0.5722, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 200/1000. Loss of 0.5997, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 201/1000. Loss of 0.6373, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 202/1000. Loss of 0.6353, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 203/1000. Loss of 0.6109, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 204/1000. Loss of 0.6140, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 205/1000. Loss of 0.5523, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 206/1000. Loss of 0.6150, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 207/1000. Loss of 0.5689, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 208/1000. Loss of 0.5850, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 209/1000. Loss of 0.5849, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 210/1000. Loss of 0.6265, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 211/1000. Loss of 0.6481, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 212/1000. Loss of 0.6533, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 213/1000. Loss of 0.7437, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 214/1000. Loss of 0.6984, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 215/1000. Loss of 0.6182, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 216/1000. Loss of 0.5875, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 217/1000. Loss of 0.6564, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 218/1000. Loss of 0.6305, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 219/1000. Loss of 0.6123, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 220/1000. Loss of 0.6282, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 221/1000. Loss of 0.6429, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 222/1000. Loss of 0.6082, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 223/1000. Loss of 0.5534, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 224/1000. Loss of 0.5788, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 225/1000. Loss of 0.6518, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 226/1000. Loss of 0.5524, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 227/1000. Loss of 0.5351, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 228/1000. Loss of 0.5830, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 229/1000. Loss of 0.5644, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 230/1000. Loss of 0.6900, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 231/1000. Loss of 0.6681, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 232/1000. Loss of 0.6046, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 233/1000. Loss of 0.6927, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 234/1000. Loss of 0.7503, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 235/1000. Loss of 0.5638, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 236/1000. Loss of 0.7668, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 237/1000. Loss of 0.6942, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 238/1000. Loss of 0.7316, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 239/1000. Loss of 0.7108, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 240/1000. Loss of 0.6339, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 241/1000. Loss of 0.6450, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 242/1000. Loss of 0.6731, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 243/1000. Loss of 0.6754, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 244/1000. Loss of 0.6361, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 245/1000. Loss of 0.5538, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 246/1000. Loss of 0.6217, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 247/1000. Loss of 0.6937, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 248/1000. Loss of 0.7177, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 249/1000. Loss of 0.6328, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 250/1000. Loss of 0.6387, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 251/1000. Loss of 0.6308, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 252/1000. Loss of 0.6684, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 253/1000. Loss of 0.7394, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 254/1000. Loss of 0.6560, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 255/1000. Loss of 0.7501, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 256/1000. Loss of 0.6560, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 257/1000. Loss of 0.7073, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 258/1000. Loss of 0.6258, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 259/1000. Loss of 0.6595, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 260/1000. Loss of 0.7182, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 261/1000. Loss of 0.6049, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 262/1000. Loss of 0.7170, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 263/1000. Loss of 0.6490, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 264/1000. Loss of 0.6850, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 265/1000. Loss of 0.6872, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 266/1000. Loss of 0.5760, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 267/1000. Loss of 0.5728, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 268/1000. Loss of 0.5953, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 269/1000. Loss of 0.6184, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 270/1000. Loss of 0.6370, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 271/1000. Loss of 0.6266, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 272/1000. Loss of 0.5995, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 273/1000. Loss of 0.5820, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 274/1000. Loss of 0.5581, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 275/1000. Loss of 0.6928, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 276/1000. Loss of 0.5820, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 277/1000. Loss of 0.5967, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 278/1000. Loss of 0.6006, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 279/1000. Loss of 0.7118, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 280/1000. Loss of 0.6671, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 281/1000. Loss of 0.5128, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 282/1000. Loss of 0.6301, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 283/1000. Loss of 0.6479, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 284/1000. Loss of 0.6806, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 285/1000. Loss of 0.7199, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 286/1000. Loss of 0.6635, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 287/1000. Loss of 0.7059, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 288/1000. Loss of 0.5679, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 289/1000. Loss of 0.7462, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 290/1000. Loss of 0.6668, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 291/1000. Loss of 0.6326, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 292/1000. Loss of 0.5840, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 293/1000. Loss of 0.5805, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 294/1000. Loss of 0.5798, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 295/1000. Loss of 0.7029, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 296/1000. Loss of 0.5909, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 297/1000. Loss of 0.5243, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 298/1000. Loss of 0.6973, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 299/1000. Loss of 0.5939, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 300/1000. Loss of 0.7235, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 301/1000. Loss of 0.4845, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 302/1000. Loss of 0.6598, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 303/1000. Loss of 0.5664, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 304/1000. Loss of 0.6070, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 305/1000. Loss of 0.5286, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 306/1000. Loss of 0.6755, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 307/1000. Loss of 0.6839, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 308/1000. Loss of 0.6673, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 309/1000. Loss of 0.6956, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 310/1000. Loss of 0.6409, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 311/1000. Loss of 0.6495, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 312/1000. Loss of 0.7320, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 313/1000. Loss of 0.5718, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 314/1000. Loss of 0.6753, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 315/1000. Loss of 0.6299, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 316/1000. Loss of 0.6111, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 317/1000. Loss of 0.6968, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 318/1000. Loss of 0.6877, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 319/1000. Loss of 0.6450, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 320/1000. Loss of 0.5591, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 321/1000. Loss of 0.6716, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 322/1000. Loss of 0.6743, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 323/1000. Loss of 0.6471, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 324/1000. Loss of 0.7933, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 325/1000. Loss of 0.5796, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 326/1000. Loss of 0.6333, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 327/1000. Loss of 0.6678, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 328/1000. Loss of 0.6855, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 329/1000. Loss of 0.5768, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 330/1000. Loss of 0.6083, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 331/1000. Loss of 0.6139, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 332/1000. Loss of 0.5598, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 333/1000. Loss of 0.6301, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 334/1000. Loss of 0.7683, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 335/1000. Loss of 0.7614, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 336/1000. Loss of 0.7546, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 337/1000. Loss of 0.6138, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 338/1000. Loss of 0.6747, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 339/1000. Loss of 0.7309, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 340/1000. Loss of 0.6102, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 341/1000. Loss of 0.6809, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 342/1000. Loss of 0.6301, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 343/1000. Loss of 0.5935, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 344/1000. Loss of 0.6895, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 345/1000. Loss of 0.6629, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 346/1000. Loss of 0.5779, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 347/1000. Loss of 0.6502, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 348/1000. Loss of 0.6622, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 349/1000. Loss of 0.6556, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 350/1000. Loss of 0.7393, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 351/1000. Loss of 0.7101, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 352/1000. Loss of 0.6916, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 353/1000. Loss of 0.6557, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 354/1000. Loss of 0.6651, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 355/1000. Loss of 0.7436, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 356/1000. Loss of 0.6924, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 357/1000. Loss of 0.7037, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 358/1000. Loss of 0.6424, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 359/1000. Loss of 0.6755, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 360/1000. Loss of 0.6831, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 361/1000. Loss of 0.6379, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 362/1000. Loss of 0.6295, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 363/1000. Loss of 0.6100, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 364/1000. Loss of 0.6787, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 365/1000. Loss of 0.6601, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 366/1000. Loss of 0.6031, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 367/1000. Loss of 0.6128, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 368/1000. Loss of 0.6042, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 369/1000. Loss of 0.7079, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 370/1000. Loss of 0.6521, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 371/1000. Loss of 0.6185, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 372/1000. Loss of 0.6227, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 373/1000. Loss of 0.7343, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 374/1000. Loss of 0.6833, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 375/1000. Loss of 0.5946, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 376/1000. Loss of 0.7559, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 377/1000. Loss of 0.6012, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 378/1000. Loss of 0.7102, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 379/1000. Loss of 0.5979, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 380/1000. Loss of 0.6866, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 381/1000. Loss of 0.7143, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 382/1000. Loss of 0.6490, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 383/1000. Loss of 0.7618, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 384/1000. Loss of 0.6394, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 385/1000. Loss of 0.7383, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 386/1000. Loss of 0.5993, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 387/1000. Loss of 0.7721, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 388/1000. Loss of 0.7205, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 389/1000. Loss of 0.6286, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 390/1000. Loss of 0.6664, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 391/1000. Loss of 0.6501, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 392/1000. Loss of 0.6166, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 393/1000. Loss of 0.6753, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 394/1000. Loss of 0.5724, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 395/1000. Loss of 0.5684, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 396/1000. Loss of 0.6590, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 397/1000. Loss of 0.6817, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 398/1000. Loss of 0.6721, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 399/1000. Loss of 0.6209, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 400/1000. Loss of 0.7238, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 401/1000. Loss of 0.7299, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 402/1000. Loss of 0.6918, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 403/1000. Loss of 0.6758, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 404/1000. Loss of 0.6506, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 405/1000. Loss of 0.8418, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 406/1000. Loss of 0.6131, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 407/1000. Loss of 0.6653, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 408/1000. Loss of 0.6262, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 409/1000. Loss of 0.6053, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 410/1000. Loss of 0.6159, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 411/1000. Loss of 0.6296, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 412/1000. Loss of 0.7145, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 413/1000. Loss of 0.7011, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 414/1000. Loss of 0.6432, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 415/1000. Loss of 0.7604, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 416/1000. Loss of 0.6522, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 417/1000. Loss of 0.5375, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 418/1000. Loss of 0.6255, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 419/1000. Loss of 0.6038, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 420/1000. Loss of 0.5781, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 421/1000. Loss of 0.6151, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 422/1000. Loss of 0.7213, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 423/1000. Loss of 0.6354, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 424/1000. Loss of 0.6480, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 425/1000. Loss of 0.6188, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 426/1000. Loss of 0.7228, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 427/1000. Loss of 0.6251, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 428/1000. Loss of 0.6255, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 429/1000. Loss of 0.5805, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 430/1000. Loss of 0.5756, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 431/1000. Loss of 0.5974, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 432/1000. Loss of 0.6721, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 433/1000. Loss of 0.6109, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 434/1000. Loss of 0.5852, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 435/1000. Loss of 0.7125, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 436/1000. Loss of 0.5989, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 437/1000. Loss of 0.6614, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 438/1000. Loss of 0.5938, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 439/1000. Loss of 0.7031, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 440/1000. Loss of 0.6173, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 441/1000. Loss of 0.6328, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 442/1000. Loss of 0.6364, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 443/1000. Loss of 0.6922, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 444/1000. Loss of 0.6152, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 445/1000. Loss of 0.6551, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 446/1000. Loss of 0.6511, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 447/1000. Loss of 0.6075, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 448/1000. Loss of 0.6641, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 449/1000. Loss of 0.6224, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 450/1000. Loss of 0.6149, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 451/1000. Loss of 0.6416, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 452/1000. Loss of 0.6226, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 453/1000. Loss of 0.6465, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 454/1000. Loss of 0.6657, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 455/1000. Loss of 0.5641, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 456/1000. Loss of 0.6796, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 457/1000. Loss of 0.7638, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 458/1000. Loss of 0.6427, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 459/1000. Loss of 0.6894, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 460/1000. Loss of 0.6218, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 461/1000. Loss of 0.7138, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 462/1000. Loss of 0.5167, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 463/1000. Loss of 0.6214, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 464/1000. Loss of 0.6529, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 465/1000. Loss of 0.6142, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 466/1000. Loss of 0.5908, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 467/1000. Loss of 0.6910, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 468/1000. Loss of 0.6348, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 469/1000. Loss of 0.6156, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 470/1000. Loss of 0.7267, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 471/1000. Loss of 0.5313, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 472/1000. Loss of 0.7476, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 473/1000. Loss of 0.6756, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 474/1000. Loss of 0.6627, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 475/1000. Loss of 0.5385, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 476/1000. Loss of 0.6930, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 477/1000. Loss of 0.5979, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 478/1000. Loss of 0.7984, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 479/1000. Loss of 0.6173, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 480/1000. Loss of 0.6831, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 481/1000. Loss of 0.7418, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 482/1000. Loss of 0.6424, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 483/1000. Loss of 0.6142, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 484/1000. Loss of 0.5417, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 485/1000. Loss of 0.6758, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 486/1000. Loss of 0.6645, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 487/1000. Loss of 0.6313, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 488/1000. Loss of 0.6012, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 489/1000. Loss of 0.7149, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 490/1000. Loss of 0.5496, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 491/1000. Loss of 0.6076, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 492/1000. Loss of 0.6964, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 493/1000. Loss of 0.6739, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 494/1000. Loss of 0.6064, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 495/1000. Loss of 0.8473, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 496/1000. Loss of 0.6437, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 497/1000. Loss of 0.6395, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 498/1000. Loss of 0.6533, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 499/1000. Loss of 0.6221, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 500/1000. Loss of 0.6934, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 501/1000. Loss of 0.7606, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 502/1000. Loss of 0.6502, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 503/1000. Loss of 0.6128, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 504/1000. Loss of 0.6600, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 505/1000. Loss of 0.6937, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 506/1000. Loss of 0.7319, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 507/1000. Loss of 0.6186, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 508/1000. Loss of 0.6631, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 509/1000. Loss of 0.6789, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 510/1000. Loss of 0.6091, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 511/1000. Loss of 0.6100, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 512/1000. Loss of 0.6845, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 513/1000. Loss of 0.6991, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 514/1000. Loss of 0.6850, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 515/1000. Loss of 0.5962, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 516/1000. Loss of 0.5686, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 517/1000. Loss of 0.5890, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 518/1000. Loss of 0.6646, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 519/1000. Loss of 0.7285, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 520/1000. Loss of 0.6050, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 521/1000. Loss of 0.6116, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 522/1000. Loss of 0.5592, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 523/1000. Loss of 0.6712, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 524/1000. Loss of 0.6043, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 525/1000. Loss of 0.7024, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 526/1000. Loss of 0.6183, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 527/1000. Loss of 0.6440, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 528/1000. Loss of 0.6234, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 529/1000. Loss of 0.6783, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 530/1000. Loss of 0.6354, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 531/1000. Loss of 0.6539, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 532/1000. Loss of 0.6586, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 533/1000. Loss of 0.6201, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 534/1000. Loss of 0.6327, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 535/1000. Loss of 0.7033, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 536/1000. Loss of 0.6375, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 537/1000. Loss of 0.6547, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 538/1000. Loss of 0.6180, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 539/1000. Loss of 0.7247, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 540/1000. Loss of 0.5692, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 541/1000. Loss of 0.5399, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 542/1000. Loss of 0.7861, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 543/1000. Loss of 0.5658, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 544/1000. Loss of 0.5484, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 545/1000. Loss of 0.7504, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 546/1000. Loss of 0.6634, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 547/1000. Loss of 0.8246, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 548/1000. Loss of 0.6539, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 549/1000. Loss of 0.5617, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 550/1000. Loss of 0.5799, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 551/1000. Loss of 0.7381, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 552/1000. Loss of 0.6064, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 553/1000. Loss of 0.6277, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 554/1000. Loss of 0.6575, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 555/1000. Loss of 0.6240, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 556/1000. Loss of 0.6841, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 557/1000. Loss of 0.7063, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 558/1000. Loss of 0.6091, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 559/1000. Loss of 0.6925, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 560/1000. Loss of 0.6745, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 561/1000. Loss of 0.7061, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 562/1000. Loss of 0.6884, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 563/1000. Loss of 0.6452, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 564/1000. Loss of 0.6524, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 565/1000. Loss of 0.6094, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 566/1000. Loss of 0.6036, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 567/1000. Loss of 0.6363, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 568/1000. Loss of 0.5654, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 569/1000. Loss of 0.6406, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 570/1000. Loss of 0.7019, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 571/1000. Loss of 0.7188, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 572/1000. Loss of 0.6893, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 573/1000. Loss of 0.7221, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 574/1000. Loss of 0.5937, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 575/1000. Loss of 0.6797, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 576/1000. Loss of 0.6709, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 577/1000. Loss of 0.6391, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 578/1000. Loss of 0.6048, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 579/1000. Loss of 0.6597, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 580/1000. Loss of 0.6040, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 581/1000. Loss of 0.6851, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 582/1000. Loss of 0.7587, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 583/1000. Loss of 0.7459, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 584/1000. Loss of 0.5828, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 585/1000. Loss of 0.6080, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 586/1000. Loss of 0.6636, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 587/1000. Loss of 0.6979, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 588/1000. Loss of 0.6112, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 589/1000. Loss of 0.6125, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 590/1000. Loss of 0.6251, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 591/1000. Loss of 0.6862, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 592/1000. Loss of 0.6552, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 593/1000. Loss of 0.7480, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 594/1000. Loss of 0.6324, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 595/1000. Loss of 0.6367, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 596/1000. Loss of 0.5828, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 597/1000. Loss of 0.6214, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 598/1000. Loss of 0.6403, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 599/1000. Loss of 0.7034, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 600/1000. Loss of 0.6040, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 601/1000. Loss of 0.5498, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 602/1000. Loss of 0.6166, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 603/1000. Loss of 0.6435, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 604/1000. Loss of 0.6450, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 605/1000. Loss of 0.6555, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 606/1000. Loss of 0.6867, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 607/1000. Loss of 0.7160, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 608/1000. Loss of 0.6602, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 609/1000. Loss of 0.6613, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 610/1000. Loss of 0.5623, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 611/1000. Loss of 0.6309, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 612/1000. Loss of 0.6752, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 613/1000. Loss of 0.8090, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 614/1000. Loss of 0.5996, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 615/1000. Loss of 0.6083, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 616/1000. Loss of 0.5835, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 617/1000. Loss of 0.5896, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 618/1000. Loss of 0.7070, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 619/1000. Loss of 0.7059, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 620/1000. Loss of 0.6063, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 621/1000. Loss of 0.7386, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 622/1000. Loss of 0.5381, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 623/1000. Loss of 0.7796, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 624/1000. Loss of 0.6398, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 625/1000. Loss of 0.6467, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 626/1000. Loss of 0.6419, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 627/1000. Loss of 0.7050, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 628/1000. Loss of 0.7216, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 629/1000. Loss of 0.5933, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 630/1000. Loss of 0.6391, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 631/1000. Loss of 0.6063, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 632/1000. Loss of 0.6127, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 633/1000. Loss of 0.7648, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 634/1000. Loss of 0.6356, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 635/1000. Loss of 0.5906, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 636/1000. Loss of 0.5975, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 637/1000. Loss of 0.6560, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 638/1000. Loss of 0.7048, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 639/1000. Loss of 0.6568, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 640/1000. Loss of 0.6534, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 641/1000. Loss of 0.5836, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 642/1000. Loss of 0.7211, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 643/1000. Loss of 0.6362, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 644/1000. Loss of 0.6471, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 645/1000. Loss of 0.6406, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 646/1000. Loss of 0.5981, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 647/1000. Loss of 0.6131, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 648/1000. Loss of 0.6039, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 649/1000. Loss of 0.6116, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 650/1000. Loss of 0.6065, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 651/1000. Loss of 0.6718, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 652/1000. Loss of 0.7051, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 653/1000. Loss of 0.6628, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 654/1000. Loss of 0.6203, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 655/1000. Loss of 0.6519, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 656/1000. Loss of 0.6860, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 657/1000. Loss of 0.5933, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 658/1000. Loss of 0.6712, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 659/1000. Loss of 0.6155, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 660/1000. Loss of 0.6446, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 661/1000. Loss of 0.6705, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 662/1000. Loss of 0.6850, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 663/1000. Loss of 0.5762, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 664/1000. Loss of 0.6366, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 665/1000. Loss of 0.6562, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 666/1000. Loss of 0.7500, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 667/1000. Loss of 0.6320, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 668/1000. Loss of 0.6026, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 669/1000. Loss of 0.6464, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 670/1000. Loss of 0.7409, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 671/1000. Loss of 0.6192, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 672/1000. Loss of 0.6474, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 673/1000. Loss of 0.6556, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 674/1000. Loss of 0.6525, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 675/1000. Loss of 0.6747, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 676/1000. Loss of 0.6926, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 677/1000. Loss of 0.5641, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 678/1000. Loss of 0.6908, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 679/1000. Loss of 0.7277, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 680/1000. Loss of 0.5671, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 681/1000. Loss of 0.6104, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 682/1000. Loss of 0.6969, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 683/1000. Loss of 0.6059, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 684/1000. Loss of 0.7118, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 685/1000. Loss of 0.7058, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 686/1000. Loss of 0.6899, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 687/1000. Loss of 0.7452, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 688/1000. Loss of 0.7060, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 689/1000. Loss of 0.6838, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 690/1000. Loss of 0.6591, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 691/1000. Loss of 0.5897, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 692/1000. Loss of 0.6768, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 693/1000. Loss of 0.7129, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 694/1000. Loss of 0.6296, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 695/1000. Loss of 0.7400, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 696/1000. Loss of 0.7263, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 697/1000. Loss of 0.5858, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 698/1000. Loss of 0.6488, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 699/1000. Loss of 0.6595, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 700/1000. Loss of 0.6155, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 701/1000. Loss of 0.6775, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 702/1000. Loss of 0.5716, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 703/1000. Loss of 0.7206, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 704/1000. Loss of 0.7191, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 705/1000. Loss of 0.6485, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 706/1000. Loss of 0.5775, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 707/1000. Loss of 0.5775, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 708/1000. Loss of 0.7649, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 709/1000. Loss of 0.6283, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 710/1000. Loss of 0.6406, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 711/1000. Loss of 0.6286, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 712/1000. Loss of 0.8281, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 713/1000. Loss of 0.5743, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 714/1000. Loss of 0.5568, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 715/1000. Loss of 0.5986, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 716/1000. Loss of 0.6574, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 717/1000. Loss of 0.5586, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 718/1000. Loss of 0.6786, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 719/1000. Loss of 0.7141, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 720/1000. Loss of 0.6382, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 721/1000. Loss of 0.6350, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 722/1000. Loss of 0.7295, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 723/1000. Loss of 0.6174, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 724/1000. Loss of 0.7108, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 725/1000. Loss of 0.6007, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 726/1000. Loss of 0.6380, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 727/1000. Loss of 0.6873, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 728/1000. Loss of 0.6227, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 729/1000. Loss of 0.5826, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 730/1000. Loss of 0.6106, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 731/1000. Loss of 0.5969, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 732/1000. Loss of 0.6587, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 733/1000. Loss of 0.6034, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 734/1000. Loss of 0.6747, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 735/1000. Loss of 0.7512, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 736/1000. Loss of 0.5825, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 737/1000. Loss of 0.6299, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 738/1000. Loss of 0.5956, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 739/1000. Loss of 0.6683, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 740/1000. Loss of 0.6466, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 741/1000. Loss of 0.5825, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 742/1000. Loss of 0.6456, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 743/1000. Loss of 0.5975, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 744/1000. Loss of 0.6051, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 745/1000. Loss of 0.7333, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 746/1000. Loss of 0.5786, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 747/1000. Loss of 0.6330, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 748/1000. Loss of 0.6777, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 749/1000. Loss of 0.6412, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 750/1000. Loss of 0.6260, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 751/1000. Loss of 0.7084, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 752/1000. Loss of 0.6462, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 753/1000. Loss of 0.6058, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 754/1000. Loss of 0.6609, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 755/1000. Loss of 0.6203, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 756/1000. Loss of 0.6126, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 757/1000. Loss of 0.7144, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 758/1000. Loss of 0.6769, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 759/1000. Loss of 0.6972, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 760/1000. Loss of 0.6979, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 761/1000. Loss of 0.7051, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 762/1000. Loss of 0.6758, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 763/1000. Loss of 0.7101, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 764/1000. Loss of 0.6828, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 765/1000. Loss of 0.6746, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 766/1000. Loss of 0.6830, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 767/1000. Loss of 0.8017, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 768/1000. Loss of 0.6948, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 769/1000. Loss of 0.6396, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 770/1000. Loss of 0.6377, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 771/1000. Loss of 0.6152, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 772/1000. Loss of 0.5795, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 773/1000. Loss of 0.6710, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 774/1000. Loss of 0.6526, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 775/1000. Loss of 0.6588, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 776/1000. Loss of 0.6795, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 777/1000. Loss of 0.6619, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 778/1000. Loss of 0.6625, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 779/1000. Loss of 0.7648, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 780/1000. Loss of 0.6462, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 781/1000. Loss of 0.5625, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 782/1000. Loss of 0.6022, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 783/1000. Loss of 0.6080, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 784/1000. Loss of 0.6098, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 785/1000. Loss of 0.6429, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 786/1000. Loss of 0.6174, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 787/1000. Loss of 0.6154, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 788/1000. Loss of 0.5978, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 789/1000. Loss of 0.7010, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 790/1000. Loss of 0.6588, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 791/1000. Loss of 0.6560, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 792/1000. Loss of 0.6011, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 793/1000. Loss of 0.6446, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 794/1000. Loss of 0.8045, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 795/1000. Loss of 0.6287, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 796/1000. Loss of 0.6251, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 797/1000. Loss of 0.7096, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 798/1000. Loss of 0.6965, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 799/1000. Loss of 0.5732, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 800/1000. Loss of 0.6372, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 801/1000. Loss of 0.6636, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 802/1000. Loss of 0.7440, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 803/1000. Loss of 0.6393, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 804/1000. Loss of 0.5893, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 805/1000. Loss of 0.6803, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 806/1000. Loss of 0.6504, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 807/1000. Loss of 0.6466, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 808/1000. Loss of 0.5951, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 809/1000. Loss of 0.6450, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 810/1000. Loss of 0.6413, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 811/1000. Loss of 0.6219, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 812/1000. Loss of 0.6499, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 813/1000. Loss of 0.7138, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 814/1000. Loss of 0.6575, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 815/1000. Loss of 0.6278, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 816/1000. Loss of 0.6347, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 817/1000. Loss of 0.6056, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 818/1000. Loss of 0.5984, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 819/1000. Loss of 0.6132, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 820/1000. Loss of 0.5745, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 821/1000. Loss of 0.5826, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 822/1000. Loss of 0.7148, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 823/1000. Loss of 0.7504, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 824/1000. Loss of 0.6405, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 825/1000. Loss of 0.5862, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 826/1000. Loss of 0.6742, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 827/1000. Loss of 0.7336, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 828/1000. Loss of 0.6011, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 829/1000. Loss of 0.6210, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 830/1000. Loss of 0.6894, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 831/1000. Loss of 0.6222, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 832/1000. Loss of 0.5761, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 833/1000. Loss of 0.6694, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 834/1000. Loss of 0.7150, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 835/1000. Loss of 0.6829, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 836/1000. Loss of 0.7782, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 837/1000. Loss of 0.5084, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 838/1000. Loss of 0.6330, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 839/1000. Loss of 0.6575, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 840/1000. Loss of 0.6070, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 841/1000. Loss of 0.6523, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 842/1000. Loss of 0.7351, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 843/1000. Loss of 0.6268, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 844/1000. Loss of 0.6128, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 845/1000. Loss of 0.6854, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 846/1000. Loss of 0.6673, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 847/1000. Loss of 0.6487, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 848/1000. Loss of 0.7273, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 849/1000. Loss of 0.6169, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 850/1000. Loss of 0.6337, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 851/1000. Loss of 0.6945, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 852/1000. Loss of 0.6089, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 853/1000. Loss of 0.6709, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 854/1000. Loss of 0.6270, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 855/1000. Loss of 0.5590, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 856/1000. Loss of 0.6450, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 857/1000. Loss of 0.7501, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 858/1000. Loss of 0.6399, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 859/1000. Loss of 0.5932, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 860/1000. Loss of 0.6668, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 861/1000. Loss of 0.7117, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 862/1000. Loss of 0.7685, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 863/1000. Loss of 0.7195, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 864/1000. Loss of 0.6866, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 865/1000. Loss of 0.6312, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 866/1000. Loss of 0.6572, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 867/1000. Loss of 0.7007, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 868/1000. Loss of 0.7185, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 869/1000. Loss of 0.5926, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 870/1000. Loss of 0.7040, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 871/1000. Loss of 0.7167, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 872/1000. Loss of 0.6358, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 873/1000. Loss of 0.7047, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 874/1000. Loss of 0.6503, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 875/1000. Loss of 0.6362, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 876/1000. Loss of 0.7222, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 877/1000. Loss of 0.6118, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 878/1000. Loss of 0.5848, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 879/1000. Loss of 0.6258, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 880/1000. Loss of 0.6408, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 881/1000. Loss of 0.6060, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 882/1000. Loss of 0.5598, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 883/1000. Loss of 0.5891, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 884/1000. Loss of 0.6786, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 885/1000. Loss of 0.6255, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 886/1000. Loss of 0.6203, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 887/1000. Loss of 0.6462, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 888/1000. Loss of 0.5910, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 889/1000. Loss of 0.6123, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 890/1000. Loss of 0.6757, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 891/1000. Loss of 0.7046, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 892/1000. Loss of 0.7365, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 893/1000. Loss of 0.6721, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 894/1000. Loss of 0.6260, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 895/1000. Loss of 0.7015, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 896/1000. Loss of 0.6782, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 897/1000. Loss of 0.5934, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 898/1000. Loss of 0.5605, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 899/1000. Loss of 0.6844, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 900/1000. Loss of 0.6736, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 901/1000. Loss of 0.7021, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 902/1000. Loss of 0.5960, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 903/1000. Loss of 0.5645, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 904/1000. Loss of 0.6519, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 905/1000. Loss of 0.6228, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 906/1000. Loss of 0.6070, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 907/1000. Loss of 0.6006, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 908/1000. Loss of 0.6588, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 909/1000. Loss of 0.6961, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 910/1000. Loss of 0.5971, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 911/1000. Loss of 0.5722, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 912/1000. Loss of 0.6162, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 913/1000. Loss of 0.6515, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 914/1000. Loss of 0.5776, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 915/1000. Loss of 0.6373, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 916/1000. Loss of 0.6181, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 917/1000. Loss of 0.7245, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 918/1000. Loss of 0.6238, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 919/1000. Loss of 0.6622, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 920/1000. Loss of 0.6282, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 921/1000. Loss of 0.6633, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 922/1000. Loss of 0.7709, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 923/1000. Loss of 0.5815, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 924/1000. Loss of 0.6494, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 925/1000. Loss of 0.6676, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 926/1000. Loss of 0.6578, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 927/1000. Loss of 0.5971, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 928/1000. Loss of 0.6803, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 929/1000. Loss of 0.6706, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 930/1000. Loss of 0.6057, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 931/1000. Loss of 0.5726, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 932/1000. Loss of 0.7435, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 933/1000. Loss of 0.5897, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 934/1000. Loss of 0.6209, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 935/1000. Loss of 0.6050, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 936/1000. Loss of 0.5954, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 937/1000. Loss of 0.6925, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 938/1000. Loss of 0.7123, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 939/1000. Loss of 0.5459, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 940/1000. Loss of 0.7645, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 941/1000. Loss of 0.5486, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 942/1000. Loss of 0.6463, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 943/1000. Loss of 0.7050, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 944/1000. Loss of 0.7292, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 945/1000. Loss of 0.6222, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 946/1000. Loss of 0.6047, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 947/1000. Loss of 0.5378, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 948/1000. Loss of 0.6571, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 949/1000. Loss of 0.7136, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 950/1000. Loss of 0.6859, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 951/1000. Loss of 0.6316, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 952/1000. Loss of 0.6062, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 953/1000. Loss of 0.7613, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 954/1000. Loss of 0.6826, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 955/1000. Loss of 0.5504, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 956/1000. Loss of 0.5799, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 957/1000. Loss of 0.6366, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 958/1000. Loss of 0.6898, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 959/1000. Loss of 0.7126, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 960/1000. Loss of 0.6872, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 961/1000. Loss of 0.6146, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 962/1000. Loss of 0.6772, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 963/1000. Loss of 0.7699, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 964/1000. Loss of 0.5625, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 965/1000. Loss of 0.7139, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 966/1000. Loss of 0.6967, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 967/1000. Loss of 0.7296, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 968/1000. Loss of 0.6762, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 969/1000. Loss of 0.6499, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 970/1000. Loss of 0.6229, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 971/1000. Loss of 0.5729, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 972/1000. Loss of 0.7117, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 973/1000. Loss of 0.6220, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 974/1000. Loss of 0.7226, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 975/1000. Loss of 0.7156, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 976/1000. Loss of 0.6087, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 977/1000. Loss of 0.5768, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 978/1000. Loss of 0.6731, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 979/1000. Loss of 0.6546, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 980/1000. Loss of 0.6245, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 981/1000. Loss of 0.5593, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 982/1000. Loss of 0.6042, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 983/1000. Loss of 0.5908, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 984/1000. Loss of 0.6260, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 985/1000. Loss of 0.6391, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 986/1000. Loss of 0.6269, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 987/1000. Loss of 0.6527, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 988/1000. Loss of 0.6244, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 989/1000. Loss of 0.7361, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 990/1000. Loss of 0.6296, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 991/1000. Loss of 0.6312, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 992/1000. Loss of 0.6040, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 993/1000. Loss of 0.7114, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 994/1000. Loss of 0.5863, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 995/1000. Loss of 0.7495, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 996/1000. Loss of 0.6133, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 997/1000. Loss of 0.6324, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 998/1000. Loss of 0.7247, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 999/1000. Loss of 0.6396, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 1000/1000. Loss of 0.7163, cover loss of 0.0003, secret loss of 0.0002\n",
            "Epoch [48/50], Average_loss: 0.6500\n",
            "Training: Batch 1/1000. Loss of 0.7004, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 2/1000. Loss of 0.6440, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 3/1000. Loss of 0.6391, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 4/1000. Loss of 0.6748, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 5/1000. Loss of 0.6380, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 6/1000. Loss of 0.7070, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 7/1000. Loss of 0.5354, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 8/1000. Loss of 0.6338, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 9/1000. Loss of 0.7081, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 10/1000. Loss of 0.6712, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 11/1000. Loss of 0.6665, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 12/1000. Loss of 0.6614, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 13/1000. Loss of 0.6978, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 14/1000. Loss of 0.7370, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 15/1000. Loss of 0.6178, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 16/1000. Loss of 0.6583, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 17/1000. Loss of 0.5757, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 18/1000. Loss of 0.7332, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 19/1000. Loss of 0.6377, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 20/1000. Loss of 0.6580, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 21/1000. Loss of 0.6178, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 22/1000. Loss of 0.6179, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 23/1000. Loss of 0.6928, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 24/1000. Loss of 0.6410, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 25/1000. Loss of 0.6227, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 26/1000. Loss of 0.6507, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 27/1000. Loss of 0.7279, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 28/1000. Loss of 0.7756, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 29/1000. Loss of 0.6900, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 30/1000. Loss of 0.6272, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 31/1000. Loss of 0.7257, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 32/1000. Loss of 0.6235, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 33/1000. Loss of 0.6128, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 34/1000. Loss of 0.6275, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 35/1000. Loss of 0.6229, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 36/1000. Loss of 0.5683, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 37/1000. Loss of 0.6137, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 38/1000. Loss of 0.7293, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 39/1000. Loss of 0.6259, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 40/1000. Loss of 0.5783, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 41/1000. Loss of 0.7096, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 42/1000. Loss of 0.7065, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 43/1000. Loss of 0.6588, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 44/1000. Loss of 0.6300, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 45/1000. Loss of 0.6247, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 46/1000. Loss of 0.6352, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 47/1000. Loss of 0.6545, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 48/1000. Loss of 0.6590, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 49/1000. Loss of 0.6536, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 50/1000. Loss of 0.6344, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 51/1000. Loss of 0.6223, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 52/1000. Loss of 0.5655, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 53/1000. Loss of 0.7671, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 54/1000. Loss of 0.6976, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 55/1000. Loss of 0.7860, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 56/1000. Loss of 0.6084, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 57/1000. Loss of 0.6541, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 58/1000. Loss of 0.5509, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 59/1000. Loss of 0.6460, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 60/1000. Loss of 0.6924, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 61/1000. Loss of 0.7152, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 62/1000. Loss of 0.6594, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 63/1000. Loss of 0.6362, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 64/1000. Loss of 0.6331, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 65/1000. Loss of 0.7682, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 66/1000. Loss of 0.6415, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 67/1000. Loss of 0.6333, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 68/1000. Loss of 0.6107, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 69/1000. Loss of 0.6841, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 70/1000. Loss of 0.7148, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 71/1000. Loss of 0.5870, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 72/1000. Loss of 0.6224, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 73/1000. Loss of 0.6532, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 74/1000. Loss of 0.6569, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 75/1000. Loss of 0.6041, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 76/1000. Loss of 0.6149, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 77/1000. Loss of 0.6661, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 78/1000. Loss of 0.7259, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 79/1000. Loss of 0.6921, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 80/1000. Loss of 0.6052, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 81/1000. Loss of 0.7243, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 82/1000. Loss of 0.6391, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 83/1000. Loss of 0.6128, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 84/1000. Loss of 0.6843, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 85/1000. Loss of 0.6050, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 86/1000. Loss of 0.5855, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 87/1000. Loss of 0.6301, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 88/1000. Loss of 0.5189, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 89/1000. Loss of 0.6802, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 90/1000. Loss of 0.6071, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 91/1000. Loss of 0.6025, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 92/1000. Loss of 0.5641, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 93/1000. Loss of 0.6816, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 94/1000. Loss of 0.5948, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 95/1000. Loss of 0.6458, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 96/1000. Loss of 0.7012, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 97/1000. Loss of 0.6045, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 98/1000. Loss of 0.5314, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 99/1000. Loss of 0.7399, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 100/1000. Loss of 0.6216, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 101/1000. Loss of 0.7275, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 102/1000. Loss of 0.5494, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 103/1000. Loss of 0.5945, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 104/1000. Loss of 0.7553, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 105/1000. Loss of 0.6636, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 106/1000. Loss of 0.6129, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 107/1000. Loss of 0.6152, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 108/1000. Loss of 0.5948, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 109/1000. Loss of 0.5626, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 110/1000. Loss of 0.6688, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 111/1000. Loss of 0.6665, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 112/1000. Loss of 0.6524, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 113/1000. Loss of 0.6627, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 114/1000. Loss of 0.6261, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 115/1000. Loss of 0.6679, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 116/1000. Loss of 0.6468, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 117/1000. Loss of 0.6692, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 118/1000. Loss of 0.5963, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 119/1000. Loss of 0.6961, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 120/1000. Loss of 0.5874, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 121/1000. Loss of 0.5890, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 122/1000. Loss of 0.6454, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 123/1000. Loss of 0.6809, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 124/1000. Loss of 0.6645, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 125/1000. Loss of 0.5901, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 126/1000. Loss of 0.6355, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 127/1000. Loss of 0.5965, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 128/1000. Loss of 0.7180, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 129/1000. Loss of 0.7675, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 130/1000. Loss of 0.6168, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 131/1000. Loss of 0.6058, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 132/1000. Loss of 0.6602, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 133/1000. Loss of 0.7004, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 134/1000. Loss of 0.6052, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 135/1000. Loss of 0.6204, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 136/1000. Loss of 0.6736, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 137/1000. Loss of 0.6721, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 138/1000. Loss of 0.7009, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 139/1000. Loss of 0.7745, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 140/1000. Loss of 0.6207, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 141/1000. Loss of 0.6766, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 142/1000. Loss of 0.7116, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 143/1000. Loss of 0.5849, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 144/1000. Loss of 0.7598, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 145/1000. Loss of 0.6556, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 146/1000. Loss of 0.6749, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 147/1000. Loss of 0.6162, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 148/1000. Loss of 0.5975, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 149/1000. Loss of 0.7386, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 150/1000. Loss of 0.6177, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 151/1000. Loss of 0.5779, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 152/1000. Loss of 0.7203, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 153/1000. Loss of 0.5674, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 154/1000. Loss of 0.5601, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 155/1000. Loss of 0.6437, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 156/1000. Loss of 0.5950, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 157/1000. Loss of 0.6439, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 158/1000. Loss of 0.6832, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 159/1000. Loss of 0.5893, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 160/1000. Loss of 0.6315, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 161/1000. Loss of 0.5765, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 162/1000. Loss of 0.6022, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 163/1000. Loss of 0.6445, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 164/1000. Loss of 0.5864, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 165/1000. Loss of 0.5684, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 166/1000. Loss of 0.6431, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 167/1000. Loss of 0.6456, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 168/1000. Loss of 0.6387, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 169/1000. Loss of 0.6139, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 170/1000. Loss of 0.6742, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 171/1000. Loss of 0.5724, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 172/1000. Loss of 0.5249, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 173/1000. Loss of 0.6450, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 174/1000. Loss of 0.6782, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 175/1000. Loss of 0.7266, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 176/1000. Loss of 0.6296, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 177/1000. Loss of 0.6488, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 178/1000. Loss of 0.6722, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 179/1000. Loss of 0.6739, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 180/1000. Loss of 0.6604, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 181/1000. Loss of 0.5963, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 182/1000. Loss of 0.6333, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 183/1000. Loss of 0.5654, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 184/1000. Loss of 0.8007, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 185/1000. Loss of 0.6996, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 186/1000. Loss of 0.5915, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 187/1000. Loss of 0.6211, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 188/1000. Loss of 0.6256, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 189/1000. Loss of 0.6728, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 190/1000. Loss of 0.5892, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 191/1000. Loss of 0.6463, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 192/1000. Loss of 0.6363, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 193/1000. Loss of 0.6521, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 194/1000. Loss of 0.7244, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 195/1000. Loss of 0.6045, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 196/1000. Loss of 0.6641, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 197/1000. Loss of 0.6327, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 198/1000. Loss of 0.5685, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 199/1000. Loss of 0.5931, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 200/1000. Loss of 0.6842, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 201/1000. Loss of 0.6041, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 202/1000. Loss of 0.6719, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 203/1000. Loss of 0.6246, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 204/1000. Loss of 0.6100, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 205/1000. Loss of 0.6276, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 206/1000. Loss of 0.6787, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 207/1000. Loss of 0.7089, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 208/1000. Loss of 0.6692, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 209/1000. Loss of 0.6591, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 210/1000. Loss of 0.6223, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 211/1000. Loss of 0.6733, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 212/1000. Loss of 0.6287, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 213/1000. Loss of 0.6346, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 214/1000. Loss of 0.6503, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 215/1000. Loss of 0.6109, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 216/1000. Loss of 0.6881, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 217/1000. Loss of 0.5757, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 218/1000. Loss of 0.6693, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 219/1000. Loss of 0.7428, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 220/1000. Loss of 0.6776, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 221/1000. Loss of 0.7339, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 222/1000. Loss of 0.7021, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 223/1000. Loss of 0.6198, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 224/1000. Loss of 0.6609, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 225/1000. Loss of 0.6930, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 226/1000. Loss of 0.6020, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 227/1000. Loss of 0.6301, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 228/1000. Loss of 0.6503, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 229/1000. Loss of 0.6724, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 230/1000. Loss of 0.6293, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 231/1000. Loss of 0.6300, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 232/1000. Loss of 0.6771, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 233/1000. Loss of 0.6195, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 234/1000. Loss of 0.6233, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 235/1000. Loss of 0.6384, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 236/1000. Loss of 0.6618, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 237/1000. Loss of 0.6274, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 238/1000. Loss of 0.5813, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 239/1000. Loss of 0.6180, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 240/1000. Loss of 0.6765, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 241/1000. Loss of 0.6932, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 242/1000. Loss of 0.6278, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 243/1000. Loss of 0.6728, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 244/1000. Loss of 0.5610, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 245/1000. Loss of 0.7308, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 246/1000. Loss of 0.5147, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 247/1000. Loss of 0.5968, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 248/1000. Loss of 0.6772, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 249/1000. Loss of 0.6594, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 250/1000. Loss of 0.6349, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 251/1000. Loss of 0.6180, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 252/1000. Loss of 0.7312, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 253/1000. Loss of 0.6335, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 254/1000. Loss of 0.5995, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 255/1000. Loss of 0.6060, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 256/1000. Loss of 0.7046, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 257/1000. Loss of 0.6987, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 258/1000. Loss of 0.7454, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 259/1000. Loss of 0.6493, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 260/1000. Loss of 0.6775, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 261/1000. Loss of 0.6741, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 262/1000. Loss of 0.6694, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 263/1000. Loss of 0.6646, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 264/1000. Loss of 0.5363, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 265/1000. Loss of 0.5769, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 266/1000. Loss of 0.5282, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 267/1000. Loss of 0.6240, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 268/1000. Loss of 0.6052, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 269/1000. Loss of 0.7208, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 270/1000. Loss of 0.5578, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 271/1000. Loss of 0.6511, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 272/1000. Loss of 0.6609, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 273/1000. Loss of 0.5730, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 274/1000. Loss of 0.7553, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 275/1000. Loss of 0.7172, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 276/1000. Loss of 0.6743, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 277/1000. Loss of 0.6611, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 278/1000. Loss of 0.6634, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 279/1000. Loss of 0.6221, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 280/1000. Loss of 0.7068, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 281/1000. Loss of 0.6422, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 282/1000. Loss of 0.7433, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 283/1000. Loss of 0.6825, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 284/1000. Loss of 0.6139, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 285/1000. Loss of 0.6936, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 286/1000. Loss of 0.6186, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 287/1000. Loss of 0.6587, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 288/1000. Loss of 0.5805, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 289/1000. Loss of 0.7582, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 290/1000. Loss of 0.6825, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 291/1000. Loss of 0.7278, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 292/1000. Loss of 0.5584, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 293/1000. Loss of 0.7837, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 294/1000. Loss of 0.6892, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 295/1000. Loss of 0.6630, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 296/1000. Loss of 0.6383, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 297/1000. Loss of 0.6327, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 298/1000. Loss of 0.6298, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 299/1000. Loss of 0.6401, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 300/1000. Loss of 0.6724, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 301/1000. Loss of 0.8162, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 302/1000. Loss of 0.6001, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 303/1000. Loss of 0.6221, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 304/1000. Loss of 0.7005, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 305/1000. Loss of 0.7280, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 306/1000. Loss of 0.7375, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 307/1000. Loss of 0.7914, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 308/1000. Loss of 0.7129, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 309/1000. Loss of 0.6618, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 310/1000. Loss of 0.6782, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 311/1000. Loss of 0.6965, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 312/1000. Loss of 0.6415, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 313/1000. Loss of 0.7648, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 314/1000. Loss of 0.6696, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 315/1000. Loss of 0.6550, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 316/1000. Loss of 0.6251, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 317/1000. Loss of 0.5799, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 318/1000. Loss of 0.6155, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 319/1000. Loss of 0.6521, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 320/1000. Loss of 0.6393, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 321/1000. Loss of 0.6637, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 322/1000. Loss of 0.6294, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 323/1000. Loss of 0.6029, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 324/1000. Loss of 0.6506, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 325/1000. Loss of 0.5913, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 326/1000. Loss of 0.7124, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 327/1000. Loss of 0.6256, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 328/1000. Loss of 0.6189, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 329/1000. Loss of 0.6413, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 330/1000. Loss of 0.6416, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 331/1000. Loss of 0.6918, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 332/1000. Loss of 0.7280, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 333/1000. Loss of 0.6241, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 334/1000. Loss of 0.6496, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 335/1000. Loss of 0.6826, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 336/1000. Loss of 0.5935, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 337/1000. Loss of 0.5814, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 338/1000. Loss of 0.6173, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 339/1000. Loss of 0.7078, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 340/1000. Loss of 0.7002, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 341/1000. Loss of 0.6347, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 342/1000. Loss of 0.6448, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 343/1000. Loss of 0.6119, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 344/1000. Loss of 0.6211, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 345/1000. Loss of 0.7192, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 346/1000. Loss of 0.6958, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 347/1000. Loss of 0.6585, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 348/1000. Loss of 0.6090, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 349/1000. Loss of 0.6203, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 350/1000. Loss of 0.6927, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 351/1000. Loss of 0.6287, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 352/1000. Loss of 0.5009, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 353/1000. Loss of 0.6031, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 354/1000. Loss of 0.6754, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 355/1000. Loss of 0.5806, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 356/1000. Loss of 0.6537, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 357/1000. Loss of 0.5594, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 358/1000. Loss of 0.6505, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 359/1000. Loss of 0.5947, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 360/1000. Loss of 0.5694, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 361/1000. Loss of 0.6817, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 362/1000. Loss of 0.6210, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 363/1000. Loss of 0.6030, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 364/1000. Loss of 0.5954, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 365/1000. Loss of 0.5859, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 366/1000. Loss of 0.7431, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 367/1000. Loss of 0.6974, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 368/1000. Loss of 0.5969, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 369/1000. Loss of 0.7524, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 370/1000. Loss of 0.6124, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 371/1000. Loss of 0.5900, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 372/1000. Loss of 0.7977, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 373/1000. Loss of 0.6149, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 374/1000. Loss of 0.6943, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 375/1000. Loss of 0.5953, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 376/1000. Loss of 0.6543, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 377/1000. Loss of 0.6474, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 378/1000. Loss of 0.5200, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 379/1000. Loss of 0.6908, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 380/1000. Loss of 0.5937, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 381/1000. Loss of 0.5886, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 382/1000. Loss of 0.6618, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 383/1000. Loss of 0.6583, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 384/1000. Loss of 0.6460, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 385/1000. Loss of 0.5933, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 386/1000. Loss of 0.6834, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 387/1000. Loss of 0.5702, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 388/1000. Loss of 0.5746, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 389/1000. Loss of 0.5904, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 390/1000. Loss of 0.5961, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 391/1000. Loss of 0.5570, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 392/1000. Loss of 0.7257, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 393/1000. Loss of 0.6694, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 394/1000. Loss of 0.7080, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 395/1000. Loss of 0.6850, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 396/1000. Loss of 0.6492, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 397/1000. Loss of 0.6388, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 398/1000. Loss of 0.5654, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 399/1000. Loss of 0.6324, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 400/1000. Loss of 0.6231, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 401/1000. Loss of 0.5970, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 402/1000. Loss of 0.6006, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 403/1000. Loss of 0.6132, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 404/1000. Loss of 0.7079, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 405/1000. Loss of 0.7701, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 406/1000. Loss of 0.5504, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 407/1000. Loss of 0.6560, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 408/1000. Loss of 0.6285, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 409/1000. Loss of 0.6783, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 410/1000. Loss of 0.6555, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 411/1000. Loss of 0.7004, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 412/1000. Loss of 0.6116, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 413/1000. Loss of 0.6195, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 414/1000. Loss of 0.5882, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 415/1000. Loss of 0.6107, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 416/1000. Loss of 0.7616, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 417/1000. Loss of 0.6633, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 418/1000. Loss of 0.6704, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 419/1000. Loss of 0.6442, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 420/1000. Loss of 0.5711, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 421/1000. Loss of 0.7725, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 422/1000. Loss of 0.6467, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 423/1000. Loss of 0.6425, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 424/1000. Loss of 0.6186, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 425/1000. Loss of 0.5483, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 426/1000. Loss of 0.6547, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 427/1000. Loss of 0.7216, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 428/1000. Loss of 0.6665, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 429/1000. Loss of 0.6113, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 430/1000. Loss of 0.6403, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 431/1000. Loss of 0.5641, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 432/1000. Loss of 0.7617, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 433/1000. Loss of 0.6953, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 434/1000. Loss of 0.6344, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 435/1000. Loss of 0.5745, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 436/1000. Loss of 0.6386, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 437/1000. Loss of 0.6733, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 438/1000. Loss of 0.6682, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 439/1000. Loss of 0.5696, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 440/1000. Loss of 0.7177, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 441/1000. Loss of 0.5798, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 442/1000. Loss of 0.6650, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 443/1000. Loss of 0.5504, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 444/1000. Loss of 0.6366, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 445/1000. Loss of 0.6421, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 446/1000. Loss of 0.6656, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 447/1000. Loss of 0.6201, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 448/1000. Loss of 0.6319, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 449/1000. Loss of 0.6869, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 450/1000. Loss of 0.5797, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 451/1000. Loss of 0.6582, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 452/1000. Loss of 0.6546, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 453/1000. Loss of 0.6167, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 454/1000. Loss of 0.6697, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 455/1000. Loss of 0.6652, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 456/1000. Loss of 0.6589, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 457/1000. Loss of 0.6983, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 458/1000. Loss of 0.6369, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 459/1000. Loss of 0.7006, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 460/1000. Loss of 0.7988, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 461/1000. Loss of 0.7186, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 462/1000. Loss of 0.7160, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 463/1000. Loss of 0.7122, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 464/1000. Loss of 0.5881, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 465/1000. Loss of 0.6312, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 466/1000. Loss of 0.6482, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 467/1000. Loss of 0.6384, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 468/1000. Loss of 0.6378, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 469/1000. Loss of 0.6984, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 470/1000. Loss of 0.6698, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 471/1000. Loss of 0.6659, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 472/1000. Loss of 0.5450, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 473/1000. Loss of 0.6359, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 474/1000. Loss of 0.6812, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 475/1000. Loss of 0.7016, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 476/1000. Loss of 0.7196, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 477/1000. Loss of 0.6669, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 478/1000. Loss of 0.7306, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 479/1000. Loss of 0.6327, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 480/1000. Loss of 0.5606, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 481/1000. Loss of 0.5673, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 482/1000. Loss of 0.6518, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 483/1000. Loss of 0.6451, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 484/1000. Loss of 0.6425, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 485/1000. Loss of 0.6128, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 486/1000. Loss of 0.6312, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 487/1000. Loss of 0.6899, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 488/1000. Loss of 0.7631, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 489/1000. Loss of 0.5860, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 490/1000. Loss of 0.6550, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 491/1000. Loss of 0.6506, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 492/1000. Loss of 0.5596, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 493/1000. Loss of 0.6025, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 494/1000. Loss of 0.6109, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 495/1000. Loss of 0.6365, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 496/1000. Loss of 0.7390, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 497/1000. Loss of 0.5737, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 498/1000. Loss of 0.6533, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 499/1000. Loss of 0.7141, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 500/1000. Loss of 0.6206, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 501/1000. Loss of 0.5888, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 502/1000. Loss of 0.5756, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 503/1000. Loss of 0.6822, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 504/1000. Loss of 0.7110, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 505/1000. Loss of 0.6530, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 506/1000. Loss of 0.5635, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 507/1000. Loss of 0.5845, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 508/1000. Loss of 0.6097, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 509/1000. Loss of 0.6766, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 510/1000. Loss of 0.7234, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 511/1000. Loss of 0.6360, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 512/1000. Loss of 0.6592, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 513/1000. Loss of 0.6947, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 514/1000. Loss of 0.6691, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 515/1000. Loss of 0.7741, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 516/1000. Loss of 0.5897, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 517/1000. Loss of 0.5923, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 518/1000. Loss of 0.6446, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 519/1000. Loss of 0.6928, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 520/1000. Loss of 0.5456, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 521/1000. Loss of 0.6781, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 522/1000. Loss of 0.6259, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 523/1000. Loss of 0.6833, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 524/1000. Loss of 0.6401, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 525/1000. Loss of 0.6882, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 526/1000. Loss of 0.6560, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 527/1000. Loss of 0.6792, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 528/1000. Loss of 0.5603, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 529/1000. Loss of 0.7331, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 530/1000. Loss of 0.6357, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 531/1000. Loss of 0.7157, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 532/1000. Loss of 0.7689, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 533/1000. Loss of 0.6113, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 534/1000. Loss of 0.6798, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 535/1000. Loss of 0.6757, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 536/1000. Loss of 0.5638, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 537/1000. Loss of 0.6860, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 538/1000. Loss of 0.6452, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 539/1000. Loss of 0.6470, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 540/1000. Loss of 0.6250, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 541/1000. Loss of 0.5763, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 542/1000. Loss of 0.6229, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 543/1000. Loss of 0.6392, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 544/1000. Loss of 0.6955, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 545/1000. Loss of 0.6291, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 546/1000. Loss of 0.6083, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 547/1000. Loss of 0.7514, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 548/1000. Loss of 0.6506, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 549/1000. Loss of 0.6443, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 550/1000. Loss of 0.6968, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 551/1000. Loss of 0.6908, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 552/1000. Loss of 0.6426, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 553/1000. Loss of 0.6372, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 554/1000. Loss of 0.5546, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 555/1000. Loss of 0.5449, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 556/1000. Loss of 0.7023, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 557/1000. Loss of 0.6913, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 558/1000. Loss of 0.7223, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 559/1000. Loss of 0.6500, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 560/1000. Loss of 0.7106, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 561/1000. Loss of 0.7474, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 562/1000. Loss of 0.6730, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 563/1000. Loss of 0.7247, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 564/1000. Loss of 0.6828, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 565/1000. Loss of 0.6832, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 566/1000. Loss of 0.7011, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 567/1000. Loss of 0.6749, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 568/1000. Loss of 0.7199, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 569/1000. Loss of 0.7280, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 570/1000. Loss of 0.5663, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 571/1000. Loss of 0.5416, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 572/1000. Loss of 0.6877, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 573/1000. Loss of 0.6490, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 574/1000. Loss of 0.5958, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 575/1000. Loss of 0.5536, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 576/1000. Loss of 0.6661, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 577/1000. Loss of 0.6529, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 578/1000. Loss of 0.6692, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 579/1000. Loss of 0.6661, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 580/1000. Loss of 0.6188, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 581/1000. Loss of 0.7177, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 582/1000. Loss of 0.6980, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 583/1000. Loss of 0.7537, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 584/1000. Loss of 0.7053, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 585/1000. Loss of 0.6163, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 586/1000. Loss of 0.6693, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 587/1000. Loss of 0.7374, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 588/1000. Loss of 0.6917, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 589/1000. Loss of 0.6439, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 590/1000. Loss of 0.6924, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 591/1000. Loss of 0.5761, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 592/1000. Loss of 0.5311, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 593/1000. Loss of 0.5505, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 594/1000. Loss of 0.7500, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 595/1000. Loss of 0.6889, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 596/1000. Loss of 0.6727, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 597/1000. Loss of 0.6786, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 598/1000. Loss of 0.5674, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 599/1000. Loss of 0.6441, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 600/1000. Loss of 0.5711, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 601/1000. Loss of 0.6896, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 602/1000. Loss of 0.5998, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 603/1000. Loss of 0.6994, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 604/1000. Loss of 0.6216, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 605/1000. Loss of 0.6023, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 606/1000. Loss of 0.6063, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 607/1000. Loss of 0.7526, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 608/1000. Loss of 0.7017, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 609/1000. Loss of 0.5881, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 610/1000. Loss of 0.7193, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 611/1000. Loss of 0.5411, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 612/1000. Loss of 0.6320, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 613/1000. Loss of 0.6639, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 614/1000. Loss of 0.7685, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 615/1000. Loss of 0.6373, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 616/1000. Loss of 0.5329, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 617/1000. Loss of 0.7046, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 618/1000. Loss of 0.6957, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 619/1000. Loss of 0.6243, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 620/1000. Loss of 0.6006, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 621/1000. Loss of 0.7691, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 622/1000. Loss of 0.6048, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 623/1000. Loss of 0.5556, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 624/1000. Loss of 0.6200, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 625/1000. Loss of 0.6498, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 626/1000. Loss of 0.5969, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 627/1000. Loss of 0.6840, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 628/1000. Loss of 0.6892, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 629/1000. Loss of 0.6955, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 630/1000. Loss of 0.5778, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 631/1000. Loss of 0.5418, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 632/1000. Loss of 0.6881, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 633/1000. Loss of 0.6513, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 634/1000. Loss of 0.7005, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 635/1000. Loss of 0.6637, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 636/1000. Loss of 0.6795, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 637/1000. Loss of 0.6857, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 638/1000. Loss of 0.6693, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 639/1000. Loss of 0.6666, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 640/1000. Loss of 0.7016, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 641/1000. Loss of 0.6731, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 642/1000. Loss of 0.7333, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 643/1000. Loss of 0.7157, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 644/1000. Loss of 0.6672, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 645/1000. Loss of 0.6494, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 646/1000. Loss of 0.7428, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 647/1000. Loss of 0.5905, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 648/1000. Loss of 0.7029, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 649/1000. Loss of 0.6088, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 650/1000. Loss of 0.7187, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 651/1000. Loss of 0.6094, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 652/1000. Loss of 0.6755, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 653/1000. Loss of 0.6673, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 654/1000. Loss of 0.6157, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 655/1000. Loss of 0.6444, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 656/1000. Loss of 0.6549, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 657/1000. Loss of 0.7434, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 658/1000. Loss of 0.7060, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 659/1000. Loss of 0.6898, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 660/1000. Loss of 0.6301, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 661/1000. Loss of 0.7151, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 662/1000. Loss of 0.6007, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 663/1000. Loss of 0.7101, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 664/1000. Loss of 0.5998, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 665/1000. Loss of 0.6329, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 666/1000. Loss of 0.6560, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 667/1000. Loss of 0.6833, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 668/1000. Loss of 0.6913, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 669/1000. Loss of 0.7025, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 670/1000. Loss of 0.6338, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 671/1000. Loss of 0.7029, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 672/1000. Loss of 0.6985, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 673/1000. Loss of 0.6613, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 674/1000. Loss of 0.5831, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 675/1000. Loss of 0.6567, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 676/1000. Loss of 0.5918, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 677/1000. Loss of 0.6376, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 678/1000. Loss of 0.8004, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 679/1000. Loss of 0.6791, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 680/1000. Loss of 0.6442, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 681/1000. Loss of 0.6375, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 682/1000. Loss of 0.6010, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 683/1000. Loss of 0.6849, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 684/1000. Loss of 0.6183, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 685/1000. Loss of 0.6750, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 686/1000. Loss of 0.6226, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 687/1000. Loss of 0.5896, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 688/1000. Loss of 0.7716, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 689/1000. Loss of 0.6745, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 690/1000. Loss of 0.7282, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 691/1000. Loss of 0.6389, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 692/1000. Loss of 0.5939, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 693/1000. Loss of 0.6814, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 694/1000. Loss of 0.7506, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 695/1000. Loss of 0.5972, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 696/1000. Loss of 0.6434, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 697/1000. Loss of 0.6201, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 698/1000. Loss of 0.5767, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 699/1000. Loss of 0.7540, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 700/1000. Loss of 0.7064, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 701/1000. Loss of 0.7254, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 702/1000. Loss of 0.5937, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 703/1000. Loss of 0.5991, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 704/1000. Loss of 0.6370, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 705/1000. Loss of 0.6753, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 706/1000. Loss of 0.6096, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 707/1000. Loss of 0.6176, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 708/1000. Loss of 0.6303, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 709/1000. Loss of 0.6975, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 710/1000. Loss of 0.6358, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 711/1000. Loss of 0.6159, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 712/1000. Loss of 0.6315, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 713/1000. Loss of 0.6482, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 714/1000. Loss of 0.5558, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 715/1000. Loss of 0.6328, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 716/1000. Loss of 0.6316, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 717/1000. Loss of 0.5944, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 718/1000. Loss of 0.6688, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 719/1000. Loss of 0.6950, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 720/1000. Loss of 0.6893, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 721/1000. Loss of 0.6517, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 722/1000. Loss of 0.6529, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 723/1000. Loss of 0.6504, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 724/1000. Loss of 0.7128, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 725/1000. Loss of 0.5752, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 726/1000. Loss of 0.5280, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 727/1000. Loss of 0.6747, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 728/1000. Loss of 0.5740, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 729/1000. Loss of 0.6627, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 730/1000. Loss of 0.7021, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 731/1000. Loss of 0.6533, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 732/1000. Loss of 0.6416, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 733/1000. Loss of 0.6237, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 734/1000. Loss of 0.6991, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 735/1000. Loss of 0.6664, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 736/1000. Loss of 0.7179, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 737/1000. Loss of 0.6427, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 738/1000. Loss of 0.5337, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 739/1000. Loss of 0.6349, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 740/1000. Loss of 0.6723, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 741/1000. Loss of 0.5898, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 742/1000. Loss of 0.6681, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 743/1000. Loss of 0.5749, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 744/1000. Loss of 0.6651, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 745/1000. Loss of 0.6403, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 746/1000. Loss of 0.6020, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 747/1000. Loss of 0.5491, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 748/1000. Loss of 0.6393, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 749/1000. Loss of 0.5484, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 750/1000. Loss of 0.6840, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 751/1000. Loss of 0.6071, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 752/1000. Loss of 0.6405, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 753/1000. Loss of 0.6668, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 754/1000. Loss of 0.5174, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 755/1000. Loss of 0.6640, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 756/1000. Loss of 0.6557, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 757/1000. Loss of 0.6226, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 758/1000. Loss of 0.6023, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 759/1000. Loss of 0.6908, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 760/1000. Loss of 0.6093, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 761/1000. Loss of 0.5928, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 762/1000. Loss of 0.6646, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 763/1000. Loss of 0.6109, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 764/1000. Loss of 0.5631, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 765/1000. Loss of 0.6353, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 766/1000. Loss of 0.6419, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 767/1000. Loss of 0.7099, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 768/1000. Loss of 0.6585, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 769/1000. Loss of 0.6333, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 770/1000. Loss of 0.6242, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 771/1000. Loss of 0.6279, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 772/1000. Loss of 0.6704, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 773/1000. Loss of 0.7223, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 774/1000. Loss of 0.6274, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 775/1000. Loss of 0.6105, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 776/1000. Loss of 0.6469, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 777/1000. Loss of 0.4711, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 778/1000. Loss of 0.7614, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 779/1000. Loss of 0.5886, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 780/1000. Loss of 0.6674, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 781/1000. Loss of 0.5445, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 782/1000. Loss of 0.5958, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 783/1000. Loss of 0.7206, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 784/1000. Loss of 0.6215, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 785/1000. Loss of 0.5778, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 786/1000. Loss of 0.6518, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 787/1000. Loss of 0.5883, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 788/1000. Loss of 0.6673, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 789/1000. Loss of 0.6008, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 790/1000. Loss of 0.7888, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 791/1000. Loss of 0.5281, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 792/1000. Loss of 0.6321, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 793/1000. Loss of 0.6179, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 794/1000. Loss of 0.5936, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 795/1000. Loss of 0.6732, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 796/1000. Loss of 0.6415, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 797/1000. Loss of 0.8344, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 798/1000. Loss of 0.5769, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 799/1000. Loss of 0.6652, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 800/1000. Loss of 0.7243, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 801/1000. Loss of 0.6456, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 802/1000. Loss of 0.6228, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 803/1000. Loss of 0.7258, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 804/1000. Loss of 0.6634, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 805/1000. Loss of 0.6497, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 806/1000. Loss of 0.7225, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 807/1000. Loss of 0.6733, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 808/1000. Loss of 0.7239, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 809/1000. Loss of 0.7028, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 810/1000. Loss of 0.6699, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 811/1000. Loss of 0.5646, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 812/1000. Loss of 0.6833, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 813/1000. Loss of 0.7758, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 814/1000. Loss of 0.6259, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 815/1000. Loss of 0.5951, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 816/1000. Loss of 0.6831, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 817/1000. Loss of 0.6488, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 818/1000. Loss of 0.6038, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 819/1000. Loss of 0.5969, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 820/1000. Loss of 0.6316, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 821/1000. Loss of 0.7804, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 822/1000. Loss of 0.6110, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 823/1000. Loss of 0.7250, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 824/1000. Loss of 0.6769, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 825/1000. Loss of 0.6704, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 826/1000. Loss of 0.6087, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 827/1000. Loss of 0.7450, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 828/1000. Loss of 0.5423, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 829/1000. Loss of 0.6089, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 830/1000. Loss of 0.6524, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 831/1000. Loss of 0.6015, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 832/1000. Loss of 0.6871, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 833/1000. Loss of 0.6755, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 834/1000. Loss of 0.6630, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 835/1000. Loss of 0.6082, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 836/1000. Loss of 0.6966, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 837/1000. Loss of 0.5977, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 838/1000. Loss of 0.7145, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 839/1000. Loss of 0.7520, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 840/1000. Loss of 0.5876, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 841/1000. Loss of 0.7048, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 842/1000. Loss of 0.7140, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 843/1000. Loss of 0.7737, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 844/1000. Loss of 0.6359, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 845/1000. Loss of 0.6069, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 846/1000. Loss of 0.7081, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 847/1000. Loss of 0.5894, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 848/1000. Loss of 0.6228, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 849/1000. Loss of 0.7018, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 850/1000. Loss of 0.6745, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 851/1000. Loss of 0.7076, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 852/1000. Loss of 0.5931, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 853/1000. Loss of 0.6473, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 854/1000. Loss of 0.6770, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 855/1000. Loss of 0.6530, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 856/1000. Loss of 0.6178, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 857/1000. Loss of 0.7174, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 858/1000. Loss of 0.6920, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 859/1000. Loss of 0.7290, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 860/1000. Loss of 0.6805, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 861/1000. Loss of 0.6641, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 862/1000. Loss of 0.7029, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 863/1000. Loss of 0.6577, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 864/1000. Loss of 0.6840, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 865/1000. Loss of 0.6805, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 866/1000. Loss of 0.5478, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 867/1000. Loss of 0.6241, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 868/1000. Loss of 0.6289, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 869/1000. Loss of 0.5757, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 870/1000. Loss of 0.6209, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 871/1000. Loss of 0.7014, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 872/1000. Loss of 0.6938, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 873/1000. Loss of 0.6870, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 874/1000. Loss of 0.5777, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 875/1000. Loss of 0.6200, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 876/1000. Loss of 0.6457, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 877/1000. Loss of 0.6401, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 878/1000. Loss of 0.6229, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 879/1000. Loss of 0.6152, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 880/1000. Loss of 0.6728, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 881/1000. Loss of 0.6781, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 882/1000. Loss of 0.6474, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 883/1000. Loss of 0.7293, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 884/1000. Loss of 0.5557, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 885/1000. Loss of 0.7070, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 886/1000. Loss of 0.6357, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 887/1000. Loss of 0.6559, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 888/1000. Loss of 0.6916, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 889/1000. Loss of 0.5933, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 890/1000. Loss of 0.6558, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 891/1000. Loss of 0.7178, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 892/1000. Loss of 0.6113, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 893/1000. Loss of 0.6937, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 894/1000. Loss of 0.6017, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 895/1000. Loss of 0.6349, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 896/1000. Loss of 0.6538, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 897/1000. Loss of 0.5803, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 898/1000. Loss of 0.6551, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 899/1000. Loss of 0.6559, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 900/1000. Loss of 0.6801, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 901/1000. Loss of 0.6892, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 902/1000. Loss of 0.6760, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 903/1000. Loss of 0.5394, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 904/1000. Loss of 0.6321, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 905/1000. Loss of 0.6516, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 906/1000. Loss of 0.6258, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 907/1000. Loss of 0.6382, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 908/1000. Loss of 0.6432, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 909/1000. Loss of 0.6190, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 910/1000. Loss of 0.5932, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 911/1000. Loss of 0.6203, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 912/1000. Loss of 0.6615, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 913/1000. Loss of 0.7115, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 914/1000. Loss of 0.6506, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 915/1000. Loss of 0.5864, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 916/1000. Loss of 0.7725, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 917/1000. Loss of 0.6086, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 918/1000. Loss of 0.7039, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 919/1000. Loss of 0.6888, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 920/1000. Loss of 0.6764, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 921/1000. Loss of 0.6440, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 922/1000. Loss of 0.6489, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 923/1000. Loss of 0.6852, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 924/1000. Loss of 0.7068, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 925/1000. Loss of 0.6306, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 926/1000. Loss of 0.6909, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 927/1000. Loss of 0.6652, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 928/1000. Loss of 0.6180, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 929/1000. Loss of 0.5942, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 930/1000. Loss of 0.5972, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 931/1000. Loss of 0.6179, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 932/1000. Loss of 0.6325, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 933/1000. Loss of 0.5871, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 934/1000. Loss of 0.5936, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 935/1000. Loss of 0.6104, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 936/1000. Loss of 0.6236, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 937/1000. Loss of 0.6319, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 938/1000. Loss of 0.7097, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 939/1000. Loss of 0.5995, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 940/1000. Loss of 0.5856, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 941/1000. Loss of 0.6558, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 942/1000. Loss of 0.7053, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 943/1000. Loss of 0.7345, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 944/1000. Loss of 0.6225, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 945/1000. Loss of 0.6783, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 946/1000. Loss of 0.5743, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 947/1000. Loss of 0.6352, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 948/1000. Loss of 0.5903, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 949/1000. Loss of 0.6148, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 950/1000. Loss of 0.7958, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 951/1000. Loss of 0.6418, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 952/1000. Loss of 0.6547, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 953/1000. Loss of 0.7718, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 954/1000. Loss of 0.6214, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 955/1000. Loss of 0.6446, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 956/1000. Loss of 0.6664, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 957/1000. Loss of 0.6274, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 958/1000. Loss of 0.6875, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 959/1000. Loss of 0.7816, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 960/1000. Loss of 0.6338, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 961/1000. Loss of 0.5901, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 962/1000. Loss of 0.6466, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 963/1000. Loss of 0.6162, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 964/1000. Loss of 0.6953, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 965/1000. Loss of 0.6346, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 966/1000. Loss of 0.5516, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 967/1000. Loss of 0.6104, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 968/1000. Loss of 0.6061, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 969/1000. Loss of 0.6558, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 970/1000. Loss of 0.6372, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 971/1000. Loss of 0.7159, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 972/1000. Loss of 0.7218, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 973/1000. Loss of 0.5952, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 974/1000. Loss of 0.5841, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 975/1000. Loss of 0.7567, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 976/1000. Loss of 0.6481, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 977/1000. Loss of 0.6917, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 978/1000. Loss of 0.6447, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 979/1000. Loss of 0.7007, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 980/1000. Loss of 0.6006, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 981/1000. Loss of 0.6105, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 982/1000. Loss of 0.5956, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 983/1000. Loss of 0.7219, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 984/1000. Loss of 0.7436, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 985/1000. Loss of 0.6259, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 986/1000. Loss of 0.6198, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 987/1000. Loss of 0.6285, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 988/1000. Loss of 0.5604, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 989/1000. Loss of 0.6050, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 990/1000. Loss of 0.8071, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 991/1000. Loss of 0.6801, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 992/1000. Loss of 0.7123, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 993/1000. Loss of 0.7058, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 994/1000. Loss of 0.6704, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 995/1000. Loss of 0.7218, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 996/1000. Loss of 0.6181, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 997/1000. Loss of 0.6215, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 998/1000. Loss of 0.6573, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 999/1000. Loss of 0.6503, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 1000/1000. Loss of 0.6665, cover loss of 0.0003, secret loss of 0.0002\n",
            "Epoch [49/50], Average_loss: 0.6499\n",
            "Training: Batch 1/1000. Loss of 0.6799, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 2/1000. Loss of 0.6424, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 3/1000. Loss of 0.6692, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 4/1000. Loss of 0.6773, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 5/1000. Loss of 0.6254, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 6/1000. Loss of 0.7211, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 7/1000. Loss of 0.6976, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 8/1000. Loss of 0.5976, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 9/1000. Loss of 0.6291, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 10/1000. Loss of 0.6099, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 11/1000. Loss of 0.6204, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 12/1000. Loss of 0.7449, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 13/1000. Loss of 0.6192, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 14/1000. Loss of 0.5562, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 15/1000. Loss of 0.5848, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 16/1000. Loss of 0.6434, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 17/1000. Loss of 0.7057, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 18/1000. Loss of 0.7080, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 19/1000. Loss of 0.7391, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 20/1000. Loss of 0.5819, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 21/1000. Loss of 0.7930, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 22/1000. Loss of 0.6870, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 23/1000. Loss of 0.5841, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 24/1000. Loss of 0.6135, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 25/1000. Loss of 0.6562, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 26/1000. Loss of 0.6420, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 27/1000. Loss of 0.6817, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 28/1000. Loss of 0.6935, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 29/1000. Loss of 0.5815, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 30/1000. Loss of 0.6596, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 31/1000. Loss of 0.5880, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 32/1000. Loss of 0.6704, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 33/1000. Loss of 0.7664, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 34/1000. Loss of 0.5550, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 35/1000. Loss of 0.7099, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 36/1000. Loss of 0.5757, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 37/1000. Loss of 0.7117, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 38/1000. Loss of 0.7013, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 39/1000. Loss of 0.6582, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 40/1000. Loss of 0.5443, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 41/1000. Loss of 0.6106, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 42/1000. Loss of 0.6064, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 43/1000. Loss of 0.6944, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 44/1000. Loss of 0.6595, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 45/1000. Loss of 0.6497, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 46/1000. Loss of 0.5317, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 47/1000. Loss of 0.7822, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 48/1000. Loss of 0.6381, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 49/1000. Loss of 0.7750, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 50/1000. Loss of 0.5965, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 51/1000. Loss of 0.6795, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 52/1000. Loss of 0.6003, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 53/1000. Loss of 0.6386, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 54/1000. Loss of 0.6825, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 55/1000. Loss of 0.6683, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 56/1000. Loss of 0.6603, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 57/1000. Loss of 0.6451, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 58/1000. Loss of 0.6874, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 59/1000. Loss of 0.6235, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 60/1000. Loss of 0.5929, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 61/1000. Loss of 0.6037, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 62/1000. Loss of 0.7834, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 63/1000. Loss of 0.6737, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 64/1000. Loss of 0.6499, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 65/1000. Loss of 0.6921, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 66/1000. Loss of 0.5213, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 67/1000. Loss of 0.5893, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 68/1000. Loss of 0.7226, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 69/1000. Loss of 0.5895, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 70/1000. Loss of 0.6402, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 71/1000. Loss of 0.5773, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 72/1000. Loss of 0.6520, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 73/1000. Loss of 0.6503, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 74/1000. Loss of 0.5951, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 75/1000. Loss of 0.6738, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 76/1000. Loss of 0.6910, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 77/1000. Loss of 0.6041, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 78/1000. Loss of 0.6123, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 79/1000. Loss of 0.6414, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 80/1000. Loss of 0.6713, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 81/1000. Loss of 0.5635, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 82/1000. Loss of 0.5996, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 83/1000. Loss of 0.7089, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 84/1000. Loss of 0.7246, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 85/1000. Loss of 0.6105, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 86/1000. Loss of 0.7517, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 87/1000. Loss of 0.6047, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 88/1000. Loss of 0.5630, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 89/1000. Loss of 0.6274, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 90/1000. Loss of 0.6484, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 91/1000. Loss of 0.6066, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 92/1000. Loss of 0.6610, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 93/1000. Loss of 0.6329, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 94/1000. Loss of 0.7239, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 95/1000. Loss of 0.5972, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 96/1000. Loss of 0.6452, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 97/1000. Loss of 0.6381, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 98/1000. Loss of 0.7345, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 99/1000. Loss of 0.7093, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 100/1000. Loss of 0.5823, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 101/1000. Loss of 0.6030, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 102/1000. Loss of 0.6333, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 103/1000. Loss of 0.6313, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 104/1000. Loss of 0.6686, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 105/1000. Loss of 0.5978, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 106/1000. Loss of 0.6155, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 107/1000. Loss of 0.6938, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 108/1000. Loss of 0.6567, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 109/1000. Loss of 0.6283, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 110/1000. Loss of 0.6603, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 111/1000. Loss of 0.6518, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 112/1000. Loss of 0.6047, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 113/1000. Loss of 0.6868, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 114/1000. Loss of 0.7370, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 115/1000. Loss of 0.5774, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 116/1000. Loss of 0.6846, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 117/1000. Loss of 0.7075, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 118/1000. Loss of 0.7139, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 119/1000. Loss of 0.6416, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 120/1000. Loss of 0.6525, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 121/1000. Loss of 0.7317, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 122/1000. Loss of 0.6104, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 123/1000. Loss of 0.5937, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 124/1000. Loss of 0.7392, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 125/1000. Loss of 0.6700, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 126/1000. Loss of 0.6629, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 127/1000. Loss of 0.6006, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 128/1000. Loss of 0.6742, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 129/1000. Loss of 0.6875, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 130/1000. Loss of 0.6812, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 131/1000. Loss of 0.6266, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 132/1000. Loss of 0.5914, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 133/1000. Loss of 0.6734, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 134/1000. Loss of 0.6656, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 135/1000. Loss of 0.5819, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 136/1000. Loss of 0.6578, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 137/1000. Loss of 0.6842, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 138/1000. Loss of 0.6078, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 139/1000. Loss of 0.7597, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 140/1000. Loss of 0.6246, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 141/1000. Loss of 0.6827, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 142/1000. Loss of 0.6394, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 143/1000. Loss of 0.7337, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 144/1000. Loss of 0.6759, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 145/1000. Loss of 0.7069, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 146/1000. Loss of 0.5552, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 147/1000. Loss of 0.6986, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 148/1000. Loss of 0.5926, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 149/1000. Loss of 0.5446, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 150/1000. Loss of 0.7340, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 151/1000. Loss of 0.5741, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 152/1000. Loss of 0.6952, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 153/1000. Loss of 0.5763, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 154/1000. Loss of 0.6444, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 155/1000. Loss of 0.5748, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 156/1000. Loss of 0.6074, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 157/1000. Loss of 0.6885, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 158/1000. Loss of 0.7256, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 159/1000. Loss of 0.7952, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 160/1000. Loss of 0.6799, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 161/1000. Loss of 0.7610, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 162/1000. Loss of 0.6124, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 163/1000. Loss of 0.6690, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 164/1000. Loss of 0.8143, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 165/1000. Loss of 0.6437, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 166/1000. Loss of 0.6475, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 167/1000. Loss of 0.6323, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 168/1000. Loss of 0.6067, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 169/1000. Loss of 0.5741, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 170/1000. Loss of 0.6842, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 171/1000. Loss of 0.6845, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 172/1000. Loss of 0.4932, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 173/1000. Loss of 0.6597, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 174/1000. Loss of 0.6077, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 175/1000. Loss of 0.6211, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 176/1000. Loss of 0.6750, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 177/1000. Loss of 0.5796, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 178/1000. Loss of 0.6249, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 179/1000. Loss of 0.5828, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 180/1000. Loss of 0.6598, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 181/1000. Loss of 0.6203, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 182/1000. Loss of 0.6501, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 183/1000. Loss of 0.6545, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 184/1000. Loss of 0.6347, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 185/1000. Loss of 0.7250, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 186/1000. Loss of 0.6220, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 187/1000. Loss of 0.6515, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 188/1000. Loss of 0.6347, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 189/1000. Loss of 0.6768, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 190/1000. Loss of 0.7381, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 191/1000. Loss of 0.7069, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 192/1000. Loss of 0.5866, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 193/1000. Loss of 0.6349, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 194/1000. Loss of 0.6833, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 195/1000. Loss of 0.6940, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 196/1000. Loss of 0.6234, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 197/1000. Loss of 0.7799, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 198/1000. Loss of 0.6343, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 199/1000. Loss of 0.7708, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 200/1000. Loss of 0.6107, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 201/1000. Loss of 0.5695, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 202/1000. Loss of 0.5666, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 203/1000. Loss of 0.5755, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 204/1000. Loss of 0.7528, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 205/1000. Loss of 0.6499, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 206/1000. Loss of 0.6379, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 207/1000. Loss of 0.8486, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 208/1000. Loss of 0.6401, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 209/1000. Loss of 0.5215, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 210/1000. Loss of 0.7412, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 211/1000. Loss of 0.6295, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 212/1000. Loss of 0.5892, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 213/1000. Loss of 0.5168, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 214/1000. Loss of 0.6309, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 215/1000. Loss of 0.7480, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 216/1000. Loss of 0.6884, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 217/1000. Loss of 0.7363, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 218/1000. Loss of 0.6570, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 219/1000. Loss of 0.6508, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 220/1000. Loss of 0.7244, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 221/1000. Loss of 0.6608, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 222/1000. Loss of 0.6458, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 223/1000. Loss of 0.6755, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 224/1000. Loss of 0.6561, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 225/1000. Loss of 0.6911, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 226/1000. Loss of 0.6322, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 227/1000. Loss of 0.5602, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 228/1000. Loss of 0.6100, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 229/1000. Loss of 0.5805, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 230/1000. Loss of 0.6721, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 231/1000. Loss of 0.7365, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 232/1000. Loss of 0.5860, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 233/1000. Loss of 0.5988, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 234/1000. Loss of 0.6913, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 235/1000. Loss of 0.7335, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 236/1000. Loss of 0.6689, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 237/1000. Loss of 0.7121, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 238/1000. Loss of 0.6498, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 239/1000. Loss of 0.7197, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 240/1000. Loss of 0.6664, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 241/1000. Loss of 0.7261, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 242/1000. Loss of 0.6417, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 243/1000. Loss of 0.7676, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 244/1000. Loss of 0.6696, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 245/1000. Loss of 0.6766, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 246/1000. Loss of 0.5972, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 247/1000. Loss of 0.6405, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 248/1000. Loss of 0.6924, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 249/1000. Loss of 0.6213, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 250/1000. Loss of 0.6300, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 251/1000. Loss of 0.7940, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 252/1000. Loss of 0.5714, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 253/1000. Loss of 0.6762, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 254/1000. Loss of 0.7297, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 255/1000. Loss of 0.7743, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 256/1000. Loss of 0.7261, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 257/1000. Loss of 0.6613, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 258/1000. Loss of 0.6935, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 259/1000. Loss of 0.5870, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 260/1000. Loss of 0.6127, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 261/1000. Loss of 0.6970, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 262/1000. Loss of 0.6392, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 263/1000. Loss of 0.6546, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 264/1000. Loss of 0.6647, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 265/1000. Loss of 0.6857, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 266/1000. Loss of 0.6978, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 267/1000. Loss of 0.6651, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 268/1000. Loss of 0.6201, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 269/1000. Loss of 0.5779, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 270/1000. Loss of 0.5833, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 271/1000. Loss of 0.6916, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 272/1000. Loss of 0.6504, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 273/1000. Loss of 0.7554, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 274/1000. Loss of 0.7239, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 275/1000. Loss of 0.6155, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 276/1000. Loss of 0.5888, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 277/1000. Loss of 0.5927, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 278/1000. Loss of 0.6084, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 279/1000. Loss of 0.6188, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 280/1000. Loss of 0.6900, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 281/1000. Loss of 0.6591, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 282/1000. Loss of 0.5540, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 283/1000. Loss of 0.6776, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 284/1000. Loss of 0.5939, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 285/1000. Loss of 0.6625, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 286/1000. Loss of 0.7688, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 287/1000. Loss of 0.6109, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 288/1000. Loss of 0.6338, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 289/1000. Loss of 0.7275, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 290/1000. Loss of 0.6695, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 291/1000. Loss of 0.7642, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 292/1000. Loss of 0.6658, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 293/1000. Loss of 0.7126, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 294/1000. Loss of 0.5525, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 295/1000. Loss of 0.6454, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 296/1000. Loss of 0.7728, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 297/1000. Loss of 0.5219, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 298/1000. Loss of 0.6828, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 299/1000. Loss of 0.7094, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 300/1000. Loss of 0.7291, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 301/1000. Loss of 0.5731, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 302/1000. Loss of 0.6858, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 303/1000. Loss of 0.5140, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 304/1000. Loss of 0.6150, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 305/1000. Loss of 0.6236, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 306/1000. Loss of 0.6546, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 307/1000. Loss of 0.7990, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 308/1000. Loss of 0.7696, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 309/1000. Loss of 0.6796, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 310/1000. Loss of 0.6005, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 311/1000. Loss of 0.7001, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 312/1000. Loss of 0.5568, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 313/1000. Loss of 0.5952, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 314/1000. Loss of 0.6266, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 315/1000. Loss of 0.7670, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 316/1000. Loss of 0.5773, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 317/1000. Loss of 0.6291, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 318/1000. Loss of 0.6391, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 319/1000. Loss of 0.5513, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 320/1000. Loss of 0.6388, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 321/1000. Loss of 0.6765, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 322/1000. Loss of 0.8018, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 323/1000. Loss of 0.6668, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 324/1000. Loss of 0.6379, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 325/1000. Loss of 0.6393, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 326/1000. Loss of 0.6773, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 327/1000. Loss of 0.6001, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 328/1000. Loss of 0.7152, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 329/1000. Loss of 0.7107, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 330/1000. Loss of 0.5863, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 331/1000. Loss of 0.5645, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 332/1000. Loss of 0.6860, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 333/1000. Loss of 0.6471, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 334/1000. Loss of 0.6818, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 335/1000. Loss of 0.6660, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 336/1000. Loss of 0.6737, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 337/1000. Loss of 0.7012, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 338/1000. Loss of 0.6372, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 339/1000. Loss of 0.6701, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 340/1000. Loss of 0.6082, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 341/1000. Loss of 0.5929, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 342/1000. Loss of 0.7225, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 343/1000. Loss of 0.6263, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 344/1000. Loss of 0.6179, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 345/1000. Loss of 0.6428, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 346/1000. Loss of 0.6183, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 347/1000. Loss of 0.6854, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 348/1000. Loss of 0.5406, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 349/1000. Loss of 0.7142, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 350/1000. Loss of 0.5761, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 351/1000. Loss of 0.6048, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 352/1000. Loss of 0.6038, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 353/1000. Loss of 0.7063, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 354/1000. Loss of 0.5730, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 355/1000. Loss of 0.7011, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 356/1000. Loss of 0.5733, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 357/1000. Loss of 0.6584, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 358/1000. Loss of 0.6748, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 359/1000. Loss of 0.6401, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 360/1000. Loss of 0.6090, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 361/1000. Loss of 0.6252, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 362/1000. Loss of 0.5875, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 363/1000. Loss of 0.7436, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 364/1000. Loss of 0.7219, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 365/1000. Loss of 0.6507, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 366/1000. Loss of 0.7188, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 367/1000. Loss of 0.6328, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 368/1000. Loss of 0.6036, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 369/1000. Loss of 0.6803, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 370/1000. Loss of 0.7039, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 371/1000. Loss of 0.6571, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 372/1000. Loss of 0.5520, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 373/1000. Loss of 0.6448, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 374/1000. Loss of 0.6606, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 375/1000. Loss of 0.5887, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 376/1000. Loss of 0.6833, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 377/1000. Loss of 0.7037, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 378/1000. Loss of 0.6185, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 379/1000. Loss of 0.6584, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 380/1000. Loss of 0.6235, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 381/1000. Loss of 0.6886, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 382/1000. Loss of 0.6281, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 383/1000. Loss of 0.6231, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 384/1000. Loss of 0.7272, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 385/1000. Loss of 0.5318, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 386/1000. Loss of 0.7524, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 387/1000. Loss of 0.6413, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 388/1000. Loss of 0.6685, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 389/1000. Loss of 0.6385, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 390/1000. Loss of 0.6186, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 391/1000. Loss of 0.6395, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 392/1000. Loss of 0.6429, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 393/1000. Loss of 0.6691, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 394/1000. Loss of 0.6419, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 395/1000. Loss of 0.6165, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 396/1000. Loss of 0.7099, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 397/1000. Loss of 0.6220, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 398/1000. Loss of 0.6467, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 399/1000. Loss of 0.6220, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 400/1000. Loss of 0.5671, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 401/1000. Loss of 0.6689, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 402/1000. Loss of 0.7059, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 403/1000. Loss of 0.5969, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 404/1000. Loss of 0.7432, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 405/1000. Loss of 0.6559, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 406/1000. Loss of 0.6546, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 407/1000. Loss of 0.6381, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 408/1000. Loss of 0.6256, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 409/1000. Loss of 0.6396, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 410/1000. Loss of 0.6767, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 411/1000. Loss of 0.6477, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 412/1000. Loss of 0.6593, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 413/1000. Loss of 0.6349, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 414/1000. Loss of 0.6226, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 415/1000. Loss of 0.6610, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 416/1000. Loss of 0.7563, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 417/1000. Loss of 0.6456, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 418/1000. Loss of 0.6804, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 419/1000. Loss of 0.6334, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 420/1000. Loss of 0.5837, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 421/1000. Loss of 0.6580, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 422/1000. Loss of 0.6852, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 423/1000. Loss of 0.6907, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 424/1000. Loss of 0.6731, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 425/1000. Loss of 0.7251, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 426/1000. Loss of 0.6335, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 427/1000. Loss of 0.6989, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 428/1000. Loss of 0.5560, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 429/1000. Loss of 0.7077, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 430/1000. Loss of 0.7261, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 431/1000. Loss of 0.6778, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 432/1000. Loss of 0.5110, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 433/1000. Loss of 0.6451, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 434/1000. Loss of 0.6742, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 435/1000. Loss of 0.6634, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 436/1000. Loss of 0.6359, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 437/1000. Loss of 0.6540, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 438/1000. Loss of 0.6107, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 439/1000. Loss of 0.6808, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 440/1000. Loss of 0.5630, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 441/1000. Loss of 0.7398, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 442/1000. Loss of 0.7265, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 443/1000. Loss of 0.7506, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 444/1000. Loss of 0.6970, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 445/1000. Loss of 0.5739, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 446/1000. Loss of 0.5890, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 447/1000. Loss of 0.5807, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 448/1000. Loss of 0.6442, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 449/1000. Loss of 0.7045, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 450/1000. Loss of 0.6070, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 451/1000. Loss of 0.7085, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 452/1000. Loss of 0.6531, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 453/1000. Loss of 0.6687, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 454/1000. Loss of 0.6504, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 455/1000. Loss of 0.6568, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 456/1000. Loss of 0.7060, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 457/1000. Loss of 0.7286, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 458/1000. Loss of 0.5662, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 459/1000. Loss of 0.6303, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 460/1000. Loss of 0.7261, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 461/1000. Loss of 0.6633, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 462/1000. Loss of 0.5568, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 463/1000. Loss of 0.6095, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 464/1000. Loss of 0.6899, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 465/1000. Loss of 0.5700, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 466/1000. Loss of 0.6172, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 467/1000. Loss of 0.7044, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 468/1000. Loss of 0.6658, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 469/1000. Loss of 0.6764, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 470/1000. Loss of 0.6515, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 471/1000. Loss of 0.6381, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 472/1000. Loss of 0.7516, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 473/1000. Loss of 0.6498, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 474/1000. Loss of 0.6150, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 475/1000. Loss of 0.6423, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 476/1000. Loss of 0.6039, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 477/1000. Loss of 0.6121, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 478/1000. Loss of 0.5752, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 479/1000. Loss of 0.6133, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 480/1000. Loss of 0.6149, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 481/1000. Loss of 0.7144, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 482/1000. Loss of 0.5539, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 483/1000. Loss of 0.6111, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 484/1000. Loss of 0.6593, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 485/1000. Loss of 0.6662, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 486/1000. Loss of 0.6298, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 487/1000. Loss of 0.6497, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 488/1000. Loss of 0.6815, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 489/1000. Loss of 0.6705, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 490/1000. Loss of 0.6812, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 491/1000. Loss of 0.6430, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 492/1000. Loss of 0.5888, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 493/1000. Loss of 0.5894, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 494/1000. Loss of 0.5561, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 495/1000. Loss of 0.6480, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 496/1000. Loss of 0.6182, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 497/1000. Loss of 0.6684, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 498/1000. Loss of 0.6905, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 499/1000. Loss of 0.7048, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 500/1000. Loss of 0.6436, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 501/1000. Loss of 0.6830, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 502/1000. Loss of 0.6194, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 503/1000. Loss of 0.6325, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 504/1000. Loss of 0.6575, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 505/1000. Loss of 0.6760, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 506/1000. Loss of 0.6774, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 507/1000. Loss of 0.6493, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 508/1000. Loss of 0.5369, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 509/1000. Loss of 0.6006, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 510/1000. Loss of 0.6074, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 511/1000. Loss of 0.6728, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 512/1000. Loss of 0.6148, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 513/1000. Loss of 0.6536, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 514/1000. Loss of 0.6644, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 515/1000. Loss of 0.6314, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 516/1000. Loss of 0.6964, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 517/1000. Loss of 0.6166, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 518/1000. Loss of 0.6221, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 519/1000. Loss of 0.6629, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 520/1000. Loss of 0.5250, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 521/1000. Loss of 0.6672, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 522/1000. Loss of 0.7229, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 523/1000. Loss of 0.5998, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 524/1000. Loss of 0.7031, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 525/1000. Loss of 0.6169, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 526/1000. Loss of 0.6954, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 527/1000. Loss of 0.7225, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 528/1000. Loss of 0.6482, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 529/1000. Loss of 0.5768, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 530/1000. Loss of 0.6885, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 531/1000. Loss of 0.6548, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 532/1000. Loss of 0.6506, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 533/1000. Loss of 0.7000, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 534/1000. Loss of 0.6274, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 535/1000. Loss of 0.6700, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 536/1000. Loss of 0.5838, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 537/1000. Loss of 0.6538, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 538/1000. Loss of 0.6231, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 539/1000. Loss of 0.6040, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 540/1000. Loss of 0.7738, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 541/1000. Loss of 0.6390, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 542/1000. Loss of 0.6458, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 543/1000. Loss of 0.5440, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 544/1000. Loss of 0.6369, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 545/1000. Loss of 0.6738, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 546/1000. Loss of 0.5851, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 547/1000. Loss of 0.6071, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 548/1000. Loss of 0.6204, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 549/1000. Loss of 0.6215, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 550/1000. Loss of 0.7380, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 551/1000. Loss of 0.6597, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 552/1000. Loss of 0.6178, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 553/1000. Loss of 0.6394, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 554/1000. Loss of 0.6654, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 555/1000. Loss of 0.6425, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 556/1000. Loss of 0.6118, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 557/1000. Loss of 0.6578, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 558/1000. Loss of 0.5556, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 559/1000. Loss of 0.6702, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 560/1000. Loss of 0.6794, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 561/1000. Loss of 0.7677, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 562/1000. Loss of 0.7104, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 563/1000. Loss of 0.7260, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 564/1000. Loss of 0.7100, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 565/1000. Loss of 0.5640, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 566/1000. Loss of 0.7136, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 567/1000. Loss of 0.6090, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 568/1000. Loss of 0.6460, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 569/1000. Loss of 0.7471, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 570/1000. Loss of 0.5884, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 571/1000. Loss of 0.6506, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 572/1000. Loss of 0.6565, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 573/1000. Loss of 0.6642, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 574/1000. Loss of 0.5996, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 575/1000. Loss of 0.5595, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 576/1000. Loss of 0.7454, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 577/1000. Loss of 0.6396, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 578/1000. Loss of 0.6528, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 579/1000. Loss of 0.5789, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 580/1000. Loss of 0.6140, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 581/1000. Loss of 0.6892, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 582/1000. Loss of 0.7572, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 583/1000. Loss of 0.6390, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 584/1000. Loss of 0.6006, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 585/1000. Loss of 0.6369, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 586/1000. Loss of 0.5787, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 587/1000. Loss of 0.5750, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 588/1000. Loss of 0.6355, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 589/1000. Loss of 0.6409, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 590/1000. Loss of 0.6147, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 591/1000. Loss of 0.6505, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 592/1000. Loss of 0.6683, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 593/1000. Loss of 0.6749, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 594/1000. Loss of 0.5743, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 595/1000. Loss of 0.6686, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 596/1000. Loss of 0.5673, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 597/1000. Loss of 0.6452, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 598/1000. Loss of 0.5968, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 599/1000. Loss of 0.6648, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 600/1000. Loss of 0.6736, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 601/1000. Loss of 0.6853, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 602/1000. Loss of 0.6475, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 603/1000. Loss of 0.7522, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 604/1000. Loss of 0.6895, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 605/1000. Loss of 0.7136, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 606/1000. Loss of 0.6034, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 607/1000. Loss of 0.7420, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 608/1000. Loss of 0.6277, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 609/1000. Loss of 0.6042, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 610/1000. Loss of 0.6448, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 611/1000. Loss of 0.5934, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 612/1000. Loss of 0.6742, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 613/1000. Loss of 0.6574, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 614/1000. Loss of 0.6063, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 615/1000. Loss of 0.6939, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 616/1000. Loss of 0.5725, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 617/1000. Loss of 0.6078, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 618/1000. Loss of 0.7166, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 619/1000. Loss of 0.6218, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 620/1000. Loss of 0.7752, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 621/1000. Loss of 0.6644, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 622/1000. Loss of 0.6117, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 623/1000. Loss of 0.6330, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 624/1000. Loss of 0.5501, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 625/1000. Loss of 0.5832, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 626/1000. Loss of 0.7157, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 627/1000. Loss of 0.7047, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 628/1000. Loss of 0.6660, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 629/1000. Loss of 0.6791, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 630/1000. Loss of 0.7301, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 631/1000. Loss of 0.5853, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 632/1000. Loss of 0.6373, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 633/1000. Loss of 0.7122, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 634/1000. Loss of 0.5612, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 635/1000. Loss of 0.6455, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 636/1000. Loss of 0.6817, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 637/1000. Loss of 0.6620, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 638/1000. Loss of 0.6056, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 639/1000. Loss of 0.6524, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 640/1000. Loss of 0.7204, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 641/1000. Loss of 0.6578, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 642/1000. Loss of 0.6095, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 643/1000. Loss of 0.6357, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 644/1000. Loss of 0.7259, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 645/1000. Loss of 0.6496, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 646/1000. Loss of 0.7046, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 647/1000. Loss of 0.6102, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 648/1000. Loss of 0.5806, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 649/1000. Loss of 0.6169, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 650/1000. Loss of 0.7279, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 651/1000. Loss of 0.6231, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 652/1000. Loss of 0.6606, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 653/1000. Loss of 0.6938, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 654/1000. Loss of 0.5765, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 655/1000. Loss of 0.7271, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 656/1000. Loss of 0.6673, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 657/1000. Loss of 0.6622, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 658/1000. Loss of 0.5882, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 659/1000. Loss of 0.6526, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 660/1000. Loss of 0.6924, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 661/1000. Loss of 0.6418, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 662/1000. Loss of 0.6590, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 663/1000. Loss of 0.7434, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 664/1000. Loss of 0.6324, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 665/1000. Loss of 0.6120, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 666/1000. Loss of 0.7132, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 667/1000. Loss of 0.6308, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 668/1000. Loss of 0.6161, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 669/1000. Loss of 0.6583, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 670/1000. Loss of 0.6112, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 671/1000. Loss of 0.6558, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 672/1000. Loss of 0.6322, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 673/1000. Loss of 0.6900, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 674/1000. Loss of 0.6762, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 675/1000. Loss of 0.6781, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 676/1000. Loss of 0.7717, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 677/1000. Loss of 0.6757, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 678/1000. Loss of 0.6746, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 679/1000. Loss of 0.7544, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 680/1000. Loss of 0.6982, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 681/1000. Loss of 0.6455, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 682/1000. Loss of 0.5955, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 683/1000. Loss of 0.7207, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 684/1000. Loss of 0.6753, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 685/1000. Loss of 0.6258, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 686/1000. Loss of 0.6537, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 687/1000. Loss of 0.6095, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 688/1000. Loss of 0.6147, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 689/1000. Loss of 0.5661, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 690/1000. Loss of 0.6460, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 691/1000. Loss of 0.5348, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 692/1000. Loss of 0.5849, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 693/1000. Loss of 0.6311, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 694/1000. Loss of 0.6423, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 695/1000. Loss of 0.6359, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 696/1000. Loss of 0.6193, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 697/1000. Loss of 0.6442, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 698/1000. Loss of 0.6601, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 699/1000. Loss of 0.7507, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 700/1000. Loss of 0.6170, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 701/1000. Loss of 0.7277, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 702/1000. Loss of 0.6549, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 703/1000. Loss of 0.5626, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 704/1000. Loss of 0.6917, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 705/1000. Loss of 0.6416, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 706/1000. Loss of 0.5380, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 707/1000. Loss of 0.6501, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 708/1000. Loss of 0.6173, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 709/1000. Loss of 0.6614, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 710/1000. Loss of 0.6874, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 711/1000. Loss of 0.6937, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 712/1000. Loss of 0.6282, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 713/1000. Loss of 0.7059, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 714/1000. Loss of 0.5626, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 715/1000. Loss of 0.5856, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 716/1000. Loss of 0.6301, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 717/1000. Loss of 0.5982, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 718/1000. Loss of 0.5779, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 719/1000. Loss of 0.7564, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 720/1000. Loss of 0.5040, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 721/1000. Loss of 0.6760, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 722/1000. Loss of 0.4926, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 723/1000. Loss of 0.5895, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 724/1000. Loss of 0.6472, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 725/1000. Loss of 0.6702, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 726/1000. Loss of 0.6562, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 727/1000. Loss of 0.6125, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 728/1000. Loss of 0.5662, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 729/1000. Loss of 0.6841, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 730/1000. Loss of 0.6543, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 731/1000. Loss of 0.6001, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 732/1000. Loss of 0.6610, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 733/1000. Loss of 0.6305, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 734/1000. Loss of 0.6276, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 735/1000. Loss of 0.6551, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 736/1000. Loss of 0.6520, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 737/1000. Loss of 0.5902, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 738/1000. Loss of 0.7182, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 739/1000. Loss of 0.6877, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 740/1000. Loss of 0.6950, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 741/1000. Loss of 0.6137, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 742/1000. Loss of 0.6856, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 743/1000. Loss of 0.5685, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 744/1000. Loss of 0.6819, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 745/1000. Loss of 0.6250, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 746/1000. Loss of 0.7109, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 747/1000. Loss of 0.5737, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 748/1000. Loss of 0.6795, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 749/1000. Loss of 0.6527, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 750/1000. Loss of 0.7180, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 751/1000. Loss of 0.6793, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 752/1000. Loss of 0.5493, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 753/1000. Loss of 0.7228, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 754/1000. Loss of 0.6261, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 755/1000. Loss of 0.6795, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 756/1000. Loss of 0.6520, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 757/1000. Loss of 0.6266, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 758/1000. Loss of 0.6442, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 759/1000. Loss of 0.5752, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 760/1000. Loss of 0.6162, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 761/1000. Loss of 0.6374, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 762/1000. Loss of 0.6144, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 763/1000. Loss of 0.6298, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 764/1000. Loss of 0.6405, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 765/1000. Loss of 0.6564, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 766/1000. Loss of 0.6105, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 767/1000. Loss of 0.7200, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 768/1000. Loss of 0.6560, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 769/1000. Loss of 0.6383, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 770/1000. Loss of 0.6545, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 771/1000. Loss of 0.6001, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 772/1000. Loss of 0.6747, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 773/1000. Loss of 0.6078, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 774/1000. Loss of 0.5652, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 775/1000. Loss of 0.6655, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 776/1000. Loss of 0.5944, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 777/1000. Loss of 0.5948, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 778/1000. Loss of 0.6586, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 779/1000. Loss of 0.6760, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 780/1000. Loss of 0.6567, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 781/1000. Loss of 0.6212, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 782/1000. Loss of 0.5892, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 783/1000. Loss of 0.6393, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 784/1000. Loss of 0.6507, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 785/1000. Loss of 0.6116, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 786/1000. Loss of 0.6516, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 787/1000. Loss of 0.6350, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 788/1000. Loss of 0.5898, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 789/1000. Loss of 0.6755, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 790/1000. Loss of 0.5597, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 791/1000. Loss of 0.6670, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 792/1000. Loss of 0.5963, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 793/1000. Loss of 0.7304, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 794/1000. Loss of 0.7468, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 795/1000. Loss of 0.5933, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 796/1000. Loss of 0.7411, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 797/1000. Loss of 0.6038, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 798/1000. Loss of 0.6032, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 799/1000. Loss of 0.6231, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 800/1000. Loss of 0.7271, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 801/1000. Loss of 0.7477, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 802/1000. Loss of 0.6601, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 803/1000. Loss of 0.6250, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 804/1000. Loss of 0.6445, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 805/1000. Loss of 0.6108, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 806/1000. Loss of 0.5882, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 807/1000. Loss of 0.6300, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 808/1000. Loss of 0.6493, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 809/1000. Loss of 0.6240, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 810/1000. Loss of 0.6791, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 811/1000. Loss of 0.6805, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 812/1000. Loss of 0.6497, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 813/1000. Loss of 0.7274, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 814/1000. Loss of 0.6861, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 815/1000. Loss of 0.6022, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 816/1000. Loss of 0.6644, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 817/1000. Loss of 0.6134, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 818/1000. Loss of 0.6982, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 819/1000. Loss of 0.6869, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 820/1000. Loss of 0.5427, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 821/1000. Loss of 0.7191, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 822/1000. Loss of 0.7035, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 823/1000. Loss of 0.5778, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 824/1000. Loss of 0.5684, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 825/1000. Loss of 0.7256, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 826/1000. Loss of 0.6252, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 827/1000. Loss of 0.6400, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 828/1000. Loss of 0.6441, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 829/1000. Loss of 0.7563, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 830/1000. Loss of 0.7454, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 831/1000. Loss of 0.7333, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 832/1000. Loss of 0.6425, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 833/1000. Loss of 0.6054, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 834/1000. Loss of 0.6714, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 835/1000. Loss of 0.6284, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 836/1000. Loss of 0.6737, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 837/1000. Loss of 0.4956, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 838/1000. Loss of 0.7261, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 839/1000. Loss of 0.6706, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 840/1000. Loss of 0.7343, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 841/1000. Loss of 0.6667, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 842/1000. Loss of 0.6391, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 843/1000. Loss of 0.6053, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 844/1000. Loss of 0.6334, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 845/1000. Loss of 0.7403, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 846/1000. Loss of 0.6539, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 847/1000. Loss of 0.6364, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 848/1000. Loss of 0.6801, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 849/1000. Loss of 0.6000, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 850/1000. Loss of 0.6105, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 851/1000. Loss of 0.7068, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 852/1000. Loss of 0.6187, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 853/1000. Loss of 0.6439, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 854/1000. Loss of 0.6745, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 855/1000. Loss of 0.6310, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 856/1000. Loss of 0.6906, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 857/1000. Loss of 0.6295, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 858/1000. Loss of 0.6579, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 859/1000. Loss of 0.6995, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 860/1000. Loss of 0.6477, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 861/1000. Loss of 0.6011, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 862/1000. Loss of 0.6399, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 863/1000. Loss of 0.6154, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 864/1000. Loss of 0.7115, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 865/1000. Loss of 0.6286, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 866/1000. Loss of 0.6874, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 867/1000. Loss of 0.5632, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 868/1000. Loss of 0.5862, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 869/1000. Loss of 0.5555, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 870/1000. Loss of 0.5961, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 871/1000. Loss of 0.5996, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 872/1000. Loss of 0.6396, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 873/1000. Loss of 0.7255, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 874/1000. Loss of 0.6209, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 875/1000. Loss of 0.6508, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 876/1000. Loss of 0.5996, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 877/1000. Loss of 0.6011, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 878/1000. Loss of 0.6008, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 879/1000. Loss of 0.6086, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 880/1000. Loss of 0.5490, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 881/1000. Loss of 0.6540, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 882/1000. Loss of 0.6285, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 883/1000. Loss of 0.6233, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 884/1000. Loss of 0.5823, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 885/1000. Loss of 0.6203, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 886/1000. Loss of 0.6415, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 887/1000. Loss of 0.6526, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 888/1000. Loss of 0.5694, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 889/1000. Loss of 0.6901, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 890/1000. Loss of 0.6138, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 891/1000. Loss of 0.6387, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 892/1000. Loss of 0.6334, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 893/1000. Loss of 0.6106, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 894/1000. Loss of 0.6140, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 895/1000. Loss of 0.6503, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 896/1000. Loss of 0.6658, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 897/1000. Loss of 0.6703, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 898/1000. Loss of 0.6835, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 899/1000. Loss of 0.7066, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 900/1000. Loss of 0.7100, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 901/1000. Loss of 0.5977, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 902/1000. Loss of 0.6465, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 903/1000. Loss of 0.5740, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 904/1000. Loss of 0.6831, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 905/1000. Loss of 0.6596, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 906/1000. Loss of 0.5634, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 907/1000. Loss of 0.7014, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 908/1000. Loss of 0.7049, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 909/1000. Loss of 0.6283, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 910/1000. Loss of 0.6993, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 911/1000. Loss of 0.7013, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 912/1000. Loss of 0.7086, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 913/1000. Loss of 0.5932, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 914/1000. Loss of 0.5745, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 915/1000. Loss of 0.6543, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 916/1000. Loss of 0.6342, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 917/1000. Loss of 0.6392, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 918/1000. Loss of 0.6247, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 919/1000. Loss of 0.5818, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 920/1000. Loss of 0.6866, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 921/1000. Loss of 0.6226, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 922/1000. Loss of 0.6794, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 923/1000. Loss of 0.6319, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 924/1000. Loss of 0.6520, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 925/1000. Loss of 0.6491, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 926/1000. Loss of 0.7107, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 927/1000. Loss of 0.7283, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 928/1000. Loss of 0.6700, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 929/1000. Loss of 0.7491, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 930/1000. Loss of 0.6464, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 931/1000. Loss of 0.6711, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 932/1000. Loss of 0.7487, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 933/1000. Loss of 0.5444, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 934/1000. Loss of 0.6361, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 935/1000. Loss of 0.6210, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 936/1000. Loss of 0.6499, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 937/1000. Loss of 0.8371, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 938/1000. Loss of 0.6847, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 939/1000. Loss of 0.6717, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 940/1000. Loss of 0.6752, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 941/1000. Loss of 0.7577, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 942/1000. Loss of 0.5099, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 943/1000. Loss of 0.5708, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 944/1000. Loss of 0.6657, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 945/1000. Loss of 0.6862, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 946/1000. Loss of 0.6664, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 947/1000. Loss of 0.5690, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 948/1000. Loss of 0.5947, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 949/1000. Loss of 0.6286, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 950/1000. Loss of 0.7491, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 951/1000. Loss of 0.6302, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 952/1000. Loss of 0.6209, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 953/1000. Loss of 0.6502, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 954/1000. Loss of 0.5928, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 955/1000. Loss of 0.6696, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 956/1000. Loss of 0.5977, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 957/1000. Loss of 0.6824, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 958/1000. Loss of 0.6896, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 959/1000. Loss of 0.6405, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 960/1000. Loss of 0.6702, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 961/1000. Loss of 0.5896, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 962/1000. Loss of 0.6700, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 963/1000. Loss of 0.6397, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 964/1000. Loss of 0.6774, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 965/1000. Loss of 0.5970, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 966/1000. Loss of 0.5968, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 967/1000. Loss of 0.6600, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 968/1000. Loss of 0.7272, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 969/1000. Loss of 0.7300, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 970/1000. Loss of 0.6286, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 971/1000. Loss of 0.6192, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 972/1000. Loss of 0.6064, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 973/1000. Loss of 0.6615, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 974/1000. Loss of 0.6944, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 975/1000. Loss of 0.6250, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 976/1000. Loss of 0.6054, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 977/1000. Loss of 0.7021, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 978/1000. Loss of 0.6076, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 979/1000. Loss of 0.6730, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 980/1000. Loss of 0.5586, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 981/1000. Loss of 0.5936, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 982/1000. Loss of 0.7306, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 983/1000. Loss of 0.6100, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 984/1000. Loss of 0.7015, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 985/1000. Loss of 0.8350, cover loss of 0.0003, secret loss of 0.0003\n",
            "Training: Batch 986/1000. Loss of 0.5858, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 987/1000. Loss of 0.7245, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 988/1000. Loss of 0.7419, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 989/1000. Loss of 0.7053, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 990/1000. Loss of 0.7878, cover loss of 0.0002, secret loss of 0.0003\n",
            "Training: Batch 991/1000. Loss of 0.6415, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 992/1000. Loss of 0.6451, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 993/1000. Loss of 0.6470, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 994/1000. Loss of 0.6373, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 995/1000. Loss of 0.5444, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 996/1000. Loss of 0.5733, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 997/1000. Loss of 0.7202, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 998/1000. Loss of 0.6041, cover loss of 0.0002, secret loss of 0.0002\n",
            "Training: Batch 999/1000. Loss of 0.6853, cover loss of 0.0003, secret loss of 0.0002\n",
            "Training: Batch 1000/1000. Loss of 0.7270, cover loss of 0.0003, secret loss of 0.0003\n",
            "Epoch [50/50], Average_loss: 0.6501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Plot loss through epochs\n",
        "plt.plot(loss_history)\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Batch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aU4ct5jW3q_A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "279f0b34-2a3c-44c6-de03-bedb94486b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pixel_errors(input_S, input_C, decoded_S, decoded_C):\n",
        "    \"\"\"Calculates mean of Sum of Squared Errors per pixel for cover and secret images. \"\"\"\n",
        "    input_S = np.array(input_S.detach().cpu())\n",
        "    input_C = np.array(input_C.detach().cpu())\n",
        "    decoded_S = np.array(decoded_S.detach().cpu())\n",
        "    decoded_C = np.array(decoded_C.detach().cpu())\n",
        "    # print(input_S)\n",
        "    see_Spixel = np.sqrt(np.mean(np.square(255*(input_S - decoded_S))))\n",
        "    see_Cpixel = np.sqrt(np.mean(np.square(255*(input_C - decoded_C))))\n",
        "    \n",
        "    return see_Spixel, see_Cpixel\n",
        "\n",
        "import math\n",
        "def psnr(input_S, input_C, decoded_S, decoded_C):\n",
        "    input_S = np.array(input_S.detach().cpu())\n",
        "    input_C = np.array(input_C.detach().cpu())\n",
        "    decoded_S = np.array(decoded_S.detach().cpu())\n",
        "    decoded_C = np.array(decoded_C.detach().cpu())\n",
        "    mse_s = np.mean(np.square(255*(input_S - decoded_S)))\n",
        "    mse_c = np.mean(np.square(255*(input_C - decoded_C)))\n",
        "    psnr_s = 10*math.log10(255**2/mse_s)\n",
        "    psnr_c = 10*math.log10(255**2/mse_c)\n",
        "    return psnr_s, psnr_c\n",
        "\n",
        "def pixel_histogram(diff_S, diff_C):\n",
        "    \"\"\"Calculates histograms of errors for cover and secret image. \"\"\"\n",
        "    diff_Sflat = diff_S.flatten()\n",
        "    diff_Cflat = diff_C.flatten()\n",
        "    \n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    a=fig.add_subplot(1,2,1)\n",
        "        \n",
        "    imgplot = plt.hist(255* diff_Cflat, 100, density=1, alpha=0.75, facecolor='red')\n",
        "    a.set_title('Distribution of error in the Cover image.')\n",
        "    plt.axis([0, 250, 0, 0.2])\n",
        "    \n",
        "    a=fig.add_subplot(1,2,2)\n",
        "    imgplot = plt.hist(255* diff_Sflat, 100, density=1, alpha=0.75, facecolor='red')\n",
        "    a.set_title('Distribution of errors in the Secret image.')\n",
        "    plt.axis([0, 250, 0, 0.2])\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qCW2najJdW0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# net.load_state_dict(torch.load(MODELS_PATH+'Epoch N4.pkl'))\n",
        "\n",
        "# Switch to evaluate mode\n",
        "net.eval()\n",
        "Path(PATH+\"ds_img_50\").mkdir(parents=True, exist_ok=True)\n",
        "Path(PATH+\"ds_img_50/cover_images/\").mkdir(parents=True, exist_ok=True)\n",
        "Path(PATH+\"ds_img_50/container_images/\").mkdir(parents=True, exist_ok=True)\n",
        "save_path = PATH+\"ds_img_50/\"\n",
        "\n",
        "test_losses = []\n",
        "\n",
        "S_errors = []\n",
        "C_errors = []\n",
        "\n",
        "psnr_s,psnr_c = [], []\n",
        "# Show images\n",
        "for idx, test_batch in enumerate(test_loader):\n",
        "     # Saves images\n",
        "    data, _ = test_batch\n",
        "    data = data.to(device)\n",
        "\n",
        "    # Saves secret images and secret covers\n",
        "    test_secret = data[:len(data)//2]\n",
        "    test_cover = data[len(data)//2:]\n",
        "\n",
        "    # Creates variable from secret and cover images\n",
        "    test_secret = Variable(test_secret, volatile=True)\n",
        "    test_cover = Variable(test_cover, volatile=True)\n",
        "\n",
        "    # Compute output\n",
        "    test_hidden, test_output = net(test_secret, test_cover)\n",
        "    \n",
        "    # Calculate loss\n",
        "    test_loss, loss_cover, loss_secret = customized_loss(test_output, test_hidden, test_secret, test_cover, beta)\n",
        "    \n",
        "    diff_S, diff_C = np.abs(np.array(test_output.data.detach().cpu()) - np.array(test_secret.data.detach().cpu())), np.abs(np.array(test_hidden.data.detach().cpu()) - np.array(test_cover.data.detach().cpu()))\n",
        "    \n",
        "#     print (diff_S, diff_C)\n",
        "\n",
        "    # Print pixel-wise average errors in a 256 scale.\n",
        "    S_error, C_error = pixel_errors(test_secret, test_cover, test_output, test_hidden)\n",
        "    S_errors.append(S_error)\n",
        "    C_errors.append(C_error)\n",
        "\n",
        "    S_psnr, C_psnr = psnr(test_secret, test_cover, test_output, test_hidden)\n",
        "    psnr_s.append(S_psnr)\n",
        "    psnr_c.append(C_psnr)\n",
        "    \n",
        "    if idx in [1,2,3,4]:\n",
        "        print ('Total loss: {:.2f} \\nLoss on secret: {:.2f} \\nLoss on cover: {:.2f}'.format(test_loss.data.item(), loss_secret.data.item(), loss_cover.data.item()))\n",
        "\n",
        "        # Creates img tensor\n",
        "        imgs = [test_secret.data.detach().cpu(), test_output.data.detach().cpu(), test_cover.data.detach().cpu(), test_hidden.data.detach().cpu()]\n",
        "        imgs_tsor = torch.cat(imgs, 0)\n",
        "\n",
        "        # Prints Images\n",
        "        imshow(utils.make_grid(imgs_tsor), idx+1, learning_rate=learning_rate, beta=beta)\n",
        "\n",
        "    if idx < 500:\n",
        "        container_img = tensor_to_image(test_hidden.data.detach().cpu())\n",
        "        container_img = Image.fromarray(container_img)\n",
        "        container_img.save(save_path+\"container_images/\"+str(idx)+\".jpeg\")\n",
        "        cover_img = tensor_to_image(test_cover.data.detach().cpu())\n",
        "        cover_img = Image.fromarray(cover_img)\n",
        "        cover_img.save(save_path+\"cover_images/\"+str(idx)+\".jpeg\")\n",
        "            \n",
        "    test_losses.append(test_loss.data.item())\n",
        "        \n",
        "mean_test_loss = np.mean(test_losses)\n",
        "\n",
        "print ('Average loss on test set: {:.2f}'.format(mean_test_loss))"
      ],
      "metadata": {
        "id": "qmrXg67vOJ58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "outputId": "3ddc9cd9-2e31-4975-eed5-5eb6382e8e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss: 0.44 \n",
            "Loss on secret: 0.00 \n",
            "Loss on cover: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss: 0.40 \n",
            "Loss on secret: 0.00 \n",
            "Loss on cover: 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss: 1.22 \n",
            "Loss on secret: 0.00 \n",
            "Loss on cover: 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACECAYAAACJbXCEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9eaylW3rW93vX+qY9nrmqTlXdrtvzbbvbM7YSSLDihPBHkEOcWB6wbMWSpSiOk8gCHOLERNiSnaAEIkTAYMaYgIkhTDbIQTQOAixjPPTc7b5zjWfc4zettd78sb69z6ny7b63761bRbv3o/pU++z9DWvvs8+z3u95n/ddoqpssMEGG2zwxQfztAewwQYbbLDBm8OGwDfYYIMNvkixIfANNthggy9SbAh8gw022OCLFBsC32CDDTb4IsWGwDfYYIMNvkixIfANngpE5HtE5J89hev+JRH50Sd93Q02eDuwIfDfhhCRF0WkFJH5pe1PPe1xPW6IyK6IHD2NiaC7/jeJyCdFZCki/0REbn2efZ/t9ll2x/z7j7z+34rIPRGZishfEJH80mt/TEQ+IiJORP7oFzjGD4tI1X0HJiLyiyLyoTfxXj8oIv9IRI5FZFM88m8INgT+2xe/T1WHl7bvf9oDehvwE8AnHtfJRCT5AvbdB/4W8D8Au8C/Av7G5znk/wJ+FdgD/nvg/xaRg+5c/yHwQ8A3AbeAdwH/06VjfxP4Q8A/eKPjewTfr6rDbpwfBv7qmzhHC/wM8L1vcgwbvA3YEPiXGETk/xCRn73080+IyD+WiB0R+ftdVHvWPb55ad8Pi8iPisg/7yK6vycieyLy013k+Msi8uyl/VVEfkBEnu8it/9FRF7zOyciz4nIL4jIqYh8SkS+9XXex78NfBD4i2/hs/hGEXlVRP6wiNz7As/1nwAfU9W/qaoV8EeBrxSR517jOu8Dvgb4EVUtVfVngY8A39Lt8t3AT6nqx1T1DPhjwPesjlfVv6yqPw/MvvB3eQFV9cBfB77sTRz7KVX9KeBjb2UMGzxebAj8Sw8/CHyo06D/HWJE9d0aeyoYIondAt4BlMCj0su3Ad8F3ADeDfyL7phdYjT8I4/s//uBryMS2DcD//mjAxKRAfALwF8DrnTX+NMi8ppEIyK2G9f3A2/1dv5aN/ZbwPeJyDtE5PzzbN/RHfflwK+vTqKqC+Cz3fOP4suB51X1MgH/+qV9HzpX9/iqiOy9xff2EEQkA74T+JeXnvuO13m/73icY9jg8eIN3zJu8EWH/0dE3KWf/6Cq/jlVXYrIdwGriO6/UtVXAVT1BLgcnf8Y8E8eOe9fVNXPdq//PPBlqvr/dj//TWL0eBk/oaqnwKmI/Ang24E//8g+/xHwoqquIuBf7e4S/jMelhJW+AHgl1T1V96MnvsIAjEyrrufXwa238BxQ+DokecmwOhz7Dt5jX1vfI7XV49HwMkbGMvr4X8XkT8O9ICKePcAgKr+NeLEucEXITYE/tsX//GKWB+Fqv6SiDxPjHZ/ZvW8iPSB/w34vcBO9/RIRGx3+w1w/9Kpytf4efjI5V659Pgl4PprDOkW8A0icn7puYTX0GpF5DqRwL/2td7bm8BRJ4F8oZgD40eeG/PaMsfr7fvo66vHb0kyuYQfUNU/38lXvxP4uyLyu1X1Nx7T+Td4SthIKF+CEJH/EsiBO8Tk2Ao/CLwf+AZVHQP/7uqQt3C5Zy49fkd3zUfxCvBPVXX70jZU1f/iNfb9euAQ+HinW/9J4Os7B4d9E+N7SILpJJT559m+s9v1Y8BXXjpuQJSUXksj/hjwLhG5HJ1/5aV9HzpX9/h+d0f02KCqQVX/P2JS9Pd04/7O13m/Gwnl32BsCPxLDF1C7UeBP0DUsv+QiHxV9/KIGEWfi8guv1XPfjP4g11y9Bngv+a1nRp/H3ifiHyXiKTd9jtE5AOvse/PA88CX9Vt/yPR3fFVq7uELnn6jW9msKr68iPunUe3n+52/dvAB0XkW0Sk6MbxG6r6ydc456eBXwN+REQKEfn9wFdwIVf9FeB7ReTLRGQb+GHgL62O7z6Pgvj3mnTnsN1rz3bv99k38v5E5N8iJjE/1o3tp1/n/b7cHSfdGLLu50IuWR03eDrYEPhvX/y9RyKpvy3RJvd/EnXpX1fVzwB/BPir3R/jnyDqpMfERNc/fAzj+DvArxAJ7B8AP/XoDl1y7/cQk5d3gHtEi+BvIQhVrVX13moj6sVt95huopgRXR5vG1T1iOgi+THgDPiGbvx04/gzIvJnLh3ybcRk7hnw48B/2p0DVf2HwP9MzDe8TJSaLk+ef444sX470YJYEidfiHc4LwG3P89w/9Tqe0CUpX64c7V8IbjVXXd111ACn/oCz7HBY4ZsFnTY4O2CxIKP96rqbz7Ba/4B4MtV9b97Utd8mhCRHybq+H/2aY9lgyePDYFv8LbhaRD4Bht8KWEjoWywwQYbfJHiLRG4iPzermruN0Xkhx7XoDb47QFVlU30vcEGbx/etITSZcE/DfwHwKvALwPfrqoff3zD22CDDTbY4HPhrRTyfD3wm6r6PICI/HViqfTnJPB+r9CtrdcqVNtggw022OBz4d7942NVPXj0+bdC4Dd4uMruVaKV6iGIyPcB3wcwHg353u/6lkd32WCDDTbY4PPgx/74n33ptZ5/20vpVfUngZ8EOLx2oADDq+9iePheVEEDaPCgAUExAok1WBGiiSFgADEGEbm0XVzDISwDOAe1g8YprfO0IaBioDtWRVBi6Z0ARkCk+z+ONv5TRUQREYwYROJzqw1W+wsheMQYjAhCd0JVEMF05zYCffxDYxcRjKF73L2Rz1PvKJdeF+CVX/kwvq3RoPjgP/eBG7wurLWICIu0z0cP3oe/c0yy9GQeEhWcKPUwZXzjKj6zLOsKq9DcvocK3ZcJ2N9h6/CQtmnwTYtpPFc/fcYdLWmzgI4yuLILScqQFFs5tHV4DSyqJdQ1VC22VcZpH61aykRxucEPE7iyRbG/z7BWkkmJns2xxnLPLwnDFBn2kEGfHoYsCIvZjKZcgmvAGPKdXfqDAkksC+dxqaE4q2inM9q6ISkKDvb2ab2LWwh4A5VrsP0cm2VYawneE6oW6wKmcWjZEJY1mVhuLV9i3E5AwfvN9/KtwBhBzOdPU74VAr/Nw2XSN/n8xQRrDK+/n+tf982oKsGB+haCx2jAGqXIDJkRjFEMIZJ6YjHGdNsF6wlCCRwHQ1nBdAnzyrOsHc61eEkgSRBrwAhtUBSwImDAGhDTZXM1QFBUAyIBYy1JR86qxC9uCB2ZCgL44EmsxRobP3AEiORvDVgjJFbZC00k+e49WCsklkjiRrpzvnY+YkX4q/lBRLnzkX+Bb2uCBlzrXvO4Dd4YbDeTzrIB//Tm76C9/2nycsmwgUItpQks9rY5fNeHaHsJR5Mz8iDM7v4qNknACN4GzI13c+NDX065WNLMl9hZxQf/1Wf5lXBKueXR/ha89znIe1wJBcm0QucVjsDR5ASmE3ALstpzLewQZjMmPSj7GW5vhP3gOxm8550cnivFSyfoZ+5hRfgNPcc/s4U53MVe2WUnpAwa5ejOHeZnJ9CUSJ6x+873sLO/hWSWu8uKepSx+/KUxat3WU6n5Fs7PPf+91M3DYumYtk21EaZLs7JDvbIRwOyLCM0jurknKwOmEWNns0J5wsGJmV8e8a4naAozm++l28FCQn2dWwmb4XAfxl4r4i8k0jc3wZ8x+c/JMIHaIMiAjYBk2SgioQAweG8R4MjsYY0TSJRAqqCBiWoxsgHAKFVg6IIkBkoEgOSYr3FI3hrUBPbzuFCjLABq4IJYFSwoqC+uyWIJC9eUTWE1WQhkZCF1fwhDPoFxoBdRdtmFV0LtiNoI6ALiycGa0EV55TGryJyXUfhLoTuWrKOzq3tJjN7cdewweOHquKaFtnfpl40SM/g8pxJM6f/3E3muzmNetpSsE5gf5/tgyuoFc4WUwZ7B5zdPcL4gJY11fGU28tzGilRKSDvw3AMieF02UDaYkZC1hvCUGG3D7MSd15xNGkozyuwFgYj7MEBW4fPMFksOZ819MoFtBXn5+eEbUO6NcZub2HEgPMsl1Uk0CyDUZ90f5+9vV0SsSzmNcvpFJIRdnuI0SvoTh/f73G8ZXAU1K2hrIRyNoflnII9cgepdxQm5aWzGaQZAUdTKP33HzI3hnaaP74WXBu8Lt40gauqE5HvB/4RYIG/oKpvqNl71QROZ57EGLIkRqqGuAlCgsEoOKBFMYB1ikhYnyMIaAgoSuMtC5fhQoyITYA0RLGkRTAhRtBiDdvjhBAAjQGvIUbhVkAwkbxDYFG72OVIA6IrQjWsliOIUoqircep4lgRt+kIlwuJhHgHoQreB7wLeO9xbYv3Hq9KIKAKaZJAJ99gBGsM1l6++4h3IGFTf/XYkRjD7miLvBhzkmXUzlHlFvr7JF/3Hty4j5svsOI5tCOO7s9olhUtAZMmXN3Zpa4b6vMZy/unlB//DK+Kx6UBipysP2Q7GzBJPI1r0MQgaQ67u4yL6yQukDUBM68p7xzjjw5oXUuxM2br5jX2ez3Oz6Y8ODvDTCaYekpbT2D7BuODbexowHJZk2I4Oj6lrSpIE9LdXW5eu0pfhMnxhMnJGWF2BonhvhWSrZTBtWv0tnvUfZgvlXre4qsKtIHxkJ5J6Dmw3mOCwzhHVS/RFNgvWL5jFy0S3Cc37VGeJN6SBq6qPwf83Bd6XOuUReUx4kgMCAEriiUGHLmFpCNA46JkYYyupQuIEbKGqEn7EAg+Eq0RSIjRqxVIBbwYVEAloFUL3nd6dmRyI3FCEHStYmQmJwRPCBequdhOSw9RvFeFNO3hvSeoRvJXQbGoCj7eNhBUKZtVx9KVkC1xDCsRHoMCZZT9QRQNGuUb7zESHsoDhE0F7eNHUMKypgwB389QF2Lrpnddpxom5FYQhbRVkmXNs/tXWcwXtBbCqOB8MsWklrRfMD68SqrCVO6AOuxoRJ72GLRAnjEfjvCpwfRSitEA1wR8CPg8gcTShi3y/SEsFtgsR3oZ1bzCiMV5h/c1IfFwbZvi1iGm34vfy8ZhfIKfz1EDJitI8xxRYTlZMr1/yvzkBNoKUzaEdoEfDkhSA1VGW3ma2QJ/NoHJFGpHMtgiUUgEEOV8fk46LPBH52iSwfYQtRrDuM3d4RPFU+kH7lSpnccArSiCx0r8/ScJoIZgwegqMgaDrGWLTjEmdESORNJXIEiUPBQIRgimS14KqICYQPCRHAVBVElW8kcnw4AQckvwhhDCut+osbIm3hBMjOQRWgTvA0FDjOyDgsp6kgnB0/gQybdLTJiVPtLp60qXJEXiY+L78ZiOz6Uj9jjRbPj78SOEQDVfEDTgE8AmSM+S7e8gKsiiJpnW2FmLXbSMTU7jFG8Ek6Uc1wuyfETS65H1CkyegU+oyyVSZKQ2wU8WGJ9iU0W9haC4qqVpA+I8IU0xSUqbG/o7Q1w/xRqL5AVehEwsmqVxgvEFZneb3uE+pAnBeWzrqRc1WlXIsI/Nc5I0oa1bqsmCajLHzZbxj6tssd6RmBrRBe2kwjuHLSvCZEKYzUAh729jBdQorQbmoYZ+DqlgUkOS5WjtCK1H3CZx+STxVAg8hEAbAgbptGPDWtMI4DQSVoyko5tDRRDLmgQBTBfxWqDoCFqJhI0IwRhIYpJQBYyFPNcYGYc4MQhx4lhp2KsI3BdCiHI40HFt121aFYKC88pkAm0QnAPvuRQZR0IOEh0tRT/HGLuOohEhACGADwEXYgRmk3R9DSVOFki8nq4Gcvn/DR4bgg+UiwWgMO5hegXJuM+w6MPSE46n2OM5+dECd7LEDbeoqjmLgSW4jFornBTk/RFJmmP7GVeKIdPTM3zdoj6wPDmDeYrNEyQ1eA2U1RLXJVbaUR87GuAzi+nlJFaihJblaBCKWYMOhzRBCf2C5MZViitb1FVLaDxJq8yPTtC6xewk2F6BTRLqqqKZzmiXJbQeMgvLitwYkrZBT0vK+RLTehIErSqaqoQsoXfTYBJLkyhzHGUWSHLQUY8sy+mrQectTVliqvZp/xq/pPBUCNwHxbeeNLWkaUqWWDJrSAykiSEzYE2UNqy5SARGXVm4vCyuKiSqZN7HqFUi0amhY2YQo6zcOKEBq0Dw0baIkq4jcABFg1C3kUhXHkOJAj2K4jsCBzg8ABcszll8iAla1dW+lx4nsh4bxEmqbaBuo+1RXEC8dseZtXZ++Z50ldjcNCB7eyBGsL0CTQxJr2C0t8N4Z5twVmMWDUcffQF3NMXMWj7zyee59u5bcH2XNkmYtAswFf7uCxxbh93epZ8lXN0q0DCAqkbLBr8o2Q99tmRAz+QElPsN/Obzn4JrexRZn8E4gzSP34cshcxSZ8ISSE88tj+g7veojJIfbON6gls6EhcoVDi5cw8sJHlG2stRI9SuJXgf3VM2QY0lnE5g2eLqhraMBE8A0piHITVwbZfhzjYMchbacO4aGBe4ZUl6sE9KQlq2FI0yeTBDNgT+RPFUCLzIMorxEGOFzAppZ+dbEbaVqIEDhBBlDTXRhRJCJNrLFOa8R+tyrSevdGJ8F43bi6hYumShURelZxHUxDOqhk4bNxiTdLbAjrwduKC0zkXNXRWvgeXUE+ikEg1dQlIxxsRo25hoRywKOv0DJOqJxkBarKJtQ1A4OXN49SjdnQeXXDDduxCJCxtuaPzxQhND2O6hIWB7OUbBTxdM7h+j5wuqT72AnyxZSEJ2dZcT6whSo0Ufe2UXe7iNT4TgAn46Z/HghMXokKlWhDQQXEtbT5g+OMKczMlaxTjHcjaBW1fYHgzp9XpYhfJ8yuz4iOJgBzfMKAPgSm6UcFIvcMOc5OoOfmhZOhAP1iuJ69am3h6Rjcek/QInYFNLMRxgG09jE1oNNOWM5fMvgfNo0jkJihT6AkUGox4c7lBvpYTE05Q1WpWx4GK+YNQfk5iEti4JwdMmXeC0wRPDUyHw0VC4ctUQguAd+Aa8czjvcKHFdx7qlSVPO6+0WcsPF+dSwGgg75jRYDq3SNRDVKKrBI1WvTS1oIqRNOruIqTGYC8ReAjRLWKxGDTaw4MS2hZX17S+I3GU3qDfDbTT4LtRGWuxiUS/dxLHHLoI3XmlbQPeO3xni/RRPaIwCUGjvBJ18EB4SJ3fEPfbBbEGtgcwW5CN+niF8/mMpqlxswm+sKjpoWJwvkUPR5jDHUa3Dth511WmzjAYpMxOpiyqgPeGs5deojUBHfXRzKCjDN86/FGDV0WM4IcFg5vX2HrmKvRyqrpidnJCu1gw2B4h6QCXC41WJI2H+8e4podeGcecUatkPmBaj7Se/nhMurdLb2uA9DK8D2SFIR8LxoUYJfmWxlXo/hagkCXQS2FnSL6/TTYekg4LpFcgeQpNQ1I5iqWnmsxIxJBUHpsanAhTVxOGKSF5M6vabfBm8VQIPEtgUETyboHGgwngvaLqEe2SdCrYxER3iQomaEfMnZ5BlDSMEaxJUFbRt1nvYvCREDvvd0yBKlYkbsZgTYx2uzCfYGI0YwmIdm6RoLHQSKIH3ZgYHeddIQedTq9dNadYg02EJDEkiSAhErRXaFdyjVca5yGEzu0i5MkqCdsVD8nKc8M6eg88NIdt8JigIpAaTD9H8hTXOCrf4gl4G6JPG8AKgYB99zXyvR0GBzuMewPqe1NkvmRYeuwycHo8p1osYNxDUosd9bHDHnU2gfmSULaAgd6I0btuEMY9HEpdeZqqguUSq0qSWEIGzdwR5hVat2ib4NuaxAzwdYPxijaOcr6gNxwy2NvGDHLa1EBQGu/QcoELLZpa8n5O6WrIDzCpxfYyZFDgRz2SrSG2V2DSBLynms5hXqGLCjtfwtmcbDAkqzxWEjxCqz7+YZvNN/NJ4ukkMR24chU9x4lfrY1VOF4gxCyjNUKaWJSw9liLRC80xnY+7ahvp9bQdknEVYQqnfVQQvRVK4r3UfuOzsCVUC0kEv3VIgYjkIrCyoGiUSsvrCFLii5RGo8XIwS02wI+rNwkAa8G9UpwlkKVBCEVKAz4zFBJSmuh9YLzgaDgtY0Rt5guGWvih8Rqkojv7T4bPHaoQtuSjkedgwk0T/ApsDeIuldiIE+Q/W1615+hbzMyNbRHDfbVU+48/zLPXr9Br6w5+eSLcPMA2dkm298j39siG/Vx2w8IVuFsCl6RK/tsPXeL28cPcHULvo0zdBorwRIDGQFmC5bHU0J/AEWBVDVWiFWf3uLrhunpKVcPrzHeH1MWhkrjnd7k9AReegUkodja5sr+FSoTCIMd8lGPot/H5hmla6jrimoyQ5sWnVf48wW2bDG1g6pFypKcnF6tmAS8KomPxoMNfT9ZPBUCF40XFgGT0EkZMUGZkMfy9pUebjqNWOkKdRTnPK2vCV1VZbAWlSJKGKtGJ5e+SYGoT8enLHSES3de9eAJ2LVVkViRuRrvpbHb7tzakXevZ6ObhCiRtF5xzsWIx0V9PKiSecWamM031pKKJU/A27SzDl7IJsGs55VLFadc9N0Ant9ojY8dVoWtkJM2QggtjWsRhPRgD7M3xg0zQiqYTNi7vkN14jj95MtM708ZzwP5yZL09n3uv3IOqSXNR7RNTT8r2N8eY/oFdydTtm9do7qx37WMMLilJ6RgguKmc9xkCuUC9nbQLKVZOspFCy4hMZYizUiGI2RnD6lg3BvCbIZ3nmzQwwx7hERYli3TqmI5m8ELL8B0hly7TnJlm+GVEfs3RkzymNvRuqWdzPF3j8nmFamDzINtPG5ZM5/Hu4mmKsEK6cEB0nrq0wmTyTmhKrn+zE2KsKHwJ4mn40JpPdUiRg82EbA2Sg5WsMlFEQ8K6rrHK1IXsJKQGINmUSN3CqXrqiq7snPpHCtgCcF0Oni0LiJddSWs+ld15C9r7k86NXulOa8bUVlZ2wBVwLWdrGMEI0pqBEkMRmwXiUcUoV1PTKazFkaf98o5o50EBLGt14Wmvk7BdhPO5YKjDR4fkjawe2dOyBIaA0lqGPQyBlu7DK5v0fYMC2DaeE4+dY89GbDVZoSp0r5wn54zyHlJKTPCqMDcPODwd30QCVDfO0cfTBgPclyvIvQss+DJCBwMLNVnThgeLQjHM+bnZ9As4V03ybZyFKU+r7CzlnlVwiillyUURcLJeclIM6azBW25JOvlyLjPEqHxHvGB3CTUWQbXrtC/eY3B/i4LH2sVlneOSIKSeSGbLFl+8jbzF2+jkzm0DskydFCgyykht7A9hK1tTtolZ/fvoE2LLyvUJIRJFS2KGzwxPJ1CHtdSl2WMsK3grSUxgrOC70rrrUCaJuRZiu0qvFyI0axvHa3z+K5pVMAQNF37w+WhJCjECnmDBolEHmk87rtKDq5810AUsS9C3AvrdaevX2oh2LY+ThSd+0WJt5JGbNxFouaeWB+73q0H1Wnlnb4eh2HWRL4mb+0IflXs043p4tEGjw11iz7/gNJ4ylQIvTTq1m1Ampo2F2riHWBWN+wPC4raspi33Lt9RFsMccFjr26R39wnf+ch/YNdlrePYFoSpjOWp8f0f9dX43IlGQ9Is4zl2ZTw2TvIvQlmWZLhCTtj+jvbmCTFLStkXjEoA2VTkcgIMUKiwLIBIJRVtAkO+zDKWHoXK4UxGDW0422ynTH53haaCZPJnOnZlOLuGb5qWdQNzXRBef8Y2oDt9TBDi88SXCYMb72bdGeEbA8Jwx7L4GirCi0VJIEA59WUxm9shE8ST4XAVZXgHQQiqXpP6ORvb2VdVNPrZeR5l5zUaNXz3tO2LXXTdgRuQCxqDEFjFC9c9Dlh7d+IRLgSRi48HV1qc0WqdInO2J7wIctH3CfKHRcaevSNo6Cyov9Oe+9azMa+Juvw+1L1JesoPfrFtSsEuqjEXGUs1/s9MqYNHh9C46jvP2CuDVVh0FEPWw4RF2jP55BaQiKkRrCqmHlCOJrRPDilmswoJYFBTvGOaxTvvUFyc58gxOKZ0ynt3WMWL75IfusZ/EBIaoemOYvbJySvHBNunyDGMNgbYa9ep7c1xrWeerYkTBYMS2VmAjazSGLBg20Cvi0JrUONoL2UMLCU84pUYzLelQ1Jf0ixt4vpZTjvKMuS+tU77L80gWVN3TQsyxLvmmihHPaxoz5m2MNkwuAdN7CjPj4zBAOJa9GqxFcVYVlC07Iw4DYmlCeKp0Lg1hjSLIOV7tyJ0YFwUZLeNTaxaRe5CjgfaFt/0eubVTV61FoEAW9ADWqkK3W/MOStyirX8olEbWbtTukia5FoA2TVA7wb95pUu+ocQcmyFJBoM9TYmzv4WOS/kl2MCLVq/KMPum6HqyZ60Ndkv55pLkXndlU2f1Fqf3lS2uDxITjH9PiUBQ2Mc0hB25xmvoTJgn5e0B8MSfo5t+/d5cXmDuXLt/Hnc0Qs01Qpbl1j/L53YJ+9wnJgmb98Dz2bUb9yj+b+GfiUyUdfIByOKe+dUgYlKT39kxmLe8cMd7fZ29rm6vvex/nQ8MqdByzuHSEPpoxDDqOCdDzEZAWhhT6G+nwScz+9HB0V+AHUM08WDCxr5nePGF7ZI08zShdoqhZRhbMJs8/cRnxA8xTtWRgV+GGG3+1jD3bpP3PI7tV9xBhm8xnn5+fgWvpXD+jtbdNK1+enrWPE9HIfjp72b/JLB0+HwJOULO8BIISuB86F6hubW8XOgmfnE5IkwSQWibX0qM1IbR6J3UaxXHxLIgZrTVcPH7qmVT72916T9irahiism5hJpSPlbr+YPLyw871W2Ksq6wpPAPWCaDeezpNuOk+6GvAmOku8uXi3kallXXm5UhBXsgkrK2F3+dVQNkH444dmCe3NHegbOBjB1hDt5SznJcvn73C+qON3pZeBtnBlD9KW5PqI7JlbLCfnvOur30+1XXB+fI/Zx2/DcQsf+yxJY0ic4MqWK0mfo1/8l7gEGA1wxYBpb5vec8+w/Y4bbL/nFk3PcP/eKdXplHAerXsnk2P0dz6H7I9wieDnJWkbOD09Q/IU28/xw4ITBZcYtPKwrDGTJVs3biAI87KkLSv6JqFysP3cO2lbxyK0hEzhxu0Kb/QAACAASURBVD6jZ69THGyTDHvUGjhZLBAX0NZTZD2KwRaDYoRLDGkiJIM+TQ6181BsuhE+STwdGyHgWZFWJDwjqxVsbNe+SbEmJesyf0Jn19PV8UrrArhAKsow6XqLmy7i1oCqJ+AumkhB7EAoMbkJKyk7MuVlE1QbLo/4wkMeD1pNBtC4GCmDdCv5SGw+tIq+uzmiNsm6v7ci3Uo6URrpMpNAWPdSWZfLd3cmrLzxG7xtCJmh2R3BtW145gCzt0Xa75FXnt57niWd1UhQfGJoepb0mStMzs9o6wYfBF4IhFFB41qa2RKmFXzyJWzaQ4MneIfmhqOje/C+92FyiXZCo3D1CvnhNdjf5WyQc+/sLtXJCWHWwKJEqoY8gLuyQ5tb2qYhzEv685YwXyLJEJMl2EFBWVaoeuplSVa1bBcDrox2OBeDeA9tS3AB0pwTlugoJQy2YHeb7YN9il4PacEfzwllhalq6o9+DN96ZHsbc+0aOq1oUZIiYzAesT0sOJ0ssMvNIg5PEk9JA492wHUn1UuNmaJb26xdGi5Apyes9JLYs2KVB9TYzVCDQ/3Kd7eSTDym84lH1ULW4sOKkiMby0PkrNDJL6unLjzYwqpuR0AU7y4WWVh1S1yfhM6NqLFgCVnp6No1ytL1dVaH+nWjb72kd18mb13r5xs8Xth+wdbXfgB/OKDeHRD6Bd6mtLVn3B8wWHpoPQ0BN0wp++C29/FVjU4WZOYmLjNoGchsRn/vGtvXhcYr5WxBXVYEDYyuHZA9cwU3SKgzqFLF7W3h9reYpgmhbVm4BXgH0xmcT0nLhqv7N3h5PKBF4/e9aWJ5u/MYEWxmyXrCchk7qzXLJaasGScZhbX41kWt3HmCg15/SHV9F+3nmLyg6A/Z7vXp+Ri9t4uWxVmJzhdUi4A2DRpmtPmAgaRkRki8J20WcDTHPzhCZ+XT/jV+SeHp+MAJGHzkZCUqGciaGNeRqobo0CD6ro012FUhj72w+OHANQ4kYbWk2ao9q0WiJr52kthOIrlYQmKVxLxIQfJQtLtSvC9rz6tioNhRdrUkWnw1zjl6UTIplwi8+38VeF9cJooubs38F/5zVcvlPTfk/fbA9nLGH3oX5a7B50IjBucU11YwKjA2YBof+79nMJ2fEYY9EI+GhuxgmxpHMErSL+hvb3GjHTGZTDjLz5GmRnoZ42dvkBzuxH4mPYvkQm1izqcpK3QypzcpqWc14WwKywWSZgyevUqvlzOfzNCqIXGeZjEHC2aQkfRTUqPkVUCWjnZR47zDjMaUKZTB43yI63DWnl6vQN77DL6XYdQwMAmFQLoIqDpoA+myoTyeIppiQ8DMGtSeY0hJezmSeoJpqMqS5t59QlU/7V/jlxRel8BF5BngrwBXidzxk6r6J0VkF/gbwLPAi8C3qurZG7loYQLbSYPvFlYQfXjlGVhJHZ3LJD6DiO8cJgZxFwlFH5SGBJMUhMQSQsC7uDyaM9F6EpwDVdI06VbNsV3zrFWHv25CuNw/lks+7I5tozOkI3mhk2aU1Y1CPJaVQrOeOIy/oP+LxZUl+sDXk4aslnCOx1w6p7lE6CvJZoPHC5VAbWacLhxBMyTrkTSK/9Qn8P0bNKcNeaMUKlBOMUkguBpcjbYNy2Efd+2AMMoJuWHi5wx8y3JgOe+N8KN9tt93i5f6gXa7iOX1CIVT0k/fY+vOnPFZTXFe4U6nfOrkDlXWEm7s0LzvJh//4BY3VWhePiKtPFt5j1duvwzPHpC//4B0vIXca3n2gWLPG+4vlHI8YPr+Q14Z+pjgXwDLluZkTmsyPlCnuNawdB56lmkmTKoFvq0QqbCpYz45IzmdM3TQV0t59w4Pfv3T6OEW5p1XMe8+xF1JYJzATOI1NngieCMRuAN+UFX/tYiMgF8RkV8Avgf4x6r64yLyQ8APAX/4jVzUeU/VtBfJRPW4ELW5dck8YKwhScxDxwp0ycnYV1w1ECKT4lwbV4lf9SRZdThMDJBdrHkoinMXzbHsyv630p8RNMkfuuaam1frZT7yni5bFR96vlNzLok0a483RJ5fVXKuRJ3L51yTtrAp3nmb4SYLTn/ul/CTM6499wHy/gDbwpbb5ZUP/2smi4aqrFk0NeyPUV2ANnE1kb0t7M1Drh9cpfWO2WTK9MEDPnN/Br0cszVgMO4zGPSYlXP8x18hCJi8YKs3wswC7WTOi6/eZXHnPrqssLcOSW7sYq9ukVzdZWuwzeyjd9iTHmV1zqsvfxa2ttj7iq+ENEMmLcmsJkwW3H7pZXRrgEn7hLblWtrn6FOv4Cc1TErkeMrV97yHmQpBoNHA4sER5ekZuqyhbuK2bJAiw+30WSA0NsEkW2wNn2V0uE+xv4WM+tyenbC430AVXv+D3uCx4XUJXFXvAne7xzMR+QRwA/hm4Bu73f4y8GHeIIHHQhWLGIu1cmHXW9n8jMHYuBrOsmy6ToRdtPuQiyR29O7CclrXrO2AInQNovxadln30+7oMmjXrGolha+icCT2o+BC8og/yPqq8fO40LpXzpXPRbJGU9byDNH6GLqRWOla5F7SuVcXjUMz6/PKQ69u8DiRqnC1Talmhq2P38dUgTCrqB1w/4hGlZBYkszSnk1hIDAewO4AubZLvr+NZAnZvGVQBrQUZv2UZH+H4d4uw+0xtApHM/Rjn4WqpZWEY5NjGoevK7xrCDsDGCW4rYRr77zB+HCPpJ9y76X7bM8czbLCnU2RqmL8ZV/GoJ/jmkDSwCAkLM5nVGWJ3RlQ9DKKIqe5N6c/C7iTCp3VJEmfnf6I276BJEHUUZ1O0N98JXaXcyEmgpKEwbWr9HfHseFVlpDkWWwLEQK+aWnuHmHuH5MeNZh6Q+BPEl+QBi4izwJfDfwScLUjd4B7RInltY75PuD7AMajIXDRM8SirNoGRo9zR+AYUIsPSusVEyksbmuvdtwzXgOMVULoYlnpnCtG8X7lStG48s9ar74ojBG95DBZUbhelATrKn96mTZXUf6FGfDzitNmXVbEJY1d1/LNxWuXPd7xipfvQTbE/fYhtA3Vvdu0S8ds0iKLBj+tYsK5n9NKwKUWBjmDvTF2d4AcjNH9AVwZke5tEbzBOqVoFesMMw0k/T75YIC1KeXZHE4WyNESWdTgoK5aJBG0MFAksN2H3Svs3jog293CBwjnS/oLD7OS8vSUuq0wowGDK/s4VUwAqobl8ZT5dIa3guQJIU9AhPJoyrgVfK34FtJBjyRJaHwDLmA7myDLGqr43ZckIe316B/s0Lu2iw4yfGaQPEHrQHl0TjOZ0tw7RU5m7LRJ9J5v8MTwhglcRIbAzwL/japOL6JgUFUVee3YU1V/EvhJgMNrBwqxparzsYtZ6Kx33UVigi8YjCohCEEMEpfVIcbTciFLdMcZA9bqevkx0607uXKkKKyX3FkR9nq6UL0orBdBpfNwh/glDiuPtso68l6XwvMI+X4OdlVYTwgrwtb14ws9/bKEvkI3va1Db3norBs8TjjXcnx2B9IRs7pEygbaBp9lbL/7JmoUsYak12PnHc+Q7o2Rgy3CboHfsjgBd7REqppkUUdLXVuT2ITEJGjtmd8/w5wvkVqwTYKpWqrTGdq3kPWh6CFXtpDn3sneew5pTmpm987xJ1P2koLp6Tmz0xPcqEd2/QrpVp9JWzNQQztfMnnpFdrZEh1maD/DZ5aqbSlnM/ZdGgMjkyBpSk0gNG1sL+EUaxN8XqBtBcZiB336B3sU+1uEcUGbGxobMKZFvaOcTWmOTvB3jhmUnr3BDplsSjGfJN4QgYtISiTvn1bVv9U9fV9EDlX1rogcAg/e6EWNSTFJL66N6VdLpcWaTBScU3xwGCOkab/zccdjQwid9t1F3sYg6nG+vPBhW4vt1s9cMWZQCF653GUQidGu7drKrhtWddE+dJ1cgVUGU9YZT7lwLPIQp//Wzw+NWX1dVWdenPKSe/GhI7rhdRKK30goTwCyt0367/0+2rvHhPtncOcenLfw7pssv/Y5Dq4dstUf0MNSH89pnKN0jmZe0gZLwMOdE/T5u4SXjtC7Z3BtSLJoSfMW8QHOFqQ1ZJKTZilicirvYa8HV0bYZ6+R3LpGtjvmzv05V6RgvxjRUvPpX/xl0Br2tund2Gf8jutMQotLDKUPuPMTlp/5OOwfgrGYXoozsDw/xo6HnJ6dMios5H1OXcm0neOWHhCMGIajEe2zz1CenGGShGJrzNbNQ0LfMlksKU+XtOUC6hKaACcLOJtBVVEHS22inr7Bk8MbcaEI8FPAJ1T1f7300t8Fvhv48e7/v/NGL+oD1C4usuC970gt9vxWVdq2xrkVgWdYGzXxVcS9SnBmWYpZda0K7bqZlQ+KCwERyIztruNQDaTWrnuaXEgpnY3wUpMqvUz0DxGzElY02k0i694olxBWA+2Ol0vG8pWJZP2M/Nbw+4K8L2QaubRt8PiRqnDYZryswF4fnvlA131vhN8+4O7dI45eeIVeBYfpiMVkyszXNAloLyG3BnN/irx6jL13TjhfwPUr7IScUW0gCJoMmLdnNPWSSjVWdV49pHfrKuN3XMOMB2iWEJqAaSx3P/tZ2pfvw9E5WT6gec9VksNrFAcHFHsjanUsFlMWzsN2AV/3IaS1aFuuV6KiKPDiqK+NaRcN6jxt0qehpl/H3sWSQLY1RPo51TiFNEFGI/z2gMnpOcuzU/z5JPYwP59Fj3rXSlOGBcnuHly7Bh/pwcYK/sTwRiLw3wl8F/AREfm17rk/QiTunxGR7wVeAr71jV50sSxxxyfrxKVcCkmDhs5NEpeNT6yNEXVi14lII5BlGUWvIEtTkgQSEZIkJWigbePx1hjSfooVQZ3FeRc19c7pElvLdsS9cnp3ROo7dl3JIusEKzwUZVzIP58jAO8I/mJZTn2oTH99kfVpLj6LlbXx4b5amz4obxd0uqT5hV9Dcofe3CE/3CV75hqz6ZTkaEb7iVdp700Ic89dEupygRNHSAWKBK8gpws4XyLzGuMFJCUjgdrjmpbQOBZtTehlaJFgtscUN65ixz2Sfp+eTaHxnN87YfrZ56lnJXjF7GzhgpJ+zQco0oIizcmApHKwWKJpSu9gi+2DXZo7S8qzc1BLaAK+ZwmhhIMhLlsirSfbHtHUS7J5NAeEJKFWzzKFsDfC5hmhKFioo60b9HQOJzM4m8P5PDbqH/axW0OK3S0G168g+7vw/KaU/knijbhQ/hmfO+j7pjdz0bppqWfL6C7p2rOuCm+C+k4i6daoDHUs4OkiZyEGrFme0Ws8eZ5hTUD9kqLICT7QNNFjnmUZxuZkaYL3BudiXxNjov/bmthqU1eLF1+86+huWX8GnZL+ULKRTm+/tN9rfoCrScCzUtF1dXCn60fp6CIh2726jr4Duv456ulv5lPf4PUgywb7qZfJD/sMnrmOsX1crXCyILxyjv3sA/TBDF04fL9P5RaQ+LiSVJvGhlLLCrxDUoPtZZhBn7xfoF6pm4ZFWVLXJTooYNRD9sakBzv0+ylat1Tnc8J0TnnnhOrVI7AGGQ+R3S16owJ3ZY+8gaKFrPSE4xl5YglZRtrvkWY52SIhdYqzCU3jaXxsNBUSgWGOYMjGI5qJh7LFJgmSJSzV0fqA3RqS9jKstTSzGvGBNBi8M/gWtPbQSymGY4qDXfIrO4RRwbyd48KmlP5J4un0A/eBtnExuZN2q1Qq3cru0ZlijOCdo2qa6DyhXS86DJAklqKoybKMEBzl4pR+vx/X/2sbNCi9Xg8lo5/nhODxwWPEYFYtQVeLP0i09QmsWThZfzKrdS7X9LneSUW6hKmsE7APO1VWjhMF9Q+1o41nMFxMHb91Elmt02P0gsAf9r5s8DhhWk/v3oztYc4zw33mLuHVF+7Dqw9o//lHGfoBUgZc1TDY3WOZgBZAbtEixViLDmu0ia2SQ5phD3ZJdvtU05LFtOW8nBPaCva2kJ0xZnsEWcLOsM/xp+4wffEO9Z0HsKggMZitDIY97OEuBx+8xX315A7yhcecl9Qv3GbwvpuErE/AMpu17GcZ/cGYhsCsXjJbnMH1PUK5xBYDkqJPlmUkRQ+XKjbLILPU7RJcQ5bsUGQpiXqWjSMNYHt92oGjKh2hV2O2xmxfucLw2j6yPeBuNWFx+yWqxfxp/xq/pPCUCFwpGx8Te3SJSIm6s3MO7fqXrDRts16owawj9bb1lO0CWGBQrBq8dyRJgmiGamA6rTk/f6E7FhJrGAwGZFlGnqdYE/3Vzjmcu4gcxAhB2nVCdFVcZK3FJJYkSdbPTyYTRltjer0eadpp8oCxF5JM8Bob5651fEHVrK0o1oK1YCzUNeuJJPgAwWENZEmCjSsv4/1m1ZO3A62Fo5HhXR96PyZLsY1nMGuZfeJF+OWPcOUbfjdhPOTBfMrJliX7mq+gePYaYZDRhJaD4Yjp7QfUiwWurnGtw27v8OnZFIvgc4Onhd0RJLBzZZf+lX3OHpzTlsLyxfu0d87gvAbnoGe5/qEvR57ZZ7mVcpJBdntOv9v3wUc+TXX/HouXXsB89XOwM0arBmP6HOaj6DCxlsGVqyyBUDfQVCSNoT/O6Rcjzrdg7hxBHUma085b7LzGNGARtsk4KSsIjmTcYzQeULz/nfSHPVqU1oBt/n/23iS20jU/7/u9wzeekWSx5jvfnqS0ZMtuRUaA2PHOsZFVkAAJkiwMaBsgCBJ4l0WyyCrJKoKBLLKyAC8CbQLBQAAhmyhuKS2p1W6ph9v31q2JLJJn/IZ3zuI9ZFXDCdxt374FuPkARJGsQx6eOsX/eb/n/wyeR/M7fN5ZZFG/7afxlwpvZYBniMOJOg/ua0nd9a4vJcG1FDs31vz/GQREzjURBT4kfLjmiOVB932trs5tT9utoa4T1uaOSiEEMWbaJvPrEqEEqciqmBAPBqAkEIdmehnDofFHQlHTu8gQBgTjgQZKh4Lk64VoQvmBoixQukDKXFJhbcBd8/2HhWjdNGglD6rHCCnfV10UFFqhleSmlPkWXyjitGb3b3+E+s2P+eTsJXfbU37l8UNefvK/wd/4TV7OKspZzezrDzj+5sc81YZBJspSsVjMeHF+QXN3hg4teE90Dq2XWGOJ3hMp4N4CUg7FKpDM0EzbBc/+7C8YL9ZE51FtxfzkPne/9j77aUGUOXHz6gdP+Po44fn/9R2GF5eIAMV0Sfveh/T7MZcLHx3TjZZL4RiKyJAiNgaUETRWUidB7QMFI65SGNPjAK0KJrpkTIqmTxTGogClFEVV0e82mODRTUPx3gPGJAnDiPKBsOt4+r0f4D75lNj9TGkat/iC8HbSCLke1tc68OvP50KEa674Our1dR73G2qMNww9CQFCEUK4OZ3KQza4krnVnkN1mQ8RhCekw7DmtSVfHRygMsrMOx96Lm+kfyKhEsiQcrCRBCkKjMvRtT+Fa8fogV9vhM63cxbwSKVQqkQX+hALkB+7dfnfRYr8lkJAEgnOZ97+cKVyHTt7iy8QbYn65vt8Oq4Zx5Hd2SVS7uDOXZi3FI9PaY6PKBdz0IoHxZQXz84YPntBnNZMjuZoEsNuS0qJyXKZF4laEaPP0bGNBi+hakAWiDGSBsuw2xO8gwJoauTDY+pHx+zHnmQCcm1Ir3acffKU/uyKZD1lUeL6He1ygdMe6oKyLhn6jlUVCYUmoEgxUg1wJBWlBeEjlgEva6IP+dcqCFKUMHpQjiSzrt3qwBgdcVqToicUiqGW7M9XpG1HFSU1Cn+5JZn4hrTqFl8G3tIAz4P7TRnhtbXm+iRMulZiv2mzz29v5qVw+GxEEGKmZyDngqebBan8Kf48z/AclpVSHpJ5gKsDTZIIpAM/nnny62VlOJQ4yJiXoVpLfDg0WR4oFw5ySABibiBKSFzwxBBIKZc3V7VAqeJgLIoQsks1HsjwmBL4RCSQQkKSdewh+tff/xZfGGShqe+fcPkXP6FYOzZmx+CgOLlLaDX1wzvUJ8corTHrHW2UiB+/xG9XxEZz/zd+FW8jcbsnEhGqRJTTLGUVOc8eCWhJ2c5QqiAYR7/d5eVfAdRV5sYfHjPWimQgbDrCaCgvRlafPUPGhGqq/MIgJWpSI4uE1OKmVm2caESjc9m2TzQkZmNABotxDpscblHSqCoHvwUgeJL3h4RQQRQCKyIuOigPtStK4aVgGHroewQFtayyk1OWvNkle4tfPN4OhZLyqff6xHz9nOdTebwZ6Nfc9/UJOR1O61JmLvx6YRhTxAaf0w3fdEimgAhv3KkAVRYIKXPze4x5uel8lh2qiEoRmSQ++Jt0RCUPWSpSImK8+byUkpAiKcbDiTrz4yiZufw3wr6tdYT4Ou8FByMOQbh53CEEpk1exKaUDn+C4nC1Ej3BW6wdbgf4LwAKwTxqdp88x248dm9RUXD01Q/p78+oHpxQTKbEzrD9yXM2n57hzq+Qw4BsJMXdh9hZQcLhgsFf7li8O6FdzIhC4pMg+QCy4Gg5pxYFZj9ysbqAWkJVwp0l8tE9ysennF1tqKxneH5O/+QlR8WEy+Apj5akQmEIyKMTTK1JKuJTwDqLfngXN9OoRiMLiQiJxkO96bG2x44j+5RI9REPymN8f4iDtXn5r+oS3dS5nTB52CZwNiti6goVQUhF0iUiaQQKXVTEwiLs7QD/MvFWBniIAR8cPuTBKdJrrjila2OPOrwJIA/aa1yrQRLZRRZCxNl4cGCqG+45xJDdcSK/GCgh0bq4qUFLMRFDwKe8xElJkaJCCnkoUI6H7aLM3ZUH92V4Y7F5Pdh1ylcBHkiO/MuQrq8GJGG/pm1bmralrBqkLHA24FxuD9eyQBU1fhzRSqCVIJcRCfzoWK+vuLg44+WLpzx/8Yy/Ph8pbn9XvlDI0TP9yzPE52vS3oDSiOWS6u4Rx7/2MaGpGAfLsFpx8eQp8ifnyMFRNiWTxQmPjk/5id1AN4DpIEExeCaThBYaJQu2h6yQMgikiAQRKeYtQyPhuGX68JjFvQVlBRcvR/rzS8LnL+DJS+T8lPJb36CoK7RSzLRm/vgeP3nxhCgiajqhOTkBoZBVQaHyL7gA4mhZX1yyf7ViFyzj3QnVrKLdCIwLDINlHHqcNey9RcdcFD6mADHBaBF1i5Ytem+ZyIpUCaQL9NaSli0hjCQjcn7pLb4UvJ1KtZjVJuHQWSmEfG0vP9AkKV13UYpsn0/X3LYgh7DmU2lK2VrvY0BInZt6biiZiI8eKa5VHzJb97PeLw9wPBFHJGeNy+vsk2vJYcwSwhgO3PP1jykyz66rAq0LRFQkL3P+uDhw6fKQlJgCJ3dOMKOh3+3YrjYgFFXZUOgSISTRB6z3zKYT+v2GV6tLLl+dcX72nN32EjN0mKFj6Pd03Z6/+s2KQt5aer5IxP1A/4d/Qfr8HPHwLmkxRd07YvHNj+i1QFiHXW3ZPTuDiwvEnRmh7ymXcya/+j7i7gLz6QXh0ydIZyk//BBdF4x9Dymhewedo1aR7tkZviwp6pJ7D+5xOi+oTiSjVOw6x/mnF/irFenZU9pSsfy1X+FOPePqm6e0zYRplOgh4oyhLmrGq1ek3uBTQVWWTOwsOzpjxPcjm88usZ+/JOw7YqlANEglWO97zH7PfhwYQ85+GYYBkSJSZ+UVlKASjW6ZUmBXHXbX5UNVyvRi8+iU6tEJxf/zvRxtd4svBW+pUi0SDpQHpBwp+8/f6ob3jodi4vy1IhcfRG6Gd0oJqQAZiYd2+pt0QxEOauoISFxMKA6n6QOFEpPjRgZyUFrLw3/OJOPN94sxZsrkepkqBCl6KD1JqVw+IchmCl0fcscD1hiCrFhfXbJebdjtOlyItM2MqqyIMVMs4zBQaEVwI2bsGbod2+0l/W7F0O8Yuh3dfku/3xF/9SNuWiNu8YUgGsfwo2dZ818XqPtL5AcP2OjIsN6RVj32Yos/X0NKROVJDzPlod895dMXT+meviCselRIVK96/K9rzG6PMgG/7eDVhmo6I3ZbTKkJkwZVFlTzBu0Vbt+xP7vAffoEYm6eqk6OmD94TFU1pAcNQ9SwsVR9T2UT7yyOefb0OfvtFXHnaE7uoS9GbMzyWGstYd3hNhsIIS9QlQYfGVVi1GBUIkUBUlJITSEVGkmRNJKCYRwR0SIx1EnilSaWOnPuSmIrRTubI6ribT+Nv1R4OwM85gF+zXdf2+nfCDh8rUDh4Mg8SA25WUiSq60OlntVqEyqXDvBDnbFfEiNedGZIiHGTM0cZIyRmKmYm9uFw2LT54XMDb3Djdzw+sVBIAjBQaoINxSLyDJASoLPxbKbzRq3iVy8uuLqasV2s8O4QF21KF0QfMQay9APeGsotEDJvJr1tmPY7+n3G7r9lt1uy7DfHvj12wH+hcIF0vkG/XhOmNbI+0fId09Z2R7/7AX+8yvSekDsLYRIahXq/bvIR/cws4Jn3/0hPHmO3DtqCmavBvYTTb+xqNHAZos4u6QWDYGIUwZvLJGIDwazrdht1gzPX8CTp3B0QnW0pLh/h3R/zlCW0Aj6IeDMQLPZMkkVy9mUCxPZb3oYEnQShoDve6x1OJlISoK1UGooS0RZkEbPqAVGQ1Ap62yFoJKaRpaUCLSHYCJ+8GAjEUU7m2KKglBKUiEJSjB6SzutEfr2/+SXibd0Ak83g/B6UGe8aSc/DO8YiYcBzvXJ+vD18fBCIJSkLEqi84TguVlYXgddpfyikUjIpMkBsllbnkQ+m6eUNd4iHirSvM0yQ/m6SyfL916rWQBMbyHFfF8HlFpiJPRdz9XqiufPnnP14nOUUAe6SCGSYr/bEHzMKYk3DfWeGCA6j3cj3XbNZv0KbzqCt0giTVO9kR9ziy8KSgjmsiQt5mzvnSDuH8NiQn9xBX/6fXi6gzGRUCAM/Ju/yuRXPiKVkmcvX8Jo4WxNaxSnRcH9VvNnk5IgIRqTQ6BeXlAdP0BWso1HMAAAIABJREFUBXvv2K23bC/OwGyhbfNJwbicyX0kePDVr+Dags/GLYiScCUIPiLGEaxHjomr4QVWlOjlHaogOf+//5TTconZ7jDekaYVxb1jYqNh0iKmE4qyIg6GUQpCcmB6xL4Hlyh8pPJQpETsDcOrFUUEqSEOlvKoIKSISykrvZQk7vZE9YB0S+t9qXg7KhQRkcpnNUlKhOiIKUvwiqLIi8LgsMZgraFt2tyCEhw2elJIlGWJKhUyCYgJ34+HF4TXi8zgI4N3CCRSFQilcB5kUEipQcgsZUyBEBIpcGj9kSS5oFDqsIyMFEoynbSM/R47jigBTVOBshifKJNCAsFavEx8+4//iNXVFWY0KCWxg6EoCrQUKHI6oQ6eOgVEDBADBEelBVpINrs1Z2fP2e9WmKHj+GhJNSvxMbHedDdm/Ft8cTCF4CfvlNC/BB5zmmqO14K//P0/o7rQkGb40hOKAFMFHz/G+Eh6fgV/8Rz+8gnEkv1cIT4+pfhb36LrLqilwL24ID17Rb04wp9Omf/WV7CtRu338JefwXcCfO8KXKBcTjn663+Fq8czXgiQY6DwJdOVxP6jP4LTGdVHDym/8h7f+/xHUFTMjh6xHCJsehZ/99/BLWuicCQZoZQ4JOyHfOVQFvhZQVlripCIQqJCwR0WlM6hN5ZY1vQzzad08NEMPnlGHTVKHLF99pKmLvDGIVygmU4o1Zzps23m+W/xpeGtLjGVUnmAH7TgNzSJEK8Lj8Wh+iwKrssXouDGsHOd3x0Okjsp009RM6959IhMeewFD55rTv2gtxYCpTVKaYQQ9MNAlJKYwiEbXBGDQwooColIkRQdKTlKpXFuoNtuePXyjLapWa+u8M6QomO0jmlTomSmeQiBGB3BGWJwRO+IzhHdyPPNCmN6UgyQAil42qbG2hFju3zF8TaetF8CKCSzULFzFs43rPSPMMsl7//Gr8PzLWbbs3cjO2FhIvPBYTA5wMoY0Ap0iX7nDvK9B7hJyTup4eX3/ynh/CKbwu7Mee83PuKplvQpQlsz/fhdJA372edE42Axo/rqO0ynilJpwmqP+fycix89JQ1bmmoBMmH7niQUdD2+1ZSzlsnRnOmsYawkWiW8TAgtkAHs3hJixIv8YmVsT7SJSVVSLKZcbl6xmDbcfbTEzxv20R2yHSQIiY8w2JHOBaIXhLak0jWqKCiTRPqYFSu3+NLwVimUN99/3a6TB3O4oVjEzW0P4hFEghhyyYFUmXNL1xTENYQ4qFSyGkXGcDD8HFL+xCHONXv58yB3KRsZhEDiiT7eKE9cEAxxRAmBEjGvRX3EmQHTO4a+Y7dZc3V5ztg09H2fdeiAVgLvR6TSN+5ORCTicG7A9D1j32G6PSk6+m6LEFCVGogHN2kgBI/zjrqubkPBfwGQMVGPiZ33sNkTFlPGeaDzI3fevYfaj9h+nyWCrUQGKFyCIHKgdtMQZKQ8WaCWc/Z2JP34BeHVFpDoO1PmH73DWkU67xi9J0lojmbYxxHiCN4RZxOGuy1236F7h7zYoV+sST85x39lCccTXCGw2y24CD4gCo1Y1KS6pq8K9sMO5yNRgJCSAoUuCqpakkpBKRObUZLO98SkCErQx4H6+Bi7rLClxAzxEN5v89JTKZJWOCLRD5AEPjqsc0yiQFh/O8C/ZLy1LJTrk/f1+/Da3AN54ZjIEqXrAS4PumiREsEHhLqWFcqbYQ3XUa8/fQK/Vq5EEkLIzEcfbitSVqP4kJt7hICikvhgb6JbIxHjInVZoJQkhYAZB+zQs9us6XZbxr7DjgOF8NihO5zWNXVTM/Z7hNaHCNvMcTvXMw47+m5Ht93Sb7ccLedIEQ9KnQAi56PrQiNE1slLcZsJ/ouAuF7g1S1RF9A2pGnLq/UlR48ekSoNZUL0INuCWmrqFEmiwJQNobGEMFJNWlRTse97zHf+MhtjjpY0906Yf/iY512HrTQxeqQSMGvxd1rQJ0Ak1BW7hWI87+DKUJ1tUBcdTRfYvXOMvDsnConZbMC6LGmtS5hUDI3COEvX7fNpPiYEiqYoaeczykqjp5pCJXZKIuKe4BzBG5z0uLszNg3Y5OlTXmqy3SNkgSpriqoiUBKv9pA83hr6JGhETRrc6yD9W3wpeCsDPMeYiJ8+MfPTn7tpl5fgncuncakOOm3xWj54UH/46w7LeLCc8+aJPGvGxSH+NcVAPCQhppSIPhyUMeEmhzy4vAydtC1VWaCkxgwjGoMfLPvdlsvLC0opWF9e4KxBiMR0UtGUEF0keo9OHp0ETSVwbmC/7+n3e7rtFm9GRIq5rScF2laRkmEyrTFmoN/vKArNfr9lebzMP3twbNfj66uSW3xhkE3F5Jsfcv/f+ID+/oLLMnGVDGI55Wm3Zjy/IrqIrlsW0yWPl/fwl1usdph2ynZvMNEiokC6fNXXtlNOPnyH4nRJmtf0OlKpGrfbMZ1MULrk1ZNzph8+onrwCKcyHRgsYEeGqw1+taEeDSd3T9l//Bgxm1F0ljpKhs2Yc8WVxgjYmYG0XtMujkjbAbEfUTZQpUQZEsmCGRJDSvjLLdMAdrvDDns4apDvLHi27QjGI41HyJK02lMcHdO2Le1yzmxS8SL2pBBJfY83e8ZyjnJkmeItvjT8PKXGCvgj4FlK6e8JIT4Afhc4Af4Y+E9SSvbn/QHepE/gp2mQNwUq2VoeSSnTHtd/d51UGA6yPnFQrtwEXl2HYB105AJFCh7nPSEeMlAApTRlpVFS5wyU5BmNRXqP92BCYOg7ykKRQsANHWlcM4bApE6kInPkwe3pR09bVSQVsWbg8uyMslQ45zDjyDj02GGHMxYtQAmQB57+6uqKo+UCJSJaC8pC0E5qnBvRStHWJZvN5uf9Z77Fz4Awq9n+la8y/5Wv45NFOUM7DGxeXtLKGrcbkTbQpopjJ7lLzU9ePeHi6XPGzYZxtwOd2F1eoVRCzVuO/tpXmN87pbeWwRjiEFm9usA/+Zzm3XepHj+iNYH9Dz7l6KsPqcoKbxKqc1BNGcoOpyOhDlTv3AWlMNuOuZMcFVN+fPYZOgn8akeMjiQDIOn/4A/hbA1jQDYTyq9+hC5ymQN9oPSe2c5hVjv8fpfDfd5/TKoFrBxpsITOwPkVCI0SirIomExamuOazeYurhsIZkfYbdh5x2ADZjRv+2n8pcLPcwL/z4HvA/PDx/898D+klH5XCPE7wN8H/uef5Rtl2jnd6LivLen5VPnaJCMkWWVy+KJ8is5/r6R6/flraZ+Am8KFlEsalLy+v5xZklJApICWCX3Tw5BI0RGMwccIROrS4fqOwTq8c3hncdZQ6PwNnTG4YY8WktGOODtm/pxE33U0dZ0tzCHgrWGz2lLoAq0Uk0rTqjnWjHhjsMZgzIgdR+qyJHiHUoKmrvDeUVcl+/2eICXT6ZTZdHorI/wFICi4XEi6/Tm+66l1ResFmz/+Z+j3v0HR5fb2WnrEdmSY7th1O/bREiclzI8Rk5JQaYI3iK3jVb+mVxF7tSHsB+qyxf/Jn5JIjJMZaTrNS53n5xTVFK1LwuhIvaVbbQirNdHsoQqMj+fMdYNbb/BDwI0RNjvEyQLlY04S9IYKhdl52FgYPclK/LpjV1wQLnPSpgwJbTzD5pJYKuTJnLaZ0p93FFZAb/FnK3jyAn1yymQ+o25bhJQwwEksCN4xWMm2D4TNijiOmRu/xZeGn7WV/jHwd4H/DvgvDkXHfxv4jw43+V+B/4afcYDDT2vBX3PV3Dgu8yIxJ7jJN3K182B+M43wtaDuunKNN07xUkqI+YR+vVQUWfl9eDHIgz1Yi7Xm0KfpcMoyjiN2tATvMh8dI6PPg9yOI84aFtMpu/UKO44IEk1dMWxXuF7nF6bDz+qGHlXVUBQIlX8mEX12cqZ8xaCkoCrzL0aKWVbpnUMFSV1m7be3jkIXtxz4LwCRhI0eu92idiPSddB5ir1n+PQ5YbVHFyWyaMBH9kNPbEr03SVUGjGtULMG5y3OjIRxIGw3hCdPCas97EZSFKTBUdw7oVzMUJXGb7ZUTtBejOho8IPBGYe7vCBuN+AtqS6Js4ojUbD3CT+M7DsDMiFLhZYyD+XBUViL6wPRRLCBJCxmvSLZPSFlpYiMkipIYuyQdxZURU2D5upsRYsiXu7zAO9Gqkc19XSKrrI5rdttKNYDajvgNyOqt/h+ICXHT5WB3+IXjp/1BP4/Av8VMDt8fAKs03WlDjwFHv1/faEQ4reB3waYz6bAwaIjDn++MWzfHOQ3fHjkkMeQnZCJlMOjDnK/a0NPLmMQWd5Hrh6TIgdC5U1lJBFzVsphcGYuPKcR2nHAjCPWWbwbGfyA97nQNR4a7gsl6bYbtus149CTQqC4d8r26gJrDKXW1GqB6fdYDlkpWlHXTa5Fi4HgIsHkE/w4Zi5bIii1pik0hdbsTQ7C0iqSQqQfDB988AExBp49e45sJD+9PbjFFwEREnIzEm2PNgnzakN/vqEpF2x++GlWfCwXuPmMoASrsUOfLplNTkmzGrFoKGYtg+npNxvMy3PUtsd98hSGAJ3NZcNf/5DJN77C/MPHyLpgdfGKWT2jfNUheksaRjwRe34OyeQXh6ZGKMnEg0ew847dsIOjFmYVqtJoINhItTEYExBIktIgYdxvYGPAHdrkKYhJE5cVldA0qqK1iYuzNUIUiGeX8PIKVVTUdU01aUArhr5n+PyM9mKAdUfoB6Q12ccwb6HXcCsF/9LwLxzgQoi/B5ynlP5YCPG3ft47SCn9Q+AfAjy4f3oQiIhDTje58OC6/ICfOjznj2NEHAoqrxUlIiX0zfIyIg+Rq5IEIiFSQIiEFAolEkLGbJaRkboqGbqB/XaVVSPGkA61ZUopai1RZUUrajabDS8uLtmsV5hxoK6qTHs4m+vZ2pbNqwtKIajrGq0VKgaCscynEyCnEl6u15nZaeocNws4Y3EmrwySEAipQGtSFBS6JIQcpnV67xFnZ2f8zb/5t+n7nt/7vd/D2sjtBP/ioUbP0fdXXH73e5iyhqJCCEln16ASTBpMDW7cEPQ9Nvstd+6/T3G6YGwkptGouUSXU8qlxOG4e5Y4tyuijQihicsFhJGjrzxGHC/wMfDwww9Z/+H3+NH3PiNebfPV2Lv3gAFOjxDLBXo+oxkj+/NLUALZaGJQ8MG7mLZGq0TloZUFk0ogFwvibA6lIrQlWxWI68vctWk8OPC9QUzvUFUtbZTUJtJ2PlM0n59R7DpOv/UbpKbGiYSxA/vVFd2PP6W/MKSrfd4wTSs4mSO/8T7ih5/CxfnbfSJ/ifCznMD/LeDfE0L8u0BN5sD/J2AphNCHU/hj4NnPeqd103C0PMLafLotyxKtssQuxtetOsF7nDPZsRkiISVC8FhAS0U7mTBtJyAENniUFjfZI9vNhn63RcpIoRRCRIgOZ0faqkSSuWk/Dgx9R3CWELMVPziP2+yxzt1kfU+rmqLQlAlsSjhr6cYNdVNBSKQYcNbje8Osaok2P47oAxqFlIJCZOdojAlColQFZVkCuZez7wdmkyn7fYfWmmY+Z+gtJMnv/qN/TF3XHB+d4g9a9Vt8wdiNxD/5Z9ybTLjadrjKkRYtzCv46B24e0Rzcszy+JiT4wXD93/M1Z9+hyRBfPgek699QLfdUrV1bmRSDQ9FzfHHX+XV6pL1boMLhkd/529xPuzQZwPaJvxnr9j96CmlC4SmIdQF6u6SyQdfR7QVdVXT6BJtA6+6FXFWY+/M4N4Ejqc5LdMmtJfMdUPXn7FyeyZ3T5ndu8Pk5Ih35y3riyu2lytcP1AKxfriirqomeqKiSoI3UiBZDeM6PmC+vgOzuVqtbMXz3HOkHYd7NaMmyFHBwiBqKF590PkvWPUk/JtP4u/VPgXDvCU0j8A/gHA4QT+X6aU/mMhxD8G/n2yEuU/A37vZ71TM45sNpubAV4URW7DkeJmsfm6jSeiRF4chnDNsYlD+HxkOCQAJgL90LHf7ui6PePQ451By0ShchWZIBKDY3QD0VvM0NP3e4ZunzW5r0XfKETWaycQIaccxpSQQFs1yLpBAPv9LtuTYyQdVC3EdGMQUgiU0vjgSTYSZV7GBp+pnygjUkmU1FRlzW7fMV8sEQjG0WDGHTEGqlLy0Ucf8a1vfYvf+Z3fuZUR/gIghWCiK7xSUAjk8Qz5/gPigwXtNz7E1AU0Nb6ZsIuC+dERm+kiG2o+O2MYAuWsZXa8RGrFuDNMl3Pae8esJgK/FSSd2KtEkSThas/waoP7yUvSvkdNZtSLCep4jnp4h+ade3glqCvNpCgJvactE+tksCIgKsXswZI6gX/ZEQZDP3i65AnzGjcr6UuBtwPTTUQFmDYTUjuhmDZwd4F6vkXExNAPjARoCsrFlDpJmqLES+iHAa88UQioSzhZQDlB2ZSLTGYt84enhPkUeRtm9aXiX0UH/l8DvyuE+G+B7wD/y8/6hd57hiFzzNe2eiklEpHDqA4OzLwAzEUJ8TAgISGFxAPeOuJ1JK2MrFeX7La7fFKImUbJFs3cEk+KmG6HE4FgDWboGQ4529nxeODREchYImLWaCeyXNEfbPXy0E6vpCS67D7LA/zQjZle93ZepywGl2vR8jdLHPanBJ8FkFJKiqJku90xny9IMR5afAKPHj9m7PqDuzRSVbdhVr8ICKXQx3N66Ul1g7q/pHj/Pv7BEfr+CdZZwqHkIBlHXVY0i2OESYRdh9u/RM9nyG2grEukc9i6xdYCW0Zik0073dizFBVsR/zlDkaLnNao+0v0yRK9nODbkkAkRAiywNeKAWCQRH+oNFGSWmsWSWKkxUWDHQbG3ZrUFoRa50q0bo/yPa0sKJQkVQVhUkELk03AOUc/9oS2xBaC9v4JZQQVBIUQrPsNsVaZiy8LeHBKcVdRxpwXrqqScjGlf8vP3y8jfq4BnlL6A+APDu9/Avzmv+wdCylvqsmu1SNvNthcq0zEIQHQO0eMAZFymzyH5aV3jhQDWjjGbkcYe7QUaC1RClJIFBJKrfDOszp/iVaQUq4nC9ZCdIgUSDHnrBATdhgPyhCFVAohINqI9RZDXlBKIXDWITkM7PzIbnQxN2YkIdCHxygOKppCaUTyhBiJ3qGUzvy4gK7ruO4Nnc3n/I3f+i2+/e1v890//y4//OEP+MY3vnHDpd/ii0MqFf7Rkr3rCGVL9e4p1cM7iGmF7Xr8dktMglC1pCSpvGI2WdAsYBhh//IC+2pPf7aD6YRqWvPiyLIygX59BmGEo4ZoDMolxM6Ah+rkmGGiEQ/u4JuKkUS33VA97aAoqE8WjFqyxVF0W5ICWUoIAnFp0ZSUVmACrPsB++IZ8uP3kKWCGAjbPX5rqSZLZFthJKw7z6pw3Gsb7MrSjYbp6YLVbuTuu4+Ig8NtO46bGe4H51BUaF2jpg0cL5iWDbVQqIMvIwkYdpuDAe4WXxbeTphVSjmXG95YXubW+GvkYuPrTG+RUwqDuOmZTDEe0gotdtjjuitIeVgrIRAxEYzFjkM+hRQK7w3Ddo0UCa3Ize8pUgpyi8714JUS2dZoITO9c0g3tGZkv7eYYby5eii1RCiVB73MUbHEkBUzKd5ktIiUs1OEzKdtrfJtg7OEkHPMAwWn9+9weXlJjJG6rhEF/P4/+d/RWrM4ntPWDS7am1KJW3xxSJXGvnsHuyuo757QnCwplMK8XNP/6NPM+bYTwvEx+7rEVRXHzZR6MafwEb/eM2y2dPs9KEm1mLLrtuwunxFVRJwsKR884huL+1z9+Y/YGYOtFerxHcz9FnM0zafx9UCrCqZbRzWrSA5G6xhKz2nRcP7iOWHoUYWmfAzRwsXzl2wvrzDdDkjcu/+YqqoxVxsuPj3j7PufEY7uUN5Z4k4mrE5KOJ3Q957BjgymY/hkR/H1j/js5bO8n9EFm/PnpNFwfOeEZn6EWrS4tsApsAgKBHUU6MGz7A3Fbanxl4q3mkYohEBdywVvNN75NilGYggEZynLkkJpkjgYY5yjHwa6fcc4DPhxjzJryrIiycToHM4ZnDUk70jRZ5da8kzbCm+zhtsHm3XYIqta0AqtJFVRcnJ8yjiMGGOwbiB4h3OOEAxSRQopAZn5b3w2E8XDGfwNfTvkk3ihJD5mHt0HEEnelEkkkeDQHbHvLbKIVEVBVWk220uaqkYkT9f3JAx//7f/Uy4++zNiuNVrfZGIMTFYB5cXiKMFbjMQLwemQ2T43gtSEND2sOxhorAfPKSvK4wAkxx9LakXdymmLXoxwc0ntJ/tWG9GqCXFQnCUSgqXiCHiK0ma1FQfP2Q3T7mf42pLfHrGsEqYH58zvXtCvLdg2E/h/ow75ZTOSdx6JA6Gi7Mev+kI6zXJWagKuH+HyRhRfY873xLP13CxYi8LmqaAViF6aPyMKAUxeBh6GDzuYgXOYssSryMqXzKy/uEnbJ7XyNNj6oen2FqRpEAnKE0kXWyxFxvc/tbI82XiLWWh5FOoUjnbJMX4Bm98nTiYQ6euJYTOWpw7nLiv3YtjNt7IlKgLhUgB14+M44B1WZtaaIn3Djv2hGBp2ooUHCIeTD1KZu5bphvKJsVcpmBsxzAMmfv0Pr+gHK4cpMy5KtdRtSHmcmLStf48K0+u81cWTZPTBA9v1vtDxkvIKgaRB3iIDkRgtlhwcnLCdn+F0hFkwgdLbzyr9dnrxvtbfGEQMVHuLZyvcCc7gk2UKJZqRqoWub29CwxuC7QkYzHGUEiNrira+ZSoBHZe4RYVppXML7fQOxAF0iZqH+k2W1RZIMsCpEClxFRWOGvxe0dYdaTLQDi/ZNCa1GrCMscRx95Q+ETpBXbwmPOXxMOJX9QVxWzGZLogXe0xg8FcrknWQ1sRpzVxUiEKhXABfbDrQ0RYj55OcJ+94OiDd7FCYKxjsVzy6mJDXG9guyMaT5Aa3yiiSISYcKOHlyvCak+8tdJ/qXirA/zGOSmu2WMOYVOHJL5DWFUIuZpsGDrMOOKsJYaDiiPkLJNC5CE/dnvGsc8JflIgVAXBE70lOIOXieAtgtyio7Wm0DKbekLIckAX2XcbhmHAGIP3/rCgTG/Uv2WePqkc5xkzV5Ifm9YUWt04MYUUzGYNAM57RmvY9R3WB9KhwjsdclsmswpjDEqDkAFdgCrBO0MUjkjkT/7sj/jag7todXu5+kVCxcR077naGeJgoKoRWlPXNXfv3qO/3LLpdgx2yMvokPDGUhSCVpeEsmbtR7xMxCIhVGC6G8AmZKsoZEEtFdZYhFYIH4hDj3tyhlq1ROsITzdw0cE2HzC8iCQJSIkIgXHXE/Y9wlhEiIgQQAnipEZMJ6j5jOVkhuwMw2qDWW9zPeC8Ji1r/KxE1hqRAuXocdrjY0REKEQJF2vm75V0IhK8pSwK0LnVit5A3OJlSVSBRCQAwUV4tYEx59ff4svDW6dQxDXVkF43wnvvcaPJcaxaYYcsOxz6DmcNMUS01gebfCJ6x2g7hm6fM0NCLimWWmPHSPQmm3tIjH1PSo6qUBSlpqlqylLnwoToCT7gQ8DYyxupntICibqJu5UioVRCyoSLAURAqrx8LXVB2zYURfm6VCJGdCGpqgqEwFhDEI7eHLhxke8nIPjg469xdvaSi4tXPHn2CYvZDF0kdrsdWmsm0wn/5P/4fT74D/5DdNO8jafvX1uoAMut54koKXRJMZ1QthOMUJx++A6yPKdfKRhljkQoSqSJ1M6zpGIcE5e7HbGRMC8QOlBKDUiKdkq7PGIym2Iry/pqg3t5gXn+CvvdzxC9Q5YFyXmEdySp0O8/JN1bwt0FTGv86ortxrJ9+QLTD8imYfHxe7hSMnqLjwEPTJuaYnB0w4jrOxIO5hq3VPiFQlUKNSaaCC/Xl3hvUaqg2BhaPUO92lM2ilIJLi8vs9nu6BjWHewH/PYZYrcGPBQaioo0OkQzA3mdSXSLLwNvXcqQ4LWmOeUB7qxlHIds2+07kr8290SUEJRlHo7DkG8jxz1Tv8JaCyTqqkRnCQrW5oyStm5Qqs0FDBZS8NgxN+ZYq/DWYZ25KVt2MRwWjodOzJRwzuG9p9Casigpy5IYwyH3u6Cua6bTKYv5HK01IYSbBMKz5884Pr1DO5lQlpq6LpHF68VpPMQrfvLJD1lvViglOblzRFEoXr58wdFiASlxeXnO17/+FdStCuULhx8MFz/4hNQ4fIxEDaP2XD79hGF6jy529Dohpi3JG+4en8LekF5tuHjxhPH8kuLOlMVHM+J0wT71PHjvHc6+/RnmKnJWBs67S+b3H7D7zp8Qn62oNp4HsWE5SmIy2FKyqxTP1Mi9r32EeTiHe0uYVrz6yQ94/smPYeipphOm9+8zuXcCyxavJd12y+bzF/z5n3+XryzusV9tGIcBjhr4+C68exfZtmAD9tklu4srovZINNP5kq8VS8yrNS01F4NhGwfiMmeAP3zvmPrEo7cDS93QO4NREBWElFifX9Cqknb3Y/i5M0lv8S+LtzIFtOop9Tljb3A2IaIiBYl3MHSOZEHLgiJoMDX9fs98UqFlJAaDG3YoGcB2JNMR7BYXV/jgsgIkgBcpq00ElKWmLIrckxk7mkLjRMI6hzEJqSp01eJFg4kObwxz7Ygu3gzwlBLJJCpdUEqdHZWDQ0XHu48f07Yl4mA3TvaSrjM3YVkyRo7eeQfrPEM/EFLAExFVQVkKpBJEkdhs1/i9oSgyjdTtNpRlQV2VWDOipGJStzx78pT4638NiuJtPH3/2sIpwfmygl95n/BXPyQ8OgYJDBtefP6M4tWWJkqOFzO4f4eqVrza7BjiFipLLBx33rlDdWfBqAW8uER++zOWLy22FzAqRC/Yn/8YUdeUX3kHKSTPrecsRNJ+TdKSOG1g+YD619/j7qQm9I7tX57x6v/8c1i9gqakvH+Xxa+9g55OuHr6ArUeuGvgG+KY8+2GF59+n2FSwOmU+OyTAAAgAElEQVQM3jtl+tX38T7QbCLLAe7FO6x/+EP8cY2tIS4U+3fuYN495rPtmmFvcfuR9GTNbHGMOFaod5fEpuBPtq+I8xlJRUSh0bpAjfe52o/Yf9rCi7f9TP7y4K0McGcN/bhh6A0pKErVkEufcqLaMBhsGFFCoYRAxoAMEpECBEu0PUgPrgffQxwRIqA1mYohZokgCSUFhRZUZVZrdynA4USdufZETAGhIzFKUspv0eVTf7rWpItcNqyEzJVuPlecLY+WSAHBOYRMaAnODsTgqesCpTUmWUbnKApN8JGxH7m4PCMpmExaQvIYa5BSUFUVw77PuSx1zXa9RWlJROTGbxnouv6fK8O4xb86kpa40ylHX/sAcf8uLKaEYWBzsSJ0iTSOqKKkajXVg2PioiZsJF4HwIPIV1x6NKBUDr/qDSpCGC1+2yFqjW8LiqM5zWyKriqsd3TDQBgEKIWaTTl5/AiRwF91DC9X7D55Bmd7qAT10ZLJcklZFFhr6ddbxPmWuPdEI+h3e2wKxLJGTGr00QzRNsTtFjP27HYW0YEpBbGuEG0FTUmvYRTQBYsbR+gG6AZEMSU1ltA4UqlIWjImn6kVAi5CUypcKTJff4svDW9lgFvj2fcDxlhEKqDQOa8kCZQUpJhTAJVU1FWBEA7vPEE4gh8wdo8UnhgGvBtIwYLO9WVaFTcFCSmE3GCMuJH4hZBABGI4SBfJOSbBWRIqG4UEB+NQBNShQUgiVaZ4cgJibu+ZzeaEGHCDQcpEXRdY69FaMJsvqKqSs7Mztqs1xyfHICLOj2w3K+rZBGgyt+49UirqqmG/6lBCUlUNdlxR1TK3EemSsqoYhsOL0C2+UMiqoH3wgLvvPcYVDWHvsZd79mc7dGzxEuKsRtxfEu4vcLOSMCmh0tmhJSV+MMjeZROZFfTJY0VkjBHrBbgRdImoNLIpKdqWQgrMThObAqSknM04ufsQ+2rNcL6l+/yM3ZOXqCER5jXz03tMpgvoHUO/x6y2hPWWbtVxsR2oVElatMhZg561NPM5SSlSoRhUoI89F2NP9aAlTBtUU5Pams4ajLdZSXJd1mw9yTjSaPGbjmQMExUZt3tSKbNyKkR8VZF8IPnbRp4vE2+n1DgIglGEUTIMPdswIFFoqdGypFBQtEWuG0sjgoFhGAhhxNkeM+4Ag5YR5wwkjyJR11Pmswn/b3vnGiNZetb333s517p3V89079x29mKzaxvbCywWMQYEMcaJMJECQhEJSUB8SRD5kA9O+MJHEimRiBRFIgkkRCgkykVBETEJxiEXGWMbe+21d2dndnbu09eqrsu5v5d8OLXrxfHaiz3bw+zWTypV1enqOufpt+rpc973ef7/KAyRHqoiZz6dMj9errTHoU3YYJ3AveLh47BNBWJl2aYcwjrCQH1lsRQwxmO9XcnEapSW1HXN5uaAosip6oK8rKkaSxDEnD1zgW63w9Wr18iLBjn31KbVHT9z/hxVU5EVGUmaMB5vMZlMmM+XqCigcY6j6Zy016Ob9rCNZXv7IR5/7G28ePnyKxK7a+4dSb/Hk3/+e1GB5vrVlzi+dpN694ie3kCUNfXGkPETFzn73e/iRZ9x3JQ0aQT9PmLeIOeGjbjDTjogE57doyXPz/epmxwfaggD6EbQCynrBeW8QZmSXn9IFEe4VbVJqEJUXnN4+Tr5ldu43SlqVtBP+kyVZHO0QRgkHB/MmB7sY+d5KxPrGqAifdujHBcF0XhAb2eb0altlqEg3EpZJB3K5BC4TXV+G2YNMoxxYcgsX0Jdg3UIT2v6HQY4JVb/nEqaeUMcafy1a23/gqnB1BT9PqLTwWflfR7Ftxb3JYFni5LDXUschIQyxq3Oltsk2uCVRDqDwIJvQOTU5QRrK7yrUbIEDJ1uDD5CyZg0jHDOkOc5y9kcUzWURUGZ50gEWiqUWjW9e48S8hVJWuugcQbvDVIopAYMpGmE1hrvPXXdYExN0zTEcUyaJPT7fZqm5vbdXQb9Lv1+H/CUByX7h0c8/8IVTp8+xeNve4IPPvEkH//9T3B4lOFcw/FsRhCGnDn3EN77VoEwDJgtlkRB1PoPJh0Umtl0Qa+zwfy45A8+8UlM3WCffsf9GLo3NcI69DTnS5/9PJEBfTSl3Jswc0eQBmx/xzuJz2wxdw3SOOrnr0FhkNMMtz/F3tnFDEYEpSHKlvDMJapHN0EMoBfDRh92xmAquH4LHIjBCD3apC4rXF4RxjGitty+eg1TtdrgWimi0ZDAKUSWcXx0jF9kTA6PqG/fgjhqffm6AaIzRFzY5r3nH+bG9RuUsyX27hRTVUw+92nseIS+cIb0Pe9lfvkyuBCVpCglKPIM7QRJklI7R609RAHLbgdihctKXJazYQLYm8PhFIqivcpNE6Kd00i5rgM/Se5PHbiTSKNAtvKqOPtKY7iiAWtxtqaqlpT5nDCEopyipEMrj1YeZ2sCrVtRKA913U7JVGWJa9qabmccIFvFNBWgpcJ721qzrYSrkAJwGFPjrcPJdlsYgFQe55uVlZsjSUK0VgRB0ErLhu29J0JpibHtwegw5Cd/7Ke4dOl5vvTl5wC4eXTA1ukN+qMOWZ5hrUVIyfFi0ToBGUPTtMv3URyjlKYqa5ypaRqLsQ7TWIqyYnNjjFy3LN9zyuMFz//X/0ngIS9K6mwJVQFpyIXv/S6CUyPQCnOcsWE91bGjWZTU0yXlvNUtWZiayWKONQaxsYnfUdBNYXOI3hgSDwcUsxk2K2FZgpUwzVFFTVjWRAQkocCFEdHmCLuTQ14jrKKuW0OTKquRttWNr9MURn3w7XSh84LZ8YTrSJpF26V569oedjbHTqewcAgTEZISlAFNFBKKgNhLqsYDFqEiRD+BnoZ+DL2UvG5ANUiraGrJqD9kcZQBlihMwUk2iYlZXxmeJPenCkVqIh21eiWmARxCOKSwq7ntEmdLbD2nrmakUYySNVEoCHQrO1vkbXOOdxZrHNZAUzXtFweBFBIVaF7WhPVupb/ysmWmc6sz/3ZaxNsa5wzeeoSWdIZ9tFY0TSszGwQBaaeLaewrErhSCaRUSNWWE5ZlyWI5I0kirt+4yd3dPfKiYrw1RgcS7y1SQhQFCNk6naAE88Uck1niJMJ7GAx7NLVlfrygKS1SBGSLeTtvv5LYXS9h3nucMVSHU5RpqAOJCQQkXYgF3YtnCLsR+WzJ5OouwWRBc3iMWWa4PIO6gq0+pWzYPz5EKU331BaLbY/YGBGMhkS9LmEQto714x1UUBBUnmTpmB0cI0yDLS3GCpKNAQtqfC/GB6qVXxCa+KDGLWuU0CRRSrOxidwa0TQVtmmgNpjJMbODOdJKXNFq20fWEfgIl3uCSUFnWKFdwCwNCABlHNo6jGkgsahYEyQxYtxFDRPqWY51DqqGujbEUdRqAIURnbRLEIUMBkOCMlg78pwg96mMUBGrgKYuqU3VuuFo35YGuhpnM5zJ8S5Hy5pO2sFZiCNNFGgknip32KakqRuaxuKsWJ1x0zZhqAAlNa5xq8TusN6g5MtK4+2UiXMWsLRWrwbnDcIr+sNumyjzts0/jhNOn96iaQxZlmNNa91W1RXj8Zg8zynrmtliQafX5Xc+9jGklIzHm5w5e450I+DKlStUdd2aNXRSdKDaZG1rirIgjiPiKCJJYpYmo24KmtqSpJpiOcN7CMKErFi0Mrlr7ilCSMIoYDbZQ505jdzs43sR3haYRJEoQT2fsffcZbh8Azp9sBUIC50A8cg2TVVxUByThAkbp7coT4PsD4g7KYEOoDSkDfS7W3SkJVrW+GnG8c0jvHBUaY6rDEmv216pBRIvFM56dLdDcmeCn5cgA6I0JRptEo6G5E3dTsMscvz1fcy8QEcdhAzQThJ3+1jnaJxBFYZoVjDsdGikRljbnvhYi8tKrNbIoEMSRui0S9CNmVeOOqhwSlPVGRqHVyDikGDUpz8eEY76iN11Aj9J7k8ZYZmTz0uEgDQJSBKFUu2HaLmYkS0PcbZAScdoGDDe7CD9kjAICLVG4DgWHuUl1gtCpQmSiLKsqMtWn6JyFQKJRBGogDAKkUBdtfv11mBtjfWmbYVOI5QOW2MHJRkOu61fpWwrV+I4pNOJyauavCxw1lGZimee+SI/+7M/y9WrL5I2KT/w3h/i+HiC8Q5jG6QUXLt1k1Ouy4svXWE2W5KmHb793e/gxu1rvPjidTyOJI2QUpMkCS9euoVUiuGohxuAcKD7A4q8YDo5AtldqxG+AVgNs00F597O6OJ5XByQmZqqyrn06U+3nYjTBRzOoMxhZxMGfdjowEMbhI+eQ5aG+vYBPqtwScCF0TZZnlPPl63n6jxDTDOG0YDNqM+m7IKWPHf7CBsqTCfAGMNhJ6I2OfRSCDXeOZamYnNSUGtBXRuWpsFtdCDMScKQxGqqwpPNLVH3FOfOnydKEg6mR+xND0lGA2xRsCgKsutXefdT34HBUGVLfFlDWWIOJ5jZgs5ySFJ5okoQzLq46TH58YxqMqPcPaKc5UCDSUIOB4rksdPseks5WU/tnST3JYEr5YhCj5QQyoamrJkXC5bLI7LlhF4v5PTWkLJccrB3m1hZdu/cQmtNGEQoqZkezOh2engP1hnyPAMvUUKjA4lAgRUY4zBNjbethKsONVoJrLFYq/B40k7I6VMbxFHQVqR4ixMWGSiEBmsbstJyMN1ndrxksVwihKTT6XJqe4vf+djvICRYb7m1d5sP/vAPcePODYqqWBkxw2NPnOPcuXMEwR7LbMELl59n58xDPPmux+n3u2zvbPOep55CK8ULl17g9q1bLBcLHrn4GJ/+1KfbRVgxxNhTHB4eroS01txTegniu76drcEGxXSOyytUWcOzl7n41NPMqymVi/CDTfp/bkzRCZFbfdSoi+8nTLMlFx45Q5YmLK7cYO/LzxI9f5VqNsV3EnSvT7fbZXrniBcOr3LdSEIn0bVlqYBuRLQ1Qm32WNYFyc4pwjgC5zHLjOy5SySnT6O1JPM1y2wJgacqKrYGIyIjCGYWqbo8dvYi8WiA2uwxfM9jFHvXWS6muGAEQuKygi9UB2gVYfICtyxRtYX9Q8hKTKdHOTigkQodhcwXc+qyxJtWpgIFbHTwnZhmGLA43eHg9g1Kt27DPEleVwIXQgyBfwG8k7b7/W8Cl4B/BzwMXAN+wns/fT3v19Q5VVHSH3RZLiZUdUYQCEaDiDhMqKoF1gQIXyGF5Xh62DrvyICmMhR1TRp1wWq8W80Je09bWefb1zqLd7T101IiBXjRvs55GG9toAOBwBGEoLUgjFpNlGy5ZDk95vz5C61LvW3aihAtEcqzOR5R1YbJdIKXlv5oRJzGJElCmqZcfvEFijJjfHoTHWiuXLnC7u4uZdlK446TMae3t9g92APlODja58btWxxMdlGqrfCuypKyzrl0+XOMNsO2aaOsmU5nWD/Ds55Cued4j29qZgeH2EWBv3sIt/Zgf0Zz+4hhJyXYHqEHKW6QkkSSWniKWU5+MGVrOICDJaJ2yH4Xf36H9EsTqqMMP8sw6ZLloAtVjougTDS10gjrcRua0cVz6FEPlwbIQOK0JFQB9SKnLmvkYMRsOETFEdbUkC3auXd02wbXNJTzJWEYIsOA/eUxSzPHsyB4aEwshjSuXcORjSNwguFhTl6XLIsGYRydzTFFcZdmOsNOFwgdEiqNs6btSo4Dws0BthtiUo0LJDaQTG/cwCchrMtbT5TXewb+K8DHvPd/WQgRAinw94GPe+9/WQjxUeCjtDZrr4MGKXJCHZFTIHxBoEPSNEagyJYFZS7wzqKkxzY1gdJIIXEGmsqSxDHeSgSt36WU9UpXxeDtyt3HCxQgZKtpLGTbKNQfdOmPugS6rUAJgraKxdgG5y1KScqyIM8znGufK61oTE2n12E4GJHlBdP5MRujDfr9LkIJpAYhHXfu3qIxDUWdkaiYjfGQ27dvUxQFOlB0+z1GGyPyOmO2mFI1NWWd88ILM5I0Yjjst/ZvdYmSkjM72wgEOhA4QtLu1qokcs295OUywiqrWuW9u4ewewRO4vMSpzUmENgCXKqoFjnGeZRx9I0jzQVNuISklZeNHzrNxg3XmnYEILoRjHoEDLGAlxonBNYYlJQEF84gewkECiU9srHYvMZ6B0IQD4YUZ7roTqe18psFbSmfkTjvqK2htA39XpeFaJhUGYumwUcNvZ0Bcb9LtFrE17TiXaemnoPyiGxZ4vH0Tp0ilBKzWOKKBlAEHrSM8aFG9BLSs6eoOyGZb6jrAp/nVIuM7vnzrUPPmhPjGyZwIcQA+ADw1wG89zVQCyE+Anz/6mX/mtZq7XUl8EA74sgQ6JIkBa0lQeCACmsKTJWx9PVKh8S3tdhC4ozAGQ9OUpUGRYDWIYFWoNu5b9cYPCtbNqWQ0iEkqyQsiKOAs+e3X2Xy4Foj1gYWyyVhqOkP+0yygsnkiChO0EGA847Z7JizFx5mOBqiw4D+oMvFi+dpmobJ8YSj6ZyqLpkvFwRBwLWbV+l0Up5++mn+4Pc+gbGW0caAKA7xwvPwI+e5dasV5VSB4PBon24votMNKcscW+SMN3dIe63YVhgGbI5PMxj0CZaa9TT4vUXWlujGMXaxwBcVzBdgPXLQJ04iZsWC5XSXytVE53Yob94m0jGbnSEP9Tc4un6DuhvAQyPCUwPCjRGnzmv0zhjbi2DURYx7LKWlsq2dnm0ayjxDRQFVNyVIY0SgMUVBRwTMXrqNtQ1BGNKLAvYf6uI6PYTxqDRqTbcnOUVT40xFE0vcVo9dVVOGFouDuqScTBkpRdpJCYIAATSuYdTAIqvx8yU2VgzOnkI8vIOtapqixGc12jhQCqMlPg1JHtqiDgXVZEKzX8A8g7xhOLKEazXZE+X1nIFfBA6AXxdCvBv4LPALwGnv/cuyNbvA6a/1y0KInwN+DqDf6wJQ1xmz+V2yfJ+6yGlMjRTtgmFdVzR1udLsFngHRlm819jaI7xEiQhTg8XgnGy9MpuyLaPCEUVhO53RTYnjGBUopBYr2XFHZRfMZlOcdQg82WJOFAU0VU0URaTOk6QJURTR7fYoipK9vT2WeUHjLM988fNkWYYxhijRfN/3f4Abt25w7XpDebQkTGBv/4A0DegOInRo+am/+le4dedmO3ct4M7uLY5vHPHwo+dIOwlCOE7tjEiSgO/7wPvZ3bvDx3/vE3zyD5/n4sMJjzx8niSOmS+OuHzly7z/kYcI1w7g9xS7yMk+8wzgoBtCGqNPbTJ+/FFGj16kmR5RHuxTHu5TPn8JugOqF65y52jBXa/oXLjAzru/jdkspxKOjt4iOHuKxNbkkaCKFZWGXHhkEhHK1lMyqbpUVUFelug0ppOk4DxbnR7zLzyHOT5GOd36YI76xEmXqHAEhcCKJWWTU9gCaytc13O0Ieid3yFVEl83mHkGd6dwkFFLReU8eVWS1SX7z9/B4ZGxxvYU/Xdc4NrhLt3BkFG/z9GtQ5Ro58G9MSyaimUkqa2hTjVi2EcZkLsz/N4cX65LUE6S15PANfAU8PPe+08JIX6FdrrkFbz3XrTSe/8f3vtfBX4VYGd7ywNYW1EWxyuXG7MySfDtNIFSdNIe4MGtTOWdbBsyEYRhRCfpU1WGujKAb9/HVYSRRgUhUgsQBmMLam9whaFqKpy39Lod+qMdyBxCebqdDm9/4nHmizlXr1ylspZUB1zYPsuXv/xldnf32xNdIYjjmP2Dfbq9Lt/9vqf57vc9za//xr/iC8/+MddvXedoekinm/LBv/ADlEXJ2fNncM7wu7/7MZbHCz70oQ+xu7fLF559hsVyylNPvRtDw2w+ZbGckXYirK34+Cd+n6au6HU7/PiP/wBf+uIzXHnxOtZalIAwDL/pAV/z2iil6Y1GhFsD1OkRfpjiBynRqQ2qUQedQNINaEYp1WQCxsJ730YYJaRJynw646VyijcSmQc00yWfLSUuCXGJbqtMdNuDIIVENRZZNdhFgcDTVDnNeJN6a5Omqcl7fdyLd3CzJU2YYmMDL+wx6Bs2XEAyMyx2C/buTqmlwXYkbCRwOmUZ1WgVIa3D5xnNpesUxzmirKBprzxdP6WuMxh20TvbbL77Ca6aGeUoYm5LdqclvVFMtqxwdU1VlZR5DqbCHx/jhUKqBDXs0ywrdm/eolQZ616ek+P1JPBbwC3v/adWz/8DbQLfE0LseO/vCiF2gP3Xu1O/cmI3piHQeqXd7XHOtx2GXtA0FokkDAKM93gsQgics1RVgbGtW72SCh1I4rDPcNRHh5rGtn6XaE+SaoyxNBi8bQg7Pa5cu4ypS5TSFFVGnuc4B1HawVnPbJExmV6hKNvXeO+pq5o8q9jcGnLu/Dn6gx539+5y/vwZlvmcU9sbjLcH1E3JbHFE05Ts7luMqSmbJbu7t/jf/+cTRHFEnAQMNwZs72zxwpUXuHXjgIP9KYOx5uzZU2RZRtM0CO85ns44nhXUZU1TO5xVPPrI6BUz6DX3DpmEdN55kd72GDXq0sSaMoBcespigRAen4bEcojqJjTCoYc9ZBJRCoE7SHB7U8gNbrbE3TpofTR7HYgDvBZ478C13eeubBBZhV/maB2AqaiGS/xoQmkabBTQ3NgF4/EpOKPgsy+ixxlx2KVbQX3nGDlZQkcgel3k5gg2R1jlMY1BVAXM5/jZDH+cQ7WqEklDOhtD8vEWvp9iRwPyjqbXiXGLOc0yx+cVvjJ4364/WedasbeoQ53U4EAmCUHaRwaaKhT4w2uwtsU8Mb5hAvfe7wohbgoh3u69vwT8IPDl1e2ngV9e3f+X173Xl114nEcKhRIaj1tVkwisbxcqpQQtRStE5TxKKbx35MUCKTVKKaI4IAoVSaAZ9oeoALISiqrBCoPWHpRHG4cVBkPJ8fKINI6QUlDbimZhqWtHr9PHGsdykbGYLdqSQ/3KIYMQDIYDtndO0+l1mB5P2NgccvPOdU5tjknSiMn0kN39mwSBYp4dUTcVKnCU2ZKDw7uMt8ZEaUyShiBeNomwWOtZLHK0iqgqgyBAK8VkkrGYVQwHQ5JIky0bFnNYCSWuuZdEAf7iFmo4BN3KLti6JqtyLBYdRkghEXiCJCbqt9UotbRU5RLODFA42FviZzn28BiCiNXcIODbzO0lovHIrEZkVXsTAiEcrrJUWUnjDY1vYFGACiAw7Zz0jSnN1FLGXZRT1EfHaOeIVESQpujBkCaIEAE0ZYbNCtx8uUrcHkKNCDVqs0f37CnMw0NMrJGBRkUaN1sgFzny8BgzmVHmNTjR/oOREgKFHI/ROsV5hwwjVCdBJREmUYgyWSfwE+T1VqH8PPCbqwqUq8DfoJW6//dCiJ8BrgM/8br3KiRSKMBia4eXBuc81jpk64yJsxLnPKVvZV2lhygKMaZhPl8wGPTp9mL6/S6BUtgiQwmBc4amKamqHENF2DhEYEE3WFMxOb7L+HSH0XCM8AprBZHucPXFm+wdHtJUBms8gdYY45gdH7eLh1sjds5sM9wY0ut12/1Gghu3JhR1xmKpMC6iajIOp7ucO3eG49mUZbZkMOyhVMx7n3oHXgiOJkc0puTGrRsorTh3YczWdo+jownD4RjvNYEO6XV63L27T1lIzj75OINun73dA/7wk8/y/W93sPZzuKcYBdO+wNsce7CgyUrqsqRsClCOqt8FrRFOkIYxp9Muh5M5dXYE8wP49ncRn9tGmkPsNCdHQm+ISJO2G8s2rXm3jgiFRzUS3Sg0EflygQgDhNStCqBSkC+hkyCCCKEDclNDXjGp9sg4JHSirRw5s0V0eoAabxB0ehwuSoYbQ+b5nOU0o5ouW9/ZfgcRh+h+SryzQffRhzAP9aidRVsYRz0u/8EnGSR93J09zJ09SPuwu4dRqw9boPGPPkJne5MmUshIohC4bkLQj5FX4/s7iG8xXlcC995/HvjOr/GjH/ymdio1cZCgaLWz67ptV2/PqgMCHaASiXe+dZRvKsJAs71zGu8tiIYL5x8ijiOiKCaQCtUM0IGksjkdH6F0ihESpR1OGHTo6UUhg40RzkIUhtSVJytKbtzao8gMgY5RUYCnocxLzp8/z7K7QCrBxuaICxcuUJuSO7u3eelmRVYsyIs5/X6H2pSYvCTpBARLiZeG4bhLdxhS1zVPPPkYn/z0/yXPc+I45sLDD/PHn/ssURRhrEMIyWOPPc7dO4fs3t2nyCvwkkAnxOEGn/mj5zGNJQwifvQv/mjrr7nmnuKtpT6eMTnIsc/fQhznSOsg1dAF+gk4D4WBeIPrv/s5hmdPc+bCDvLit7F7J6epanoW9GBAuGMwm6fpbAxwocaFCh1php0ebpFTHE6pjxeIokHnC3wUoPoJohfjpaMuM5RUhEKhjSc7miJObVBPj6mXSzANjMdsPvkoIlToJCIykvzulMWzN1je3qU6OGqrRJIIOYgINvsEW0OiM2PKUYi9M2EzTOgR0ly6jnjmFrP9CcIJZBDixCEsMkhSKCuoKlwVEGeedKNP6GLCjqRONce25muvhK15o7g/hg51w3KZI6VaCTRJ1Eox0FtPlpWt5rZSaK2oqgpvDGGg0UFIkkT0Bx2ybMliMcEbT+JialfhZYMKPSqGOA4RocVKixIhXoL3lo3NMTdu3OXocMly0eCtIOkMaUpLknQYbo+IECwWC5IkIUli0rTLzZs3KaqctBshFFSmpD/sowPoDYZ0eglhLJnNDziaHnD69JjhaES2XLJYTnn72x8nzwtm8xm379xkvLlB1TTYoqKuG669dIPhcJNz5x6maRzZouR40pYklnlbD180ji888wIXvudpVLiuBb+nVDW8eAtzZwnP3SHKLZ04wT80YjKKIe2CsYg8J70+5eIMljdfovribezDY9InHiI9d4rZbMLs8AjyjO13vgsfByxcTeFqvGkoVYRKBXbcKv7FMiCVjmrfm4kAAAaMSURBVNwZfCfEJwGNrcAP0Cg6OiJFoQ8nLLXCTCf4qkRIRbJ1Cjcas9g/JKxytpTmDDFXLr+AnByTNo44HRCf2qDsBbjNDnKji+onZMpzqlLE0wIxn+Bu7tO/vSBo1MowXOLTmGYzwWtNFZTUqqDTCIqrd6lv7xOcGtFZZJSbKVk2xR7P7/covqW4Lwn87Kkx3/PuJxCyPcsG0TrerJ5ba1tFQSlRSlBWBUpItrY2kUqw1Q8ZDYdUdZ+mrvHOExBgnAHhkFqggraxBuXwWBAeL0BIQRJ3SLdG5GlF0zgEijCIMY0jCELSOEVLSZ7nbR16EBCE4WrxtCEIA6QUWG+I4hAhPVEcEmqN8oInt96DsYZu3CEWIXVco01Iqjo0iWFESR7k6CCkMRZjDNZYpNSkaYcgiPEe6m5DMSjxDuqqwTRtW34ap6808kgp0cHa4PhbYrUg3Hc1P1hdg7CGs0u0gTAw0DPkoQYTgPOIqCEZGnopVFVFo8GJGUwOCV2XoiioiwIhGnp3PF4JKmdovMMLCHZbJU5vLcI6AqkQUlA7C4EErahdW2GlEYRSEyCpi5JaCmxVgLUgJGF9nfQgploskdbTDUJ049jyu5ikQiStkqZmRlNJ/EQjihA5iTFK0F3YtoW+amjKnGJzivICIQReSgg0TrV2fia22G5DGGQ45zBSoKo7BLvXMbOApi7YKtpmbIFAr423vyVej2T0ffkLb2+N2N4affNv8K387orxeBvG3/Lb/Elce+sOh1/Z5mn/yhpYOaENUtpe1m9ECnyDUIVYf1HuFR0a3uf32umS7stbGyD/ykOAENj86t/OoDxsOyJeRgJ3d7/6hSfCuYA/uUZiaRcXC+DrCV58M9+Jxer2agRr16gTYH0NvmbNmjUPKOIk3c2FEAtaEay3ImPg8H4fxH1iHftbk3Xs944L3vutr9540tfel7z3X6ua5U2PEOIz69jfeqxjX8f+RrKeQlmzZs2aB5R1Al+zZs2aB5STTuC/esL7+7PEOva3JuvY35qcSOwnuoi5Zs2aNWvuHesplDVr1qx5QFkn8DVr1qx5QDmxBC6E+JAQ4pIQ4srKQ/NNjRDimhDii0KIzwshPrPatiGE+B9CiMur+2+9pfTPAEKIXxNC7Ashnn3Vtq8Zq2j5J6vPwReEEE/dvyP/1nmN2H9JCHF7NfafF0J8+FU/+3ur2C8JIX74/hz1t44Q4pwQ4hNCiC8LIb4khPiF1fY3/bh/ndhPftz9Spv7jbzRKle/CDxC24j8DPDkSez7ft2Aa8D4q7b9Q+Cjq8cfBf7B/T7OexTrB2hdm579RrECHwb+GyCA9wGfut/H/wbE/kvA3/0ar31y9dmPaK0KXwTU/Y7hm4x7B3hq9bgHvLCK700/7l8n9hMf95M6A38auOK9v+pbU+TfAj5yQvv+s8RHaA2gWd3/2H08lnuG9/5/AZOv2vxasX4E+A3f8ofAcOXo9EDyGrG/Fh8Bfst7X3nvXwKu0H43Hji893e993+8erwAngPO8BYY968T+2vxho37SSXwM8DNVz2/xdcP+M2AB/67EOKzK2NneJ1G0G8SXivWt8pn4W+vpgp+7VVTZW/K2IUQDwPvBT7FW2zcvyp2OOFxXy9ivnG833v/FPAjwN8SQnzg1T/07bXVW6KG860U64p/BjwKvAe4C/yj+3s4bxxCiC7wH4G/473/E2Lgb/Zx/xqxn/i4n1QCvw2ce9Xzs6ttb1q897dX9/vAf6a9ZNp7+bLxT2sE/QDyWrG+6T8L3vs977313jvgn/OVy+U3VexCiIA2gf2m9/4/rTa/Jcb9a8V+P8b9pBL4p4HHhRAXV76aPwn89gnt+8QRQnSEEL2XHwMfBJ6ljfmnVy/70xlBP3i8Vqy/Dfy1VVXC+4DZqy653xR81dzuX6Ide2hj/0khRCSEuAg8DvzRSR/fvUAIIYB/CTznvf/Hr/rRm37cXyv2+zLuJ7hy+2Ha1doXgV+8XyvIJxTrI7Srzs8AX3o5XlobgI8Dl4HfAzbu97Heo3j/Le0lY0M7v/czrxUrbRXCP119Dr4IfOf9Pv43IPZ/s4rtC6sv786rXv+Lq9gvAT9yv4//W4j7/bTTI18APr+6ffitMO5fJ/YTH/d1K/2aNWvWPKCsFzHXrFmz5gFlncDXrFmz5gFlncDXrFmz5gFlncDXrFmz5gFlncDXrFmz5gFlncDXrFmz5gFlncDXrFmz5gHl/wEiQ+3HqPHyIAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss: 0.27 \n",
            "Loss on secret: 0.00 \n",
            "Loss on cover: 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss on test set: 0.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Avg. S error per pixel [0, 255]:\", sum(S_errors)/len(S_errors))\n",
        "print (\"Avg. C error per pixel [0, 255]:\", sum(C_errors)/len(C_errors))"
      ],
      "metadata": {
        "id": "QgXKeiYWYJjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe1777c4-cefd-4ef7-fd4b-de096e0c90a8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. S error per pixel [0, 255]: 302.6036516082764\n",
            "Avg. C error per pixel [0, 255]: 302.5092018218994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Avg. PSNR of secret and decoded secret:\", sum(psnr_s)/len(psnr_s))\n",
        "print (\"Avg. PSNR of cover and container:\", sum(psnr_c)/len(psnr_c))"
      ],
      "metadata": {
        "id": "R-yTZisrY3ES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc9fab7-ac1a-408a-a760-a3afee679de1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. PSNR of secret and decoded secret: -1.2805003660657372\n",
            "Avg. PSNR of cover and container: -1.2605150046625124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot distribution of errors in cover and secret images.\n",
        "pixel_histogram(diff_S, diff_C)"
      ],
      "metadata": {
        "id": "JdHfdvtkfgp2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "7997f5d5-95d3-43ab-d453-42be979764a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    }
  ]
}